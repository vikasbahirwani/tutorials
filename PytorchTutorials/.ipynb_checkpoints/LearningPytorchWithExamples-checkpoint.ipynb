{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial URL: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#examples-download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PyTorch with Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Warm up: numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 31417111.294516183\n",
      "iteration 1, loss 24247502.40470317\n",
      "iteration 2, loss 20005988.921914797\n",
      "iteration 3, loss 16117189.756072668\n",
      "iteration 4, loss 12222566.806160655\n",
      "iteration 5, loss 8681132.341945734\n",
      "iteration 6, loss 5921160.999809977\n",
      "iteration 7, loss 3985456.805415063\n",
      "iteration 8, loss 2726648.7923619077\n",
      "iteration 9, loss 1930456.40365782\n",
      "iteration 10, loss 1426214.1303353019\n",
      "iteration 11, loss 1097778.2426994792\n",
      "iteration 12, loss 875346.4834761431\n",
      "iteration 13, loss 717348.0116823423\n",
      "iteration 14, loss 600032.4888708794\n",
      "iteration 15, loss 509547.3710711617\n",
      "iteration 16, loss 437700.77640174585\n",
      "iteration 17, loss 379195.22779171984\n",
      "iteration 18, loss 330728.7218008532\n",
      "iteration 19, loss 290039.7601245702\n",
      "iteration 20, loss 255541.8900700019\n",
      "iteration 21, loss 226044.82380652192\n",
      "iteration 22, loss 200671.51937150437\n",
      "iteration 23, loss 178722.92196428566\n",
      "iteration 24, loss 159637.15994853375\n",
      "iteration 25, loss 142965.61337117775\n",
      "iteration 26, loss 128348.63515735812\n",
      "iteration 27, loss 115487.18017903599\n",
      "iteration 28, loss 104146.77968425091\n",
      "iteration 29, loss 94117.28246255562\n",
      "iteration 30, loss 85216.38894299764\n",
      "iteration 31, loss 77299.4613316395\n",
      "iteration 32, loss 70243.7580730548\n",
      "iteration 33, loss 63937.5599248897\n",
      "iteration 34, loss 58288.25101463967\n",
      "iteration 35, loss 53218.799146217076\n",
      "iteration 36, loss 48655.31962890839\n",
      "iteration 37, loss 44547.22653930634\n",
      "iteration 38, loss 40838.52407140161\n",
      "iteration 39, loss 37481.28318685035\n",
      "iteration 40, loss 34438.440756911135\n",
      "iteration 41, loss 31676.865609084907\n",
      "iteration 42, loss 29166.55974447629\n",
      "iteration 43, loss 26881.150557710847\n",
      "iteration 44, loss 24797.71856374203\n",
      "iteration 45, loss 22897.72292156738\n",
      "iteration 46, loss 21161.1168493492\n",
      "iteration 47, loss 19572.394659484555\n",
      "iteration 48, loss 18117.110607618546\n",
      "iteration 49, loss 16782.763411412336\n",
      "iteration 50, loss 15557.84466422013\n",
      "iteration 51, loss 14432.525479883168\n",
      "iteration 52, loss 13397.723222830355\n",
      "iteration 53, loss 12445.130827514808\n",
      "iteration 54, loss 11567.292534964507\n",
      "iteration 55, loss 10757.798001904699\n",
      "iteration 56, loss 10010.733392416876\n",
      "iteration 57, loss 9320.874884993518\n",
      "iteration 58, loss 8683.22252838855\n",
      "iteration 59, loss 8093.349487606478\n",
      "iteration 60, loss 7547.8879675120725\n",
      "iteration 61, loss 7043.577476924342\n",
      "iteration 62, loss 6576.051537706277\n",
      "iteration 63, loss 6143.158931047864\n",
      "iteration 64, loss 5741.631174056407\n",
      "iteration 65, loss 5368.5584566479665\n",
      "iteration 66, loss 5021.898881311941\n",
      "iteration 67, loss 4699.555015722318\n",
      "iteration 68, loss 4399.7588567880775\n",
      "iteration 69, loss 4120.932450011782\n",
      "iteration 70, loss 3861.145109498384\n",
      "iteration 71, loss 3619.0085345520365\n",
      "iteration 72, loss 3393.2835935767307\n",
      "iteration 73, loss 3182.891328411502\n",
      "iteration 74, loss 2986.5951773604297\n",
      "iteration 75, loss 2803.376971569288\n",
      "iteration 76, loss 2632.3032115497585\n",
      "iteration 77, loss 2472.509825256451\n",
      "iteration 78, loss 2323.0807077775416\n",
      "iteration 79, loss 2183.2963191801473\n",
      "iteration 80, loss 2052.555762133863\n",
      "iteration 81, loss 1930.2603184058569\n",
      "iteration 82, loss 1816.2699325131566\n",
      "iteration 83, loss 1709.5287210323108\n",
      "iteration 84, loss 1609.5086667550368\n",
      "iteration 85, loss 1515.8161532890501\n",
      "iteration 86, loss 1427.9410925606135\n",
      "iteration 87, loss 1345.5256298879624\n",
      "iteration 88, loss 1268.1882012825731\n",
      "iteration 89, loss 1195.5899772758403\n",
      "iteration 90, loss 1127.4726672718225\n",
      "iteration 91, loss 1063.5058051276642\n",
      "iteration 92, loss 1003.4128652402519\n",
      "iteration 93, loss 946.95470159355\n",
      "iteration 94, loss 893.869416001294\n",
      "iteration 95, loss 843.9325787487984\n",
      "iteration 96, loss 796.9592490780517\n",
      "iteration 97, loss 752.7561751860857\n",
      "iteration 98, loss 711.1611219441531\n",
      "iteration 99, loss 671.9922468835803\n",
      "iteration 100, loss 635.1164720364347\n",
      "iteration 101, loss 600.3840303306458\n",
      "iteration 102, loss 567.6470042507874\n",
      "iteration 103, loss 536.8045043874474\n",
      "iteration 104, loss 507.7294528091004\n",
      "iteration 105, loss 480.3169084045005\n",
      "iteration 106, loss 454.4650614751598\n",
      "iteration 107, loss 430.07161101548735\n",
      "iteration 108, loss 407.07134136044994\n",
      "iteration 109, loss 385.3843867733727\n",
      "iteration 110, loss 364.9133292405037\n",
      "iteration 111, loss 345.5857420470016\n",
      "iteration 112, loss 327.3234008009787\n",
      "iteration 113, loss 310.0746123186716\n",
      "iteration 114, loss 293.78033815201445\n",
      "iteration 115, loss 278.3910321884014\n",
      "iteration 116, loss 263.844685877053\n",
      "iteration 117, loss 250.0884548020161\n",
      "iteration 118, loss 237.07991385774886\n",
      "iteration 119, loss 224.78041996070783\n",
      "iteration 120, loss 213.1468479065198\n",
      "iteration 121, loss 202.14350668934924\n",
      "iteration 122, loss 191.73509196527863\n",
      "iteration 123, loss 181.8928066064465\n",
      "iteration 124, loss 172.5735265135637\n",
      "iteration 125, loss 163.751021062282\n",
      "iteration 126, loss 155.39846558576892\n",
      "iteration 127, loss 147.49124428617728\n",
      "iteration 128, loss 140.0039936128029\n",
      "iteration 129, loss 132.90819188588034\n",
      "iteration 130, loss 126.18880765597304\n",
      "iteration 131, loss 119.81863999385153\n",
      "iteration 132, loss 113.78327748518547\n",
      "iteration 133, loss 108.06272260276917\n",
      "iteration 134, loss 102.64078627024048\n",
      "iteration 135, loss 97.50092332926336\n",
      "iteration 136, loss 92.62773990003083\n",
      "iteration 137, loss 88.00798094570783\n",
      "iteration 138, loss 83.62391919315209\n",
      "iteration 139, loss 79.46674535260544\n",
      "iteration 140, loss 75.52328778516295\n",
      "iteration 141, loss 71.781041604271\n",
      "iteration 142, loss 68.23114358637909\n",
      "iteration 143, loss 64.86426628742817\n",
      "iteration 144, loss 61.669513092173915\n",
      "iteration 145, loss 58.634501429194856\n",
      "iteration 146, loss 55.75441191271536\n",
      "iteration 147, loss 53.018975290691145\n",
      "iteration 148, loss 50.42184488133054\n",
      "iteration 149, loss 47.9558884150691\n",
      "iteration 150, loss 45.615515582414446\n",
      "iteration 151, loss 43.39195293410063\n",
      "iteration 152, loss 41.279152739948344\n",
      "iteration 153, loss 39.272140084733834\n",
      "iteration 154, loss 37.36519460333567\n",
      "iteration 155, loss 35.553739137190135\n",
      "iteration 156, loss 33.833219777332815\n",
      "iteration 157, loss 32.198126085379684\n",
      "iteration 158, loss 30.64349786457251\n",
      "iteration 159, loss 29.166330278594135\n",
      "iteration 160, loss 27.761787207785527\n",
      "iteration 161, loss 26.42640864759734\n",
      "iteration 162, loss 25.157696224908165\n",
      "iteration 163, loss 23.951668039630313\n",
      "iteration 164, loss 22.80396516418544\n",
      "iteration 165, loss 21.712619918778643\n",
      "iteration 166, loss 20.674869444684028\n",
      "iteration 167, loss 19.688206359654664\n",
      "iteration 168, loss 18.749286195126864\n",
      "iteration 169, loss 17.85683074898764\n",
      "iteration 170, loss 17.007346960969578\n",
      "iteration 171, loss 16.198971741243856\n",
      "iteration 172, loss 15.429889235616693\n",
      "iteration 173, loss 14.698451764288837\n",
      "iteration 174, loss 14.00243189135512\n",
      "iteration 175, loss 13.340310169995185\n",
      "iteration 176, loss 12.70982414824207\n",
      "iteration 177, loss 12.109651453129729\n",
      "iteration 178, loss 11.538438144572847\n",
      "iteration 179, loss 10.99454076975503\n",
      "iteration 180, loss 10.476898915311775\n",
      "iteration 181, loss 9.984357255628819\n",
      "iteration 182, loss 9.515203653591527\n",
      "iteration 183, loss 9.068483056804435\n",
      "iteration 184, loss 8.643264933243918\n",
      "iteration 185, loss 8.23815693678779\n",
      "iteration 186, loss 7.85237319386329\n",
      "iteration 187, loss 7.485282779017201\n",
      "iteration 188, loss 7.135695198751642\n",
      "iteration 189, loss 6.802641570571881\n",
      "iteration 190, loss 6.485215128207485\n",
      "iteration 191, loss 6.1830120383197045\n",
      "iteration 192, loss 5.895153595667865\n",
      "iteration 193, loss 5.621093205634286\n",
      "iteration 194, loss 5.3597262650210205\n",
      "iteration 195, loss 5.110847616059295\n",
      "iteration 196, loss 4.873595654209376\n",
      "iteration 197, loss 4.647542635367188\n",
      "iteration 198, loss 4.43224190344063\n",
      "iteration 199, loss 4.227176268019681\n",
      "iteration 200, loss 4.031546335079907\n",
      "iteration 201, loss 3.84513984692948\n",
      "iteration 202, loss 3.6675208496540534\n",
      "iteration 203, loss 3.498299114060726\n",
      "iteration 204, loss 3.3369711501070918\n",
      "iteration 205, loss 3.183153279093042\n",
      "iteration 206, loss 3.0364988803495914\n",
      "iteration 207, loss 2.896726481466609\n",
      "iteration 208, loss 2.7634587954355467\n",
      "iteration 209, loss 2.6363995874192536\n",
      "iteration 210, loss 2.515388723018228\n",
      "iteration 211, loss 2.399928396178553\n",
      "iteration 212, loss 2.2897866997169736\n",
      "iteration 213, loss 2.1847880956657857\n",
      "iteration 214, loss 2.084697712272331\n",
      "iteration 215, loss 1.98926482309933\n",
      "iteration 216, loss 1.8982665656250044\n",
      "iteration 217, loss 1.8114245995355054\n",
      "iteration 218, loss 1.7286671063131727\n",
      "iteration 219, loss 1.64967485462392\n",
      "iteration 220, loss 1.574352745233671\n",
      "iteration 221, loss 1.5025461425834816\n",
      "iteration 222, loss 1.4340464324848265\n",
      "iteration 223, loss 1.3686700596769636\n",
      "iteration 224, loss 1.3063056601938408\n",
      "iteration 225, loss 1.2468287343490752\n",
      "iteration 226, loss 1.19015171607234\n",
      "iteration 227, loss 1.1360255304022213\n",
      "iteration 228, loss 1.0843796073818015\n",
      "iteration 229, loss 1.035135982091762\n",
      "iteration 230, loss 0.9881426303179176\n",
      "iteration 231, loss 0.943308339427175\n",
      "iteration 232, loss 0.9005398248342859\n",
      "iteration 233, loss 0.8597065600606515\n",
      "iteration 234, loss 0.8207660364235301\n",
      "iteration 235, loss 0.7835839910470286\n",
      "iteration 236, loss 0.7481164231714417\n",
      "iteration 237, loss 0.714283485318675\n",
      "iteration 238, loss 0.681992273387578\n",
      "iteration 239, loss 0.6511557717082186\n",
      "iteration 240, loss 0.6217266871451587\n",
      "iteration 241, loss 0.593656683442136\n",
      "iteration 242, loss 0.5668945831625288\n",
      "iteration 243, loss 0.541308970274357\n",
      "iteration 244, loss 0.5168919131641723\n",
      "iteration 245, loss 0.4935908825590283\n",
      "iteration 246, loss 0.47135734363179527\n",
      "iteration 247, loss 0.450139537068842\n",
      "iteration 248, loss 0.4298713048739309\n",
      "iteration 249, loss 0.4105224980609829\n",
      "iteration 250, loss 0.3920662863636842\n",
      "iteration 251, loss 0.3744333976134546\n",
      "iteration 252, loss 0.35761261083514195\n",
      "iteration 253, loss 0.3415420464358775\n",
      "iteration 254, loss 0.32621983358220424\n",
      "iteration 255, loss 0.3115733980318771\n",
      "iteration 256, loss 0.2975887137911763\n",
      "iteration 257, loss 0.28424960777094727\n",
      "iteration 258, loss 0.2715099267324781\n",
      "iteration 259, loss 0.2593398820845906\n",
      "iteration 260, loss 0.24771706914159156\n",
      "iteration 261, loss 0.23662029491413955\n",
      "iteration 262, loss 0.2260379661845238\n",
      "iteration 263, loss 0.21592036723935645\n",
      "iteration 264, loss 0.20625898928805703\n",
      "iteration 265, loss 0.1970323764757913\n",
      "iteration 266, loss 0.18823247813583316\n",
      "iteration 267, loss 0.17982643819746746\n",
      "iteration 268, loss 0.17179122302189065\n",
      "iteration 269, loss 0.16411638439366438\n",
      "iteration 270, loss 0.15679172160976818\n",
      "iteration 271, loss 0.1497937575533646\n",
      "iteration 272, loss 0.14311583500624728\n",
      "iteration 273, loss 0.1367319611156979\n",
      "iteration 274, loss 0.13063747094764974\n",
      "iteration 275, loss 0.12481343226367371\n",
      "iteration 276, loss 0.11925245358344747\n",
      "iteration 277, loss 0.11394358086351852\n",
      "iteration 278, loss 0.1088744217082552\n",
      "iteration 279, loss 0.104027900232646\n",
      "iteration 280, loss 0.09939870255740563\n",
      "iteration 281, loss 0.09497718074326714\n",
      "iteration 282, loss 0.09075634652682024\n",
      "iteration 283, loss 0.08672128501260645\n",
      "iteration 284, loss 0.08286619183137772\n",
      "iteration 285, loss 0.0791838472889691\n",
      "iteration 286, loss 0.07566886705722456\n",
      "iteration 287, loss 0.07230982706264448\n",
      "iteration 288, loss 0.06909882124398296\n",
      "iteration 289, loss 0.0660332379855414\n",
      "iteration 290, loss 0.06310490387132388\n",
      "iteration 291, loss 0.060307517530663796\n",
      "iteration 292, loss 0.05763318157755216\n",
      "iteration 293, loss 0.055078227252051706\n",
      "iteration 294, loss 0.0526372757606074\n",
      "iteration 295, loss 0.05030551764771532\n",
      "iteration 296, loss 0.048078154211068105\n",
      "iteration 297, loss 0.045948779339216025\n",
      "iteration 298, loss 0.043914444458701005\n",
      "iteration 299, loss 0.04197143083178122\n",
      "iteration 300, loss 0.04011440681811232\n",
      "iteration 301, loss 0.03834142959886429\n",
      "iteration 302, loss 0.03664557185578263\n",
      "iteration 303, loss 0.03502604831193941\n",
      "iteration 304, loss 0.03347727568672607\n",
      "iteration 305, loss 0.031998548742356483\n",
      "iteration 306, loss 0.030584939331603944\n",
      "iteration 307, loss 0.029234401526508618\n",
      "iteration 308, loss 0.02794307485336256\n",
      "iteration 309, loss 0.026709282373735106\n",
      "iteration 310, loss 0.02553093807993785\n",
      "iteration 311, loss 0.024404670104496914\n",
      "iteration 312, loss 0.023328273247722388\n",
      "iteration 313, loss 0.022299332981330265\n",
      "iteration 314, loss 0.021315956947747086\n",
      "iteration 315, loss 0.020376793031492167\n",
      "iteration 316, loss 0.019478600719090595\n",
      "iteration 317, loss 0.01862006753901379\n",
      "iteration 318, loss 0.017799520219020086\n",
      "iteration 319, loss 0.017016119074757127\n",
      "iteration 320, loss 0.016266738955445233\n",
      "iteration 321, loss 0.015550527296480827\n",
      "iteration 322, loss 0.014865748093685117\n",
      "iteration 323, loss 0.01421184165737352\n",
      "iteration 324, loss 0.013587037642178987\n",
      "iteration 325, loss 0.012989251847351063\n",
      "iteration 326, loss 0.012417956419684496\n",
      "iteration 327, loss 0.011871870413845126\n",
      "iteration 328, loss 0.011350291918836512\n",
      "iteration 329, loss 0.010851361965520513\n",
      "iteration 330, loss 0.010374305401676834\n",
      "iteration 331, loss 0.009918443770952665\n",
      "iteration 332, loss 0.009483090112605647\n",
      "iteration 333, loss 0.009066711854350875\n",
      "iteration 334, loss 0.00866856272413041\n",
      "iteration 335, loss 0.008287943243239282\n",
      "iteration 336, loss 0.007924313063850318\n",
      "iteration 337, loss 0.007576673247779095\n",
      "iteration 338, loss 0.007244161407073914\n",
      "iteration 339, loss 0.006926262451961915\n",
      "iteration 340, loss 0.006622592296466511\n",
      "iteration 341, loss 0.006332182245046195\n",
      "iteration 342, loss 0.006054592481519967\n",
      "iteration 343, loss 0.005789087790992653\n",
      "iteration 344, loss 0.0055354133035596925\n",
      "iteration 345, loss 0.005292947317072591\n",
      "iteration 346, loss 0.00506113893910972\n",
      "iteration 347, loss 0.0048393785416107355\n",
      "iteration 348, loss 0.004627476119345213\n",
      "iteration 349, loss 0.004424856502929526\n",
      "iteration 350, loss 0.004231108522288481\n",
      "iteration 351, loss 0.004045901583956343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 352, loss 0.003868875353187718\n",
      "iteration 353, loss 0.0036995710574081997\n",
      "iteration 354, loss 0.0035376384170539244\n",
      "iteration 355, loss 0.0033829389284771335\n",
      "iteration 356, loss 0.003235042829697705\n",
      "iteration 357, loss 0.0030936065876860396\n",
      "iteration 358, loss 0.002958297165910378\n",
      "iteration 359, loss 0.002829001107230333\n",
      "iteration 360, loss 0.0027053299807272497\n",
      "iteration 361, loss 0.0025871272592269757\n",
      "iteration 362, loss 0.002474054226402662\n",
      "iteration 363, loss 0.0023659566557337618\n",
      "iteration 364, loss 0.0022626143673307834\n",
      "iteration 365, loss 0.0021638290809715605\n",
      "iteration 366, loss 0.002069328816086304\n",
      "iteration 367, loss 0.0019789974363336846\n",
      "iteration 368, loss 0.0018926229600031183\n",
      "iteration 369, loss 0.0018100243847241808\n",
      "iteration 370, loss 0.0017310129561394045\n",
      "iteration 371, loss 0.0016554593838388513\n",
      "iteration 372, loss 0.001583257497483751\n",
      "iteration 373, loss 0.0015142134514090394\n",
      "iteration 374, loss 0.0014481686737121775\n",
      "iteration 375, loss 0.0013849925289979\n",
      "iteration 376, loss 0.0013245944156416083\n",
      "iteration 377, loss 0.0012669101480853656\n",
      "iteration 378, loss 0.0012116925048987386\n",
      "iteration 379, loss 0.0011588540299031851\n",
      "iteration 380, loss 0.001108332720240896\n",
      "iteration 381, loss 0.0010600674095493133\n",
      "iteration 382, loss 0.001013892478687144\n",
      "iteration 383, loss 0.0009697058196074474\n",
      "iteration 384, loss 0.00092745636302291\n",
      "iteration 385, loss 0.0008870827458820991\n",
      "iteration 386, loss 0.0008484653978917626\n",
      "iteration 387, loss 0.0008115268512708443\n",
      "iteration 388, loss 0.0007761932807059064\n",
      "iteration 389, loss 0.0007424108588771224\n",
      "iteration 390, loss 0.0007101200385587628\n",
      "iteration 391, loss 0.000679208988920372\n",
      "iteration 392, loss 0.0006496494788595507\n",
      "iteration 393, loss 0.0006213811574105853\n",
      "iteration 394, loss 0.0005943688377419095\n",
      "iteration 395, loss 0.0005685128606380689\n",
      "iteration 396, loss 0.0005437840674828845\n",
      "iteration 397, loss 0.0005201414972765191\n",
      "iteration 398, loss 0.0004975513345791455\n",
      "iteration 399, loss 0.0004759177396076141\n",
      "iteration 400, loss 0.00045522270867536614\n",
      "iteration 401, loss 0.0004354319568840305\n",
      "iteration 402, loss 0.00041652213553670524\n",
      "iteration 403, loss 0.0003984245664263266\n",
      "iteration 404, loss 0.0003811079428901375\n",
      "iteration 405, loss 0.00036454522553018716\n",
      "iteration 406, loss 0.0003487222284699112\n",
      "iteration 407, loss 0.0003335800156535337\n",
      "iteration 408, loss 0.0003190950547621158\n",
      "iteration 409, loss 0.0003052334644528213\n",
      "iteration 410, loss 0.00029199053902027644\n",
      "iteration 411, loss 0.0002793185033319784\n",
      "iteration 412, loss 0.0002671885055083874\n",
      "iteration 413, loss 0.0002555881474635953\n",
      "iteration 414, loss 0.00024449849755317943\n",
      "iteration 415, loss 0.00023389457243977598\n",
      "iteration 416, loss 0.00022374368969922247\n",
      "iteration 417, loss 0.00021403546623512926\n",
      "iteration 418, loss 0.00020475876577117364\n",
      "iteration 419, loss 0.00019588348626512424\n",
      "iteration 420, loss 0.00018738674312038852\n",
      "iteration 421, loss 0.0001792576438946205\n",
      "iteration 422, loss 0.00017148333272099485\n",
      "iteration 423, loss 0.00016405541488504692\n",
      "iteration 424, loss 0.00015694227780514204\n",
      "iteration 425, loss 0.00015013721153176143\n",
      "iteration 426, loss 0.00014362756091693558\n",
      "iteration 427, loss 0.00013740920501017232\n",
      "iteration 428, loss 0.00013145669334274962\n",
      "iteration 429, loss 0.00012575993219783864\n",
      "iteration 430, loss 0.00012030971707776761\n",
      "iteration 431, loss 0.00011510319231428385\n",
      "iteration 432, loss 0.00011011569187202313\n",
      "iteration 433, loss 0.00010534481629930307\n",
      "iteration 434, loss 0.00010078188673279445\n",
      "iteration 435, loss 9.642151278020216e-05\n",
      "iteration 436, loss 9.224585389647203e-05\n",
      "iteration 437, loss 8.825089586370705e-05\n",
      "iteration 438, loss 8.44320767954259e-05\n",
      "iteration 439, loss 8.078019509612783e-05\n",
      "iteration 440, loss 7.728361816714967e-05\n",
      "iteration 441, loss 7.3938400576988e-05\n",
      "iteration 442, loss 7.073802663765467e-05\n",
      "iteration 443, loss 6.767975374833298e-05\n",
      "iteration 444, loss 6.475157357049776e-05\n",
      "iteration 445, loss 6.194931649943064e-05\n",
      "iteration 446, loss 5.926843979173309e-05\n",
      "iteration 447, loss 5.6706999461701466e-05\n",
      "iteration 448, loss 5.4256371187951385e-05\n",
      "iteration 449, loss 5.1909140471441845e-05\n",
      "iteration 450, loss 4.966363990267997e-05\n",
      "iteration 451, loss 4.751784911365546e-05\n",
      "iteration 452, loss 4.546389266028091e-05\n",
      "iteration 453, loss 4.349762340925843e-05\n",
      "iteration 454, loss 4.161681613183123e-05\n",
      "iteration 455, loss 3.981941034652883e-05\n",
      "iteration 456, loss 3.809867799211509e-05\n",
      "iteration 457, loss 3.6451875832706226e-05\n",
      "iteration 458, loss 3.4877194840016264e-05\n",
      "iteration 459, loss 3.337103811763942e-05\n",
      "iteration 460, loss 3.192960059035095e-05\n",
      "iteration 461, loss 3.05501542330575e-05\n",
      "iteration 462, loss 2.9230167510218824e-05\n",
      "iteration 463, loss 2.796840203123226e-05\n",
      "iteration 464, loss 2.6760613828757556e-05\n",
      "iteration 465, loss 2.5604816891499852e-05\n",
      "iteration 466, loss 2.4498760311661728e-05\n",
      "iteration 467, loss 2.3441818912687053e-05\n",
      "iteration 468, loss 2.2430365885576598e-05\n",
      "iteration 469, loss 2.1461755547391565e-05\n",
      "iteration 470, loss 2.053502685105104e-05\n",
      "iteration 471, loss 1.9649114826682465e-05\n",
      "iteration 472, loss 1.8801222036904764e-05\n",
      "iteration 473, loss 1.7989535312793313e-05\n",
      "iteration 474, loss 1.7213099509561865e-05\n",
      "iteration 475, loss 1.6470792501918653e-05\n",
      "iteration 476, loss 1.576024888621244e-05\n",
      "iteration 477, loss 1.5080332949284931e-05\n",
      "iteration 478, loss 1.442976072614921e-05\n",
      "iteration 479, loss 1.3807657952087448e-05\n",
      "iteration 480, loss 1.3212250141745565e-05\n",
      "iteration 481, loss 1.2642234681860108e-05\n",
      "iteration 482, loss 1.2096866166005728e-05\n",
      "iteration 483, loss 1.157548857910526e-05\n",
      "iteration 484, loss 1.1076437215979575e-05\n",
      "iteration 485, loss 1.0598757571883986e-05\n",
      "iteration 486, loss 1.0141750703686016e-05\n",
      "iteration 487, loss 9.705059866625969e-06\n",
      "iteration 488, loss 9.28683418168901e-06\n",
      "iteration 489, loss 8.886396293335371e-06\n",
      "iteration 490, loss 8.503270902120858e-06\n",
      "iteration 491, loss 8.136981092102871e-06\n",
      "iteration 492, loss 7.786447324831415e-06\n",
      "iteration 493, loss 7.450822045988844e-06\n",
      "iteration 494, loss 7.12968518483482e-06\n",
      "iteration 495, loss 6.822654365800687e-06\n",
      "iteration 496, loss 6.528881123441946e-06\n",
      "iteration 497, loss 6.247637747259678e-06\n",
      "iteration 498, loss 5.978478899315651e-06\n",
      "iteration 499, loss 5.72106893968571e-06\n"
     ]
    }
   ],
   "source": [
    "# This is a good exercise to create a simple network, forward pass and backward pass all in numpy\n",
    "\n",
    "# We will use a fully-connected ReLU network as our running example. \n",
    "# The network will have a single hidden layer, and will be trained with gradient descent to fit random data \n",
    "# by minimizing the Euclidean distance between the network output and the true output.\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "W1 = np.random.randn(D_in, H)\n",
    "W2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    z = np.matmul(x,W1)\n",
    "    h = np.maximum(z,0) # ReLU(z)\n",
    "    y_pred = np.matmul(h, W2)\n",
    "    \n",
    "    loss = np.square(y - y_pred).sum()\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss))\n",
    "    \n",
    "    # Backward pass\n",
    "    dy_pred = 2*(y_pred - y)\n",
    "    dW2 = np.matmul(h.T, dy_pred)\n",
    "    dh = np.matmul(dy_pred, W2.T)\n",
    "    dz = np.copy(dh)\n",
    "    dz[h <= 0] = 0 # elementwise signum function\n",
    "    dW1 = np.matmul(x.T, dz)\n",
    "    \n",
    "    # Upgrade weights\n",
    "    W2 -= learning_rate * dW2\n",
    "    W1 -= learning_rate * dW1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pytorch: tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 29840098.0\n",
      "iteration 1, loss 25132306.0\n",
      "iteration 2, loss 23104618.0\n",
      "iteration 3, loss 20635870.0\n",
      "iteration 4, loss 16945792.0\n",
      "iteration 5, loss 12462604.0\n",
      "iteration 6, loss 8392902.0\n",
      "iteration 7, loss 5364406.5\n",
      "iteration 8, loss 3420446.5\n",
      "iteration 9, loss 2250130.5\n",
      "iteration 10, loss 1561521.375\n",
      "iteration 11, loss 1147531.125\n",
      "iteration 12, loss 887931.75\n",
      "iteration 13, loss 715402.6875\n",
      "iteration 14, loss 593580.25\n",
      "iteration 15, loss 502751.4375\n",
      "iteration 16, loss 432092.25\n",
      "iteration 17, loss 375228.375\n",
      "iteration 18, loss 328332.15625\n",
      "iteration 19, loss 289108.84375\n",
      "iteration 20, loss 255893.953125\n",
      "iteration 21, loss 227396.875\n",
      "iteration 22, loss 202823.640625\n",
      "iteration 23, loss 181465.765625\n",
      "iteration 24, loss 162798.328125\n",
      "iteration 25, loss 146422.6875\n",
      "iteration 26, loss 132015.71875\n",
      "iteration 27, loss 119299.4453125\n",
      "iteration 28, loss 108034.640625\n",
      "iteration 29, loss 98019.734375\n",
      "iteration 30, loss 89102.0390625\n",
      "iteration 31, loss 81140.578125\n",
      "iteration 32, loss 74016.71875\n",
      "iteration 33, loss 67627.296875\n",
      "iteration 34, loss 61884.53125\n",
      "iteration 35, loss 56701.25390625\n",
      "iteration 36, loss 52028.8515625\n",
      "iteration 37, loss 47809.47265625\n",
      "iteration 38, loss 43995.1328125\n",
      "iteration 39, loss 40537.62890625\n",
      "iteration 40, loss 37398.828125\n",
      "iteration 41, loss 34545.18359375\n",
      "iteration 42, loss 31947.9765625\n",
      "iteration 43, loss 29579.29296875\n",
      "iteration 44, loss 27415.046875\n",
      "iteration 45, loss 25436.6796875\n",
      "iteration 46, loss 23624.96875\n",
      "iteration 47, loss 21963.767578125\n",
      "iteration 48, loss 20439.44921875\n",
      "iteration 49, loss 19038.6953125\n",
      "iteration 50, loss 17749.6953125\n",
      "iteration 51, loss 16562.537109375\n",
      "iteration 52, loss 15467.8427734375\n",
      "iteration 53, loss 14457.7763671875\n",
      "iteration 54, loss 13524.0234375\n",
      "iteration 55, loss 12660.904296875\n",
      "iteration 56, loss 11862.693359375\n",
      "iteration 57, loss 11122.8515625\n",
      "iteration 58, loss 10436.7802734375\n",
      "iteration 59, loss 9799.2998046875\n",
      "iteration 60, loss 9207.6103515625\n",
      "iteration 61, loss 8657.5107421875\n",
      "iteration 62, loss 8145.1220703125\n",
      "iteration 63, loss 7667.779296875\n",
      "iteration 64, loss 7222.7734375\n",
      "iteration 65, loss 6807.51953125\n",
      "iteration 66, loss 6419.92138671875\n",
      "iteration 67, loss 6058.25927734375\n",
      "iteration 68, loss 5720.4873046875\n",
      "iteration 69, loss 5404.08642578125\n",
      "iteration 70, loss 5107.8076171875\n",
      "iteration 71, loss 4830.20751953125\n",
      "iteration 72, loss 4569.92333984375\n",
      "iteration 73, loss 4325.71630859375\n",
      "iteration 74, loss 4096.7607421875\n",
      "iteration 75, loss 3881.660400390625\n",
      "iteration 76, loss 3679.442138671875\n",
      "iteration 77, loss 3489.20947265625\n",
      "iteration 78, loss 3310.199462890625\n",
      "iteration 79, loss 3141.619140625\n",
      "iteration 80, loss 2982.833984375\n",
      "iteration 81, loss 2833.29443359375\n",
      "iteration 82, loss 2692.275146484375\n",
      "iteration 83, loss 2559.189208984375\n",
      "iteration 84, loss 2433.600830078125\n",
      "iteration 85, loss 2315.093017578125\n",
      "iteration 86, loss 2202.972412109375\n",
      "iteration 87, loss 2096.999267578125\n",
      "iteration 88, loss 1996.83447265625\n",
      "iteration 89, loss 1902.04833984375\n",
      "iteration 90, loss 1812.30322265625\n",
      "iteration 91, loss 1727.3822021484375\n",
      "iteration 92, loss 1646.975830078125\n",
      "iteration 93, loss 1570.7420654296875\n",
      "iteration 94, loss 1498.5009765625\n",
      "iteration 95, loss 1430.0599365234375\n",
      "iteration 96, loss 1365.045166015625\n",
      "iteration 97, loss 1303.33154296875\n",
      "iteration 98, loss 1244.70361328125\n",
      "iteration 99, loss 1189.073974609375\n",
      "iteration 100, loss 1136.3919677734375\n",
      "iteration 101, loss 1086.3155517578125\n",
      "iteration 102, loss 1038.6396484375\n",
      "iteration 103, loss 993.3048706054688\n",
      "iteration 104, loss 950.2081298828125\n",
      "iteration 105, loss 909.1390380859375\n",
      "iteration 106, loss 870.0211791992188\n",
      "iteration 107, loss 832.7844848632812\n",
      "iteration 108, loss 797.3052978515625\n",
      "iteration 109, loss 763.4751586914062\n",
      "iteration 110, loss 731.2189331054688\n",
      "iteration 111, loss 700.4775390625\n",
      "iteration 112, loss 671.1449584960938\n",
      "iteration 113, loss 643.1890869140625\n",
      "iteration 114, loss 616.4970092773438\n",
      "iteration 115, loss 591.001220703125\n",
      "iteration 116, loss 566.6552124023438\n",
      "iteration 117, loss 543.4100952148438\n",
      "iteration 118, loss 521.1985473632812\n",
      "iteration 119, loss 499.9826354980469\n",
      "iteration 120, loss 479.7138671875\n",
      "iteration 121, loss 460.32391357421875\n",
      "iteration 122, loss 441.80377197265625\n",
      "iteration 123, loss 424.0702819824219\n",
      "iteration 124, loss 407.1065979003906\n",
      "iteration 125, loss 390.8834228515625\n",
      "iteration 126, loss 375.3635559082031\n",
      "iteration 127, loss 360.50897216796875\n",
      "iteration 128, loss 346.277099609375\n",
      "iteration 129, loss 332.6568298339844\n",
      "iteration 130, loss 319.61248779296875\n",
      "iteration 131, loss 307.1274719238281\n",
      "iteration 132, loss 295.1568298339844\n",
      "iteration 133, loss 283.6897277832031\n",
      "iteration 134, loss 272.6985778808594\n",
      "iteration 135, loss 262.1633605957031\n",
      "iteration 136, loss 252.0569610595703\n",
      "iteration 137, loss 242.36891174316406\n",
      "iteration 138, loss 233.08251953125\n",
      "iteration 139, loss 224.18421936035156\n",
      "iteration 140, loss 215.6448211669922\n",
      "iteration 141, loss 207.44337463378906\n",
      "iteration 142, loss 199.5738983154297\n",
      "iteration 143, loss 192.0283203125\n",
      "iteration 144, loss 184.77957153320312\n",
      "iteration 145, loss 177.82339477539062\n",
      "iteration 146, loss 171.14695739746094\n",
      "iteration 147, loss 164.73504638671875\n",
      "iteration 148, loss 158.57989501953125\n",
      "iteration 149, loss 152.66525268554688\n",
      "iteration 150, loss 146.9800567626953\n",
      "iteration 151, loss 141.5233154296875\n",
      "iteration 152, loss 136.27972412109375\n",
      "iteration 153, loss 131.24331665039062\n",
      "iteration 154, loss 126.39928436279297\n",
      "iteration 155, loss 121.74429321289062\n",
      "iteration 156, loss 117.26716613769531\n",
      "iteration 157, loss 112.9684066772461\n",
      "iteration 158, loss 108.83486938476562\n",
      "iteration 159, loss 104.85810089111328\n",
      "iteration 160, loss 101.03531646728516\n",
      "iteration 161, loss 97.35588073730469\n",
      "iteration 162, loss 93.819091796875\n",
      "iteration 163, loss 90.41301727294922\n",
      "iteration 164, loss 87.13748168945312\n",
      "iteration 165, loss 83.98712921142578\n",
      "iteration 166, loss 80.95771789550781\n",
      "iteration 167, loss 78.0455551147461\n",
      "iteration 168, loss 75.23774719238281\n",
      "iteration 169, loss 72.53588104248047\n",
      "iteration 170, loss 69.93557739257812\n",
      "iteration 171, loss 67.43334197998047\n",
      "iteration 172, loss 65.0223159790039\n",
      "iteration 173, loss 62.70306396484375\n",
      "iteration 174, loss 60.47007369995117\n",
      "iteration 175, loss 58.322784423828125\n",
      "iteration 176, loss 56.25173568725586\n",
      "iteration 177, loss 54.25730895996094\n",
      "iteration 178, loss 52.334861755371094\n",
      "iteration 179, loss 50.48408508300781\n",
      "iteration 180, loss 48.70068359375\n",
      "iteration 181, loss 46.982757568359375\n",
      "iteration 182, loss 45.329288482666016\n",
      "iteration 183, loss 43.73517990112305\n",
      "iteration 184, loss 42.19951248168945\n",
      "iteration 185, loss 40.71955108642578\n",
      "iteration 186, loss 39.29265213012695\n",
      "iteration 187, loss 37.917030334472656\n",
      "iteration 188, loss 36.591026306152344\n",
      "iteration 189, loss 35.31315231323242\n",
      "iteration 190, loss 34.08304977416992\n",
      "iteration 191, loss 32.896671295166016\n",
      "iteration 192, loss 31.751773834228516\n",
      "iteration 193, loss 30.647682189941406\n",
      "iteration 194, loss 29.584646224975586\n",
      "iteration 195, loss 28.559053421020508\n",
      "iteration 196, loss 27.568822860717773\n",
      "iteration 197, loss 26.615646362304688\n",
      "iteration 198, loss 25.696277618408203\n",
      "iteration 199, loss 24.808290481567383\n",
      "iteration 200, loss 23.952560424804688\n",
      "iteration 201, loss 23.12655258178711\n",
      "iteration 202, loss 22.330669403076172\n",
      "iteration 203, loss 21.562320709228516\n",
      "iteration 204, loss 20.821882247924805\n",
      "iteration 205, loss 20.107900619506836\n",
      "iteration 206, loss 19.41835594177246\n",
      "iteration 207, loss 18.752735137939453\n",
      "iteration 208, loss 18.110618591308594\n",
      "iteration 209, loss 17.490737915039062\n",
      "iteration 210, loss 16.89293098449707\n",
      "iteration 211, loss 16.315643310546875\n",
      "iteration 212, loss 15.759255409240723\n",
      "iteration 213, loss 15.22297191619873\n",
      "iteration 214, loss 14.70488452911377\n",
      "iteration 215, loss 14.203744888305664\n",
      "iteration 216, loss 13.720457077026367\n",
      "iteration 217, loss 13.254615783691406\n",
      "iteration 218, loss 12.804057121276855\n",
      "iteration 219, loss 12.369648933410645\n",
      "iteration 220, loss 11.950518608093262\n",
      "iteration 221, loss 11.545775413513184\n",
      "iteration 222, loss 11.155194282531738\n",
      "iteration 223, loss 10.777684211730957\n",
      "iteration 224, loss 10.413540840148926\n",
      "iteration 225, loss 10.061506271362305\n",
      "iteration 226, loss 9.721817016601562\n",
      "iteration 227, loss 9.393730163574219\n",
      "iteration 228, loss 9.076964378356934\n",
      "iteration 229, loss 8.771417617797852\n",
      "iteration 230, loss 8.476161003112793\n",
      "iteration 231, loss 8.191152572631836\n",
      "iteration 232, loss 7.91547966003418\n",
      "iteration 233, loss 7.649803638458252\n",
      "iteration 234, loss 7.3927764892578125\n",
      "iteration 235, loss 7.145003795623779\n",
      "iteration 236, loss 6.905197620391846\n",
      "iteration 237, loss 6.6737494468688965\n",
      "iteration 238, loss 6.450079917907715\n",
      "iteration 239, loss 6.234184741973877\n",
      "iteration 240, loss 6.025290489196777\n",
      "iteration 241, loss 5.823901653289795\n",
      "iteration 242, loss 5.62931489944458\n",
      "iteration 243, loss 5.440932273864746\n",
      "iteration 244, loss 5.259303092956543\n",
      "iteration 245, loss 5.084109783172607\n",
      "iteration 246, loss 4.914543628692627\n",
      "iteration 247, loss 4.750715732574463\n",
      "iteration 248, loss 4.592525482177734\n",
      "iteration 249, loss 4.439380645751953\n",
      "iteration 250, loss 4.291619777679443\n",
      "iteration 251, loss 4.1489787101745605\n",
      "iteration 252, loss 4.010787487030029\n",
      "iteration 253, loss 3.877544641494751\n",
      "iteration 254, loss 3.7486116886138916\n",
      "iteration 255, loss 3.6241822242736816\n",
      "iteration 256, loss 3.5040745735168457\n",
      "iteration 257, loss 3.387749195098877\n",
      "iteration 258, loss 3.275648593902588\n",
      "iteration 259, loss 3.1669161319732666\n",
      "iteration 260, loss 3.061896800994873\n",
      "iteration 261, loss 2.960484743118286\n",
      "iteration 262, loss 2.862441301345825\n",
      "iteration 263, loss 2.7676329612731934\n",
      "iteration 264, loss 2.6762006282806396\n",
      "iteration 265, loss 2.587830066680908\n",
      "iteration 266, loss 2.502189874649048\n",
      "iteration 267, loss 2.4195492267608643\n",
      "iteration 268, loss 2.3396575450897217\n",
      "iteration 269, loss 2.26253342628479\n",
      "iteration 270, loss 2.187731981277466\n",
      "iteration 271, loss 2.115607738494873\n",
      "iteration 272, loss 2.0457942485809326\n",
      "iteration 273, loss 1.978595495223999\n",
      "iteration 274, loss 1.913254737854004\n",
      "iteration 275, loss 1.8502360582351685\n",
      "iteration 276, loss 1.7892420291900635\n",
      "iteration 277, loss 1.7303210496902466\n",
      "iteration 278, loss 1.6734212636947632\n",
      "iteration 279, loss 1.6185098886489868\n",
      "iteration 280, loss 1.5652586221694946\n",
      "iteration 281, loss 1.5138020515441895\n",
      "iteration 282, loss 1.4641633033752441\n",
      "iteration 283, loss 1.416006088256836\n",
      "iteration 284, loss 1.3694922924041748\n",
      "iteration 285, loss 1.3244935274124146\n",
      "iteration 286, loss 1.281050205230713\n",
      "iteration 287, loss 1.2389709949493408\n",
      "iteration 288, loss 1.1983857154846191\n",
      "iteration 289, loss 1.1591014862060547\n",
      "iteration 290, loss 1.1210520267486572\n",
      "iteration 291, loss 1.084389328956604\n",
      "iteration 292, loss 1.0489469766616821\n",
      "iteration 293, loss 1.0145573616027832\n",
      "iteration 294, loss 0.9813918471336365\n",
      "iteration 295, loss 0.9492610096931458\n",
      "iteration 296, loss 0.9181467890739441\n",
      "iteration 297, loss 0.8880310654640198\n",
      "iteration 298, loss 0.8590739965438843\n",
      "iteration 299, loss 0.8309704065322876\n",
      "iteration 300, loss 0.8038944005966187\n",
      "iteration 301, loss 0.7774818539619446\n",
      "iteration 302, loss 0.75212562084198\n",
      "iteration 303, loss 0.7275790572166443\n",
      "iteration 304, loss 0.7038360238075256\n",
      "iteration 305, loss 0.6807970404624939\n",
      "iteration 306, loss 0.6585623025894165\n",
      "iteration 307, loss 0.6371458768844604\n",
      "iteration 308, loss 0.6163250207901001\n",
      "iteration 309, loss 0.5962913632392883\n",
      "iteration 310, loss 0.5768395662307739\n",
      "iteration 311, loss 0.5580021142959595\n",
      "iteration 312, loss 0.5398076772689819\n",
      "iteration 313, loss 0.5221596360206604\n",
      "iteration 314, loss 0.5052104592323303\n",
      "iteration 315, loss 0.48871758580207825\n",
      "iteration 316, loss 0.4728550612926483\n",
      "iteration 317, loss 0.45748987793922424\n",
      "iteration 318, loss 0.4425833821296692\n",
      "iteration 319, loss 0.42821845412254333\n",
      "iteration 320, loss 0.41428545117378235\n",
      "iteration 321, loss 0.4008563458919525\n",
      "iteration 322, loss 0.3877924978733063\n",
      "iteration 323, loss 0.37511518597602844\n",
      "iteration 324, loss 0.36299359798431396\n",
      "iteration 325, loss 0.35115885734558105\n",
      "iteration 326, loss 0.3397556245326996\n",
      "iteration 327, loss 0.32870566844940186\n",
      "iteration 328, loss 0.31804969906806946\n",
      "iteration 329, loss 0.3077273964881897\n",
      "iteration 330, loss 0.29775485396385193\n",
      "iteration 331, loss 0.28806835412979126\n",
      "iteration 332, loss 0.27877190709114075\n",
      "iteration 333, loss 0.2696819007396698\n",
      "iteration 334, loss 0.26090842485427856\n",
      "iteration 335, loss 0.2524634599685669\n",
      "iteration 336, loss 0.24430714547634125\n",
      "iteration 337, loss 0.23639357089996338\n",
      "iteration 338, loss 0.2287275195121765\n",
      "iteration 339, loss 0.2213437259197235\n",
      "iteration 340, loss 0.2141285389661789\n",
      "iteration 341, loss 0.20722849667072296\n",
      "iteration 342, loss 0.2004939615726471\n",
      "iteration 343, loss 0.1939646154642105\n",
      "iteration 344, loss 0.18771888315677643\n",
      "iteration 345, loss 0.18159537017345428\n",
      "iteration 346, loss 0.17573074996471405\n",
      "iteration 347, loss 0.170033261179924\n",
      "iteration 348, loss 0.16457176208496094\n",
      "iteration 349, loss 0.15919822454452515\n",
      "iteration 350, loss 0.1540510207414627\n",
      "iteration 351, loss 0.14907428622245789\n",
      "iteration 352, loss 0.14426620304584503\n",
      "iteration 353, loss 0.13961726427078247\n",
      "iteration 354, loss 0.13511569797992706\n",
      "iteration 355, loss 0.1307411640882492\n",
      "iteration 356, loss 0.12652179598808289\n",
      "iteration 357, loss 0.12245923280715942\n",
      "iteration 358, loss 0.11849271506071091\n",
      "iteration 359, loss 0.11467066407203674\n",
      "iteration 360, loss 0.1109810322523117\n",
      "iteration 361, loss 0.10738570243120193\n",
      "iteration 362, loss 0.10392741858959198\n",
      "iteration 363, loss 0.10060682892799377\n",
      "iteration 364, loss 0.09734249860048294\n",
      "iteration 365, loss 0.09421560168266296\n",
      "iteration 366, loss 0.09119370579719543\n",
      "iteration 367, loss 0.08825129270553589\n",
      "iteration 368, loss 0.08540467172861099\n",
      "iteration 369, loss 0.08264648169279099\n",
      "iteration 370, loss 0.07999012619256973\n",
      "iteration 371, loss 0.0774109810590744\n",
      "iteration 372, loss 0.07494254410266876\n",
      "iteration 373, loss 0.07250241935253143\n",
      "iteration 374, loss 0.07018277049064636\n",
      "iteration 375, loss 0.06794168055057526\n",
      "iteration 376, loss 0.06575240194797516\n",
      "iteration 377, loss 0.06364620476961136\n",
      "iteration 378, loss 0.06160132959485054\n",
      "iteration 379, loss 0.059620827436447144\n",
      "iteration 380, loss 0.057724449783563614\n",
      "iteration 381, loss 0.055848147720098495\n",
      "iteration 382, loss 0.054061997681856155\n",
      "iteration 383, loss 0.052329614758491516\n",
      "iteration 384, loss 0.050645407289266586\n",
      "iteration 385, loss 0.04902881756424904\n",
      "iteration 386, loss 0.047456927597522736\n",
      "iteration 387, loss 0.045947760343551636\n",
      "iteration 388, loss 0.044468484818935394\n",
      "iteration 389, loss 0.04304053261876106\n",
      "iteration 390, loss 0.04164911434054375\n",
      "iteration 391, loss 0.040339890867471695\n",
      "iteration 392, loss 0.03904943913221359\n",
      "iteration 393, loss 0.03780635818839073\n",
      "iteration 394, loss 0.03659839183092117\n",
      "iteration 395, loss 0.03542105481028557\n",
      "iteration 396, loss 0.03429286926984787\n",
      "iteration 397, loss 0.03321472927927971\n",
      "iteration 398, loss 0.032146356999874115\n",
      "iteration 399, loss 0.031120995059609413\n",
      "iteration 400, loss 0.03012801520526409\n",
      "iteration 401, loss 0.029174968600273132\n",
      "iteration 402, loss 0.02824598178267479\n",
      "iteration 403, loss 0.027353553101420403\n",
      "iteration 404, loss 0.026481859385967255\n",
      "iteration 405, loss 0.025634443387389183\n",
      "iteration 406, loss 0.02482616901397705\n",
      "iteration 407, loss 0.0240411888808012\n",
      "iteration 408, loss 0.02327655628323555\n",
      "iteration 409, loss 0.022540628910064697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 410, loss 0.02181771583855152\n",
      "iteration 411, loss 0.021132128313183784\n",
      "iteration 412, loss 0.020472481846809387\n",
      "iteration 413, loss 0.01981993019580841\n",
      "iteration 414, loss 0.019197581335902214\n",
      "iteration 415, loss 0.018587276339530945\n",
      "iteration 416, loss 0.017997348681092262\n",
      "iteration 417, loss 0.017429586499929428\n",
      "iteration 418, loss 0.016886014491319656\n",
      "iteration 419, loss 0.016340669244527817\n",
      "iteration 420, loss 0.015828918665647507\n",
      "iteration 421, loss 0.01533582340925932\n",
      "iteration 422, loss 0.014862775802612305\n",
      "iteration 423, loss 0.014388865791261196\n",
      "iteration 424, loss 0.01393766701221466\n",
      "iteration 425, loss 0.013503475114703178\n",
      "iteration 426, loss 0.013088960200548172\n",
      "iteration 427, loss 0.012679187580943108\n",
      "iteration 428, loss 0.012285293079912663\n",
      "iteration 429, loss 0.011903454549610615\n",
      "iteration 430, loss 0.01152788195759058\n",
      "iteration 431, loss 0.011171871796250343\n",
      "iteration 432, loss 0.010821240022778511\n",
      "iteration 433, loss 0.010487817227840424\n",
      "iteration 434, loss 0.010163136757910252\n",
      "iteration 435, loss 0.009843103587627411\n",
      "iteration 436, loss 0.009550357237458229\n",
      "iteration 437, loss 0.009251197800040245\n",
      "iteration 438, loss 0.008970252238214016\n",
      "iteration 439, loss 0.008694300428032875\n",
      "iteration 440, loss 0.008427818305790424\n",
      "iteration 441, loss 0.008172917179763317\n",
      "iteration 442, loss 0.00791920255869627\n",
      "iteration 443, loss 0.0076746707782149315\n",
      "iteration 444, loss 0.0074403127655386925\n",
      "iteration 445, loss 0.007217026315629482\n",
      "iteration 446, loss 0.006995806936174631\n",
      "iteration 447, loss 0.006786619778722525\n",
      "iteration 448, loss 0.006583823822438717\n",
      "iteration 449, loss 0.0063851322047412395\n",
      "iteration 450, loss 0.006193017121404409\n",
      "iteration 451, loss 0.006008613854646683\n",
      "iteration 452, loss 0.005822719074785709\n",
      "iteration 453, loss 0.005646159872412682\n",
      "iteration 454, loss 0.005478351842612028\n",
      "iteration 455, loss 0.005314812064170837\n",
      "iteration 456, loss 0.005156926345080137\n",
      "iteration 457, loss 0.0050110770389437675\n",
      "iteration 458, loss 0.004858296364545822\n",
      "iteration 459, loss 0.004720750264823437\n",
      "iteration 460, loss 0.004578078631311655\n",
      "iteration 461, loss 0.004440611228346825\n",
      "iteration 462, loss 0.004312070552259684\n",
      "iteration 463, loss 0.00418442627415061\n",
      "iteration 464, loss 0.004060246050357819\n",
      "iteration 465, loss 0.00394421024248004\n",
      "iteration 466, loss 0.0038309439551085234\n",
      "iteration 467, loss 0.0037231806199997663\n",
      "iteration 468, loss 0.0036132452078163624\n",
      "iteration 469, loss 0.0035091231111437082\n",
      "iteration 470, loss 0.00340917706489563\n",
      "iteration 471, loss 0.0033131393138319254\n",
      "iteration 472, loss 0.003215955337509513\n",
      "iteration 473, loss 0.0031235041096806526\n",
      "iteration 474, loss 0.0030365693382918835\n",
      "iteration 475, loss 0.0029504001140594482\n",
      "iteration 476, loss 0.002867419272661209\n",
      "iteration 477, loss 0.0027886051684617996\n",
      "iteration 478, loss 0.0027080124709755182\n",
      "iteration 479, loss 0.0026325867511332035\n",
      "iteration 480, loss 0.002561824629083276\n",
      "iteration 481, loss 0.00249094539321959\n",
      "iteration 482, loss 0.0024220196064561605\n",
      "iteration 483, loss 0.0023566349409520626\n",
      "iteration 484, loss 0.0022908246610313654\n",
      "iteration 485, loss 0.002229643752798438\n",
      "iteration 486, loss 0.0021703944075852633\n",
      "iteration 487, loss 0.0021127057261765003\n",
      "iteration 488, loss 0.0020545003935694695\n",
      "iteration 489, loss 0.0019987632986158133\n",
      "iteration 490, loss 0.0019450505496934056\n",
      "iteration 491, loss 0.0018942400347441435\n",
      "iteration 492, loss 0.001842196797952056\n",
      "iteration 493, loss 0.0017935538198798895\n",
      "iteration 494, loss 0.0017493364866822958\n",
      "iteration 495, loss 0.0017044928390532732\n",
      "iteration 496, loss 0.0016599004156887531\n",
      "iteration 497, loss 0.0016154174227267504\n",
      "iteration 498, loss 0.0015766264405101538\n",
      "iteration 499, loss 0.0015373863279819489\n"
     ]
    }
   ],
   "source": [
    "# implementing the same example in torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda\")\n",
    "datatype = torch.float\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "W1 = torch.randn(D_in, H, dtype=datatype, device=device)\n",
    "W2 = torch.randn(H, D_out, dtype=datatype, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    z = x.mm(W1)\n",
    "    h = z.clamp(min=0) # ReLU(z)\n",
    "    y_pred = h.mm(W2)\n",
    "    \n",
    "    loss = (y - y_pred).pow(2).sum().item()\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss))\n",
    "    \n",
    "    # Backward pass\n",
    "    dy_pred = 2*(y_pred - y)\n",
    "    dW2 = h.t().mm(dy_pred)\n",
    "    dh = dy_pred.mm(W2.t())\n",
    "    dz = dh.clone()\n",
    "    dz[h <= 0] = 0 # elementwise signum function\n",
    "    dW1 = x.t().mm(dz)\n",
    "    \n",
    "    # Upgrade weights\n",
    "    W2 -= learning_rate * dW2\n",
    "    W1 -= learning_rate * dW1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PyTorch: Tensors and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 27654396.0\n",
      "iteration 1, loss 26002932.0\n",
      "iteration 2, loss 29281282.0\n",
      "iteration 3, loss 32977422.0\n",
      "iteration 4, loss 32428502.0\n",
      "iteration 5, loss 25427372.0\n",
      "iteration 6, loss 15606912.0\n",
      "iteration 7, loss 7964821.0\n",
      "iteration 8, loss 3845618.25\n",
      "iteration 9, loss 1998753.5\n",
      "iteration 10, loss 1202566.75\n",
      "iteration 11, loss 835005.25\n",
      "iteration 12, loss 640811.3125\n",
      "iteration 13, loss 520708.25\n",
      "iteration 14, loss 436216.8125\n",
      "iteration 15, loss 371594.84375\n",
      "iteration 16, loss 319785.875\n",
      "iteration 17, loss 277117.78125\n",
      "iteration 18, loss 241437.71875\n",
      "iteration 19, loss 211293.390625\n",
      "iteration 20, loss 185650.703125\n",
      "iteration 21, loss 163716.859375\n",
      "iteration 22, loss 144910.6875\n",
      "iteration 23, loss 128678.3359375\n",
      "iteration 24, loss 114584.5234375\n",
      "iteration 25, loss 102303.171875\n",
      "iteration 26, loss 91561.8203125\n",
      "iteration 27, loss 82137.9375\n",
      "iteration 28, loss 73853.71875\n",
      "iteration 29, loss 66542.03125\n",
      "iteration 30, loss 60065.23046875\n",
      "iteration 31, loss 54323.29296875\n",
      "iteration 32, loss 49210.01171875\n",
      "iteration 33, loss 44659.69921875\n",
      "iteration 34, loss 40590.65625\n",
      "iteration 35, loss 36942.33984375\n",
      "iteration 36, loss 33666.4453125\n",
      "iteration 37, loss 30719.25\n",
      "iteration 38, loss 28063.01953125\n",
      "iteration 39, loss 25664.666015625\n",
      "iteration 40, loss 23494.947265625\n",
      "iteration 41, loss 21529.501953125\n",
      "iteration 42, loss 19746.974609375\n",
      "iteration 43, loss 18127.865234375\n",
      "iteration 44, loss 16655.931640625\n",
      "iteration 45, loss 15315.728515625\n",
      "iteration 46, loss 14093.6552734375\n",
      "iteration 47, loss 12978.9072265625\n",
      "iteration 48, loss 11960.5673828125\n",
      "iteration 49, loss 11029.908203125\n",
      "iteration 50, loss 10178.0\n",
      "iteration 51, loss 9397.6865234375\n",
      "iteration 52, loss 8682.3310546875\n",
      "iteration 53, loss 8026.02001953125\n",
      "iteration 54, loss 7423.4169921875\n",
      "iteration 55, loss 6869.7841796875\n",
      "iteration 56, loss 6360.84814453125\n",
      "iteration 57, loss 5892.51806640625\n",
      "iteration 58, loss 5461.1904296875\n",
      "iteration 59, loss 5064.01611328125\n",
      "iteration 60, loss 4697.90478515625\n",
      "iteration 61, loss 4360.14306640625\n",
      "iteration 62, loss 4048.382080078125\n",
      "iteration 63, loss 3760.501220703125\n",
      "iteration 64, loss 3494.44189453125\n",
      "iteration 65, loss 3248.549560546875\n",
      "iteration 66, loss 3021.113525390625\n",
      "iteration 67, loss 2810.712890625\n",
      "iteration 68, loss 2616.005859375\n",
      "iteration 69, loss 2435.639892578125\n",
      "iteration 70, loss 2268.463134765625\n",
      "iteration 71, loss 2113.501708984375\n",
      "iteration 72, loss 1969.802001953125\n",
      "iteration 73, loss 1836.4451904296875\n",
      "iteration 74, loss 1712.6632080078125\n",
      "iteration 75, loss 1597.7025146484375\n",
      "iteration 76, loss 1490.9263916015625\n",
      "iteration 77, loss 1391.708251953125\n",
      "iteration 78, loss 1299.5130615234375\n",
      "iteration 79, loss 1213.74658203125\n",
      "iteration 80, loss 1133.9537353515625\n",
      "iteration 81, loss 1059.7069091796875\n",
      "iteration 82, loss 990.5829467773438\n",
      "iteration 83, loss 926.2100830078125\n",
      "iteration 84, loss 866.2494506835938\n",
      "iteration 85, loss 810.3851928710938\n",
      "iteration 86, loss 758.326416015625\n",
      "iteration 87, loss 709.8340454101562\n",
      "iteration 88, loss 664.59521484375\n",
      "iteration 89, loss 622.375\n",
      "iteration 90, loss 582.989013671875\n",
      "iteration 91, loss 546.2074584960938\n",
      "iteration 92, loss 511.8606262207031\n",
      "iteration 93, loss 479.78887939453125\n",
      "iteration 94, loss 449.8230285644531\n",
      "iteration 95, loss 421.8215026855469\n",
      "iteration 96, loss 395.6493225097656\n",
      "iteration 97, loss 371.17974853515625\n",
      "iteration 98, loss 348.3027038574219\n",
      "iteration 99, loss 326.8929748535156\n",
      "iteration 100, loss 306.8616027832031\n",
      "iteration 101, loss 288.1205749511719\n",
      "iteration 102, loss 270.5769958496094\n",
      "iteration 103, loss 254.15782165527344\n",
      "iteration 104, loss 238.77090454101562\n",
      "iteration 105, loss 224.3551025390625\n",
      "iteration 106, loss 210.85284423828125\n",
      "iteration 107, loss 198.19561767578125\n",
      "iteration 108, loss 186.3347625732422\n",
      "iteration 109, loss 175.21656799316406\n",
      "iteration 110, loss 164.78981018066406\n",
      "iteration 111, loss 155.00843811035156\n",
      "iteration 112, loss 145.83346557617188\n",
      "iteration 113, loss 137.22889709472656\n",
      "iteration 114, loss 129.15087890625\n",
      "iteration 115, loss 121.57161712646484\n",
      "iteration 116, loss 114.4573745727539\n",
      "iteration 117, loss 107.77401733398438\n",
      "iteration 118, loss 101.49932861328125\n",
      "iteration 119, loss 95.60588073730469\n",
      "iteration 120, loss 90.06912231445312\n",
      "iteration 121, loss 84.86319732666016\n",
      "iteration 122, loss 79.96820068359375\n",
      "iteration 123, loss 75.36956024169922\n",
      "iteration 124, loss 71.04412078857422\n",
      "iteration 125, loss 66.97564697265625\n",
      "iteration 126, loss 63.148250579833984\n",
      "iteration 127, loss 59.54793167114258\n",
      "iteration 128, loss 56.16230010986328\n",
      "iteration 129, loss 52.97428512573242\n",
      "iteration 130, loss 49.97488784790039\n",
      "iteration 131, loss 47.150123596191406\n",
      "iteration 132, loss 44.491615295410156\n",
      "iteration 133, loss 41.987003326416016\n",
      "iteration 134, loss 39.62956237792969\n",
      "iteration 135, loss 37.40806579589844\n",
      "iteration 136, loss 35.31596374511719\n",
      "iteration 137, loss 33.3452033996582\n",
      "iteration 138, loss 31.488847732543945\n",
      "iteration 139, loss 29.737537384033203\n",
      "iteration 140, loss 28.087060928344727\n",
      "iteration 141, loss 26.532276153564453\n",
      "iteration 142, loss 25.065322875976562\n",
      "iteration 143, loss 23.68219757080078\n",
      "iteration 144, loss 22.377574920654297\n",
      "iteration 145, loss 21.147109985351562\n",
      "iteration 146, loss 19.986072540283203\n",
      "iteration 147, loss 18.892742156982422\n",
      "iteration 148, loss 17.858999252319336\n",
      "iteration 149, loss 16.884122848510742\n",
      "iteration 150, loss 15.963857650756836\n",
      "iteration 151, loss 15.095781326293945\n",
      "iteration 152, loss 14.27590560913086\n",
      "iteration 153, loss 13.502115249633789\n",
      "iteration 154, loss 12.770636558532715\n",
      "iteration 155, loss 12.080862998962402\n",
      "iteration 156, loss 11.429437637329102\n",
      "iteration 157, loss 10.813882827758789\n",
      "iteration 158, loss 10.232160568237305\n",
      "iteration 159, loss 9.682868003845215\n",
      "iteration 160, loss 9.163793563842773\n",
      "iteration 161, loss 8.6730318069458\n",
      "iteration 162, loss 8.209537506103516\n",
      "iteration 163, loss 7.771599769592285\n",
      "iteration 164, loss 7.357394695281982\n",
      "iteration 165, loss 6.965805530548096\n",
      "iteration 166, loss 6.595715045928955\n",
      "iteration 167, loss 6.245608806610107\n",
      "iteration 168, loss 5.9148759841918945\n",
      "iteration 169, loss 5.601441383361816\n",
      "iteration 170, loss 5.3056793212890625\n",
      "iteration 171, loss 5.025967597961426\n",
      "iteration 172, loss 4.76126766204834\n",
      "iteration 173, loss 4.510700225830078\n",
      "iteration 174, loss 4.273416996002197\n",
      "iteration 175, loss 4.04931116104126\n",
      "iteration 176, loss 3.8371217250823975\n",
      "iteration 177, loss 3.636108160018921\n",
      "iteration 178, loss 3.4462313652038574\n",
      "iteration 179, loss 3.2658610343933105\n",
      "iteration 180, loss 3.0958681106567383\n",
      "iteration 181, loss 2.9344642162323\n",
      "iteration 182, loss 2.7817423343658447\n",
      "iteration 183, loss 2.637364625930786\n",
      "iteration 184, loss 2.500683307647705\n",
      "iteration 185, loss 2.370812177658081\n",
      "iteration 186, loss 2.247985363006592\n",
      "iteration 187, loss 2.1317741870880127\n",
      "iteration 188, loss 2.0216827392578125\n",
      "iteration 189, loss 1.9174139499664307\n",
      "iteration 190, loss 1.8185369968414307\n",
      "iteration 191, loss 1.7248575687408447\n",
      "iteration 192, loss 1.6360907554626465\n",
      "iteration 193, loss 1.5521315336227417\n",
      "iteration 194, loss 1.4725573062896729\n",
      "iteration 195, loss 1.396908164024353\n",
      "iteration 196, loss 1.3255150318145752\n",
      "iteration 197, loss 1.2576665878295898\n",
      "iteration 198, loss 1.1934525966644287\n",
      "iteration 199, loss 1.1325557231903076\n",
      "iteration 200, loss 1.0747255086898804\n",
      "iteration 201, loss 1.0200676918029785\n",
      "iteration 202, loss 0.9681097865104675\n",
      "iteration 203, loss 0.9190395474433899\n",
      "iteration 204, loss 0.8723021745681763\n",
      "iteration 205, loss 0.8279280662536621\n",
      "iteration 206, loss 0.7859614491462708\n",
      "iteration 207, loss 0.7461706399917603\n",
      "iteration 208, loss 0.7084213495254517\n",
      "iteration 209, loss 0.6726416349411011\n",
      "iteration 210, loss 0.6386368870735168\n",
      "iteration 211, loss 0.6063960194587708\n",
      "iteration 212, loss 0.575884997844696\n",
      "iteration 213, loss 0.5469102263450623\n",
      "iteration 214, loss 0.5193606019020081\n",
      "iteration 215, loss 0.49323350191116333\n",
      "iteration 216, loss 0.4683799147605896\n",
      "iteration 217, loss 0.4448840618133545\n",
      "iteration 218, loss 0.42267361283302307\n",
      "iteration 219, loss 0.4014257490634918\n",
      "iteration 220, loss 0.3813546895980835\n",
      "iteration 221, loss 0.36227449774742126\n",
      "iteration 222, loss 0.34418627619743347\n",
      "iteration 223, loss 0.3269304931163788\n",
      "iteration 224, loss 0.310690313577652\n",
      "iteration 225, loss 0.29521065950393677\n",
      "iteration 226, loss 0.2804523706436157\n",
      "iteration 227, loss 0.26645585894584656\n",
      "iteration 228, loss 0.2532283663749695\n",
      "iteration 229, loss 0.24062879383563995\n",
      "iteration 230, loss 0.22872839868068695\n",
      "iteration 231, loss 0.21734613180160522\n",
      "iteration 232, loss 0.2065383642911911\n",
      "iteration 233, loss 0.19630758464336395\n",
      "iteration 234, loss 0.18661868572235107\n",
      "iteration 235, loss 0.17738018929958344\n",
      "iteration 236, loss 0.16857966780662537\n",
      "iteration 237, loss 0.16027621924877167\n",
      "iteration 238, loss 0.1523868441581726\n",
      "iteration 239, loss 0.14486151933670044\n",
      "iteration 240, loss 0.13771449029445648\n",
      "iteration 241, loss 0.13092726469039917\n",
      "iteration 242, loss 0.12445958703756332\n",
      "iteration 243, loss 0.11836057156324387\n",
      "iteration 244, loss 0.112562395632267\n",
      "iteration 245, loss 0.10702994465827942\n",
      "iteration 246, loss 0.10178705304861069\n",
      "iteration 247, loss 0.0967736467719078\n",
      "iteration 248, loss 0.09201229363679886\n",
      "iteration 249, loss 0.08752159774303436\n",
      "iteration 250, loss 0.08320370316505432\n",
      "iteration 251, loss 0.07915502786636353\n",
      "iteration 252, loss 0.07529754191637039\n",
      "iteration 253, loss 0.07161256670951843\n",
      "iteration 254, loss 0.06811460107564926\n",
      "iteration 255, loss 0.06479202210903168\n",
      "iteration 256, loss 0.06162481755018234\n",
      "iteration 257, loss 0.058616526424884796\n",
      "iteration 258, loss 0.05577470734715462\n",
      "iteration 259, loss 0.05304624140262604\n",
      "iteration 260, loss 0.05046704411506653\n",
      "iteration 261, loss 0.048008181154727936\n",
      "iteration 262, loss 0.04567389562726021\n",
      "iteration 263, loss 0.04346778243780136\n",
      "iteration 264, loss 0.04137549549341202\n",
      "iteration 265, loss 0.039364948868751526\n",
      "iteration 266, loss 0.03745817020535469\n",
      "iteration 267, loss 0.035654325038194656\n",
      "iteration 268, loss 0.03393615409731865\n",
      "iteration 269, loss 0.0322890542447567\n",
      "iteration 270, loss 0.030724503099918365\n",
      "iteration 271, loss 0.029232177883386612\n",
      "iteration 272, loss 0.027834869921207428\n",
      "iteration 273, loss 0.026497311890125275\n",
      "iteration 274, loss 0.025223279371857643\n",
      "iteration 275, loss 0.024008747190237045\n",
      "iteration 276, loss 0.022861720994114876\n",
      "iteration 277, loss 0.021766358986496925\n",
      "iteration 278, loss 0.02072283811867237\n",
      "iteration 279, loss 0.01973211206495762\n",
      "iteration 280, loss 0.01879909262061119\n",
      "iteration 281, loss 0.01790117472410202\n",
      "iteration 282, loss 0.017039857804775238\n",
      "iteration 283, loss 0.016238577663898468\n",
      "iteration 284, loss 0.01546380203217268\n",
      "iteration 285, loss 0.014729716815054417\n",
      "iteration 286, loss 0.01402941532433033\n",
      "iteration 287, loss 0.01336329709738493\n",
      "iteration 288, loss 0.012740368954837322\n",
      "iteration 289, loss 0.012135583907365799\n",
      "iteration 290, loss 0.011570481583476067\n",
      "iteration 291, loss 0.01102364994585514\n",
      "iteration 292, loss 0.010513663291931152\n",
      "iteration 293, loss 0.010021013207733631\n",
      "iteration 294, loss 0.009552576579153538\n",
      "iteration 295, loss 0.009112588129937649\n",
      "iteration 296, loss 0.008682569488883018\n",
      "iteration 297, loss 0.008288979530334473\n",
      "iteration 298, loss 0.007890455424785614\n",
      "iteration 299, loss 0.007532129529863596\n",
      "iteration 300, loss 0.00718182185664773\n",
      "iteration 301, loss 0.006852779071778059\n",
      "iteration 302, loss 0.006535347085446119\n",
      "iteration 303, loss 0.006237067747861147\n",
      "iteration 304, loss 0.005951886996626854\n",
      "iteration 305, loss 0.0056795962154865265\n",
      "iteration 306, loss 0.005424667149782181\n",
      "iteration 307, loss 0.005183465778827667\n",
      "iteration 308, loss 0.004947019275277853\n",
      "iteration 309, loss 0.004724250640720129\n",
      "iteration 310, loss 0.004507486708462238\n",
      "iteration 311, loss 0.004303842782974243\n",
      "iteration 312, loss 0.00411149812862277\n",
      "iteration 313, loss 0.003934036940336227\n",
      "iteration 314, loss 0.00376037135720253\n",
      "iteration 315, loss 0.0035930329468101263\n",
      "iteration 316, loss 0.0034397842828184366\n",
      "iteration 317, loss 0.0032899249345064163\n",
      "iteration 318, loss 0.003147883340716362\n",
      "iteration 319, loss 0.0030140657909214497\n",
      "iteration 320, loss 0.0028836610727012157\n",
      "iteration 321, loss 0.002760412637144327\n",
      "iteration 322, loss 0.002644900232553482\n",
      "iteration 323, loss 0.002534703118726611\n",
      "iteration 324, loss 0.002425058279186487\n",
      "iteration 325, loss 0.002323719672858715\n",
      "iteration 326, loss 0.002230137586593628\n",
      "iteration 327, loss 0.002137369243428111\n",
      "iteration 328, loss 0.00205126847140491\n",
      "iteration 329, loss 0.001964980736374855\n",
      "iteration 330, loss 0.001886655343696475\n",
      "iteration 331, loss 0.0018101357854902744\n",
      "iteration 332, loss 0.0017397226765751839\n",
      "iteration 333, loss 0.0016710248310118914\n",
      "iteration 334, loss 0.0016049789264798164\n",
      "iteration 335, loss 0.0015420301351696253\n",
      "iteration 336, loss 0.0014812909066677094\n",
      "iteration 337, loss 0.0014247571816667914\n",
      "iteration 338, loss 0.0013699809787794948\n",
      "iteration 339, loss 0.001318443682976067\n",
      "iteration 340, loss 0.0012689793948084116\n",
      "iteration 341, loss 0.0012224753154441714\n",
      "iteration 342, loss 0.0011758088367059827\n",
      "iteration 343, loss 0.0011345143429934978\n",
      "iteration 344, loss 0.0010923552326858044\n",
      "iteration 345, loss 0.0010517645860090852\n",
      "iteration 346, loss 0.0010139690712094307\n",
      "iteration 347, loss 0.0009761135443113744\n",
      "iteration 348, loss 0.0009414631058461964\n",
      "iteration 349, loss 0.0009087943471968174\n",
      "iteration 350, loss 0.0008795190951786935\n",
      "iteration 351, loss 0.0008488291059620678\n",
      "iteration 352, loss 0.0008202808676287532\n",
      "iteration 353, loss 0.0007911436259746552\n",
      "iteration 354, loss 0.0007648169412277639\n",
      "iteration 355, loss 0.0007393004489131272\n",
      "iteration 356, loss 0.0007135645719245076\n",
      "iteration 357, loss 0.0006920347805134952\n",
      "iteration 358, loss 0.0006701908423565328\n",
      "iteration 359, loss 0.0006477858987636864\n",
      "iteration 360, loss 0.0006276378408074379\n",
      "iteration 361, loss 0.0006077804137021303\n",
      "iteration 362, loss 0.0005862433463335037\n",
      "iteration 363, loss 0.0005681693437509239\n",
      "iteration 364, loss 0.0005503053544089198\n",
      "iteration 365, loss 0.0005340740899555385\n",
      "iteration 366, loss 0.0005177899147383869\n",
      "iteration 367, loss 0.000500714872032404\n",
      "iteration 368, loss 0.0004852436250075698\n",
      "iteration 369, loss 0.00047086633276194334\n",
      "iteration 370, loss 0.00045691413106396794\n",
      "iteration 371, loss 0.0004431148408912122\n",
      "iteration 372, loss 0.00043155867024324834\n",
      "iteration 373, loss 0.00041763478657230735\n",
      "iteration 374, loss 0.0004051918222103268\n",
      "iteration 375, loss 0.0003938756126444787\n",
      "iteration 376, loss 0.0003823183651547879\n",
      "iteration 377, loss 0.0003716936334967613\n",
      "iteration 378, loss 0.0003601927019190043\n",
      "iteration 379, loss 0.00035027190460823476\n",
      "iteration 380, loss 0.00034214663901366293\n",
      "iteration 381, loss 0.00033263981458730996\n",
      "iteration 382, loss 0.00032310106325894594\n",
      "iteration 383, loss 0.00031435495475307107\n",
      "iteration 384, loss 0.00030683251679874957\n",
      "iteration 385, loss 0.0002982143487315625\n",
      "iteration 386, loss 0.0002898654493037611\n",
      "iteration 387, loss 0.000282605760730803\n",
      "iteration 388, loss 0.00027465762104839087\n",
      "iteration 389, loss 0.00026771854027174413\n",
      "iteration 390, loss 0.0002604736655484885\n",
      "iteration 391, loss 0.00025427842047065496\n",
      "iteration 392, loss 0.00024760482483543456\n",
      "iteration 393, loss 0.00024220153864007443\n",
      "iteration 394, loss 0.00023573637008666992\n",
      "iteration 395, loss 0.000229855693760328\n",
      "iteration 396, loss 0.0002236929867649451\n",
      "iteration 397, loss 0.0002182559110224247\n",
      "iteration 398, loss 0.00021265610121190548\n",
      "iteration 399, loss 0.0002071994822472334\n",
      "iteration 400, loss 0.00020252280228305608\n",
      "iteration 401, loss 0.00019802062888629735\n",
      "iteration 402, loss 0.00019302121654618531\n",
      "iteration 403, loss 0.00018957600696012378\n",
      "iteration 404, loss 0.00018413654470350593\n",
      "iteration 405, loss 0.00018019051640294492\n",
      "iteration 406, loss 0.0001760539598762989\n",
      "iteration 407, loss 0.00017275489517487586\n",
      "iteration 408, loss 0.00016831968969199806\n",
      "iteration 409, loss 0.00016473924915771931\n",
      "iteration 410, loss 0.00016088347183540463\n",
      "iteration 411, loss 0.00015759376401547343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 412, loss 0.00015402314602397382\n",
      "iteration 413, loss 0.0001506778207840398\n",
      "iteration 414, loss 0.00014717770682182163\n",
      "iteration 415, loss 0.00014395145990420133\n",
      "iteration 416, loss 0.00014177484263200313\n",
      "iteration 417, loss 0.00013860299077350646\n",
      "iteration 418, loss 0.00013573697651736438\n",
      "iteration 419, loss 0.0001326702331425622\n",
      "iteration 420, loss 0.00012978589802514762\n",
      "iteration 421, loss 0.00012688440619967878\n",
      "iteration 422, loss 0.00012456234253477305\n",
      "iteration 423, loss 0.0001222957653226331\n",
      "iteration 424, loss 0.00011981152783846483\n",
      "iteration 425, loss 0.0001175985235022381\n",
      "iteration 426, loss 0.00011515052756294608\n",
      "iteration 427, loss 0.00011268934758845717\n",
      "iteration 428, loss 0.00011098255345132202\n",
      "iteration 429, loss 0.00010870154801523313\n",
      "iteration 430, loss 0.00010653772187652066\n",
      "iteration 431, loss 0.00010487579129403457\n",
      "iteration 432, loss 0.00010236288653686643\n",
      "iteration 433, loss 0.0001006933453027159\n",
      "iteration 434, loss 9.874009265331551e-05\n",
      "iteration 435, loss 9.7065327281598e-05\n",
      "iteration 436, loss 9.510698146186769e-05\n",
      "iteration 437, loss 9.341981785837561e-05\n",
      "iteration 438, loss 9.232745651388541e-05\n",
      "iteration 439, loss 9.020668949233368e-05\n",
      "iteration 440, loss 8.853334293235093e-05\n",
      "iteration 441, loss 8.676511788507923e-05\n",
      "iteration 442, loss 8.546260505681857e-05\n",
      "iteration 443, loss 8.43982124933973e-05\n",
      "iteration 444, loss 8.287913806270808e-05\n",
      "iteration 445, loss 8.128486661007628e-05\n",
      "iteration 446, loss 7.988598372321576e-05\n",
      "iteration 447, loss 7.849118992453441e-05\n",
      "iteration 448, loss 7.719800487393513e-05\n",
      "iteration 449, loss 7.620342512382194e-05\n",
      "iteration 450, loss 7.486900722142309e-05\n",
      "iteration 451, loss 7.361963798757643e-05\n",
      "iteration 452, loss 7.255417585838586e-05\n",
      "iteration 453, loss 7.138291402952746e-05\n",
      "iteration 454, loss 7.038736657705158e-05\n",
      "iteration 455, loss 6.918040162418038e-05\n",
      "iteration 456, loss 6.821875285822898e-05\n",
      "iteration 457, loss 6.724891136400402e-05\n",
      "iteration 458, loss 6.601757922908291e-05\n",
      "iteration 459, loss 6.499993469333276e-05\n",
      "iteration 460, loss 6.356930680340156e-05\n",
      "iteration 461, loss 6.245662370929495e-05\n",
      "iteration 462, loss 6.14673990639858e-05\n",
      "iteration 463, loss 6.06194298597984e-05\n",
      "iteration 464, loss 5.982729999232106e-05\n",
      "iteration 465, loss 5.907861850573681e-05\n",
      "iteration 466, loss 5.80911073484458e-05\n",
      "iteration 467, loss 5.738606341765262e-05\n",
      "iteration 468, loss 5.6635919463587925e-05\n",
      "iteration 469, loss 5.5904656619532034e-05\n",
      "iteration 470, loss 5.5159416660899296e-05\n",
      "iteration 471, loss 5.432024772744626e-05\n",
      "iteration 472, loss 5.3602998377755284e-05\n",
      "iteration 473, loss 5.266534208203666e-05\n",
      "iteration 474, loss 5.224403503234498e-05\n",
      "iteration 475, loss 5.148414129507728e-05\n",
      "iteration 476, loss 5.081546260043979e-05\n",
      "iteration 477, loss 5.0326776545261964e-05\n",
      "iteration 478, loss 4.9515940190758556e-05\n",
      "iteration 479, loss 4.910867573926225e-05\n",
      "iteration 480, loss 4.8397036152891815e-05\n",
      "iteration 481, loss 4.757174610858783e-05\n",
      "iteration 482, loss 4.7099965740926564e-05\n",
      "iteration 483, loss 4.628082024282776e-05\n",
      "iteration 484, loss 4.56398083770182e-05\n",
      "iteration 485, loss 4.506870391196571e-05\n",
      "iteration 486, loss 4.432802597875707e-05\n",
      "iteration 487, loss 4.371263275970705e-05\n",
      "iteration 488, loss 4.3320804252289236e-05\n",
      "iteration 489, loss 4.280053690308705e-05\n",
      "iteration 490, loss 4.2428775486769155e-05\n",
      "iteration 491, loss 4.192263804725371e-05\n",
      "iteration 492, loss 4.131782770855352e-05\n",
      "iteration 493, loss 4.0881619497668e-05\n",
      "iteration 494, loss 4.031864591524936e-05\n",
      "iteration 495, loss 3.9847778680268675e-05\n",
      "iteration 496, loss 3.956082946388051e-05\n",
      "iteration 497, loss 3.898113573086448e-05\n",
      "iteration 498, loss 3.8595440855715424e-05\n",
      "iteration 499, loss 3.8159902032930404e-05\n"
     ]
    }
   ],
   "source": [
    "# implementing the same example in torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\")\n",
    "datatype = torch.float\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "W1 = torch.randn(D_in, H, dtype=datatype, device=device, requires_grad=True)\n",
    "W2 = torch.randn(H, D_out, dtype=datatype, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    z = x.mm(W1)\n",
    "    h = z.clamp(min=0) # ReLU(z)\n",
    "    y_pred = h.mm(W2)\n",
    "    \n",
    "    loss = (y - y_pred).pow(2).sum()\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss.item()))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        \n",
    "        W2.grad.zero_()\n",
    "        W1.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "iteration 0, loss 30022618.0\n",
      "backward\n",
      "forward\n",
      "iteration 1, loss 22213114.0\n",
      "backward\n",
      "forward\n",
      "iteration 2, loss 19139246.0\n",
      "backward\n",
      "forward\n",
      "iteration 3, loss 17307442.0\n",
      "backward\n",
      "forward\n",
      "iteration 4, loss 15276606.0\n",
      "backward\n",
      "forward\n",
      "iteration 5, loss 12612155.0\n",
      "backward\n",
      "forward\n",
      "iteration 6, loss 9635052.0\n",
      "backward\n",
      "forward\n",
      "iteration 7, loss 6877531.0\n",
      "backward\n",
      "forward\n",
      "iteration 8, loss 4696401.0\n",
      "backward\n",
      "forward\n",
      "iteration 9, loss 3153943.0\n",
      "backward\n",
      "forward\n",
      "iteration 10, loss 2133609.25\n",
      "backward\n",
      "forward\n",
      "iteration 11, loss 1481988.625\n",
      "backward\n",
      "forward\n",
      "iteration 12, loss 1067844.0\n",
      "backward\n",
      "forward\n",
      "iteration 13, loss 801169.25\n",
      "backward\n",
      "forward\n",
      "iteration 14, loss 623969.0625\n",
      "backward\n",
      "forward\n",
      "iteration 15, loss 501846.4375\n",
      "backward\n",
      "forward\n",
      "iteration 16, loss 414046.6875\n",
      "backward\n",
      "forward\n",
      "iteration 17, loss 348356.8125\n",
      "backward\n",
      "forward\n",
      "iteration 18, loss 297296.125\n",
      "backward\n",
      "forward\n",
      "iteration 19, loss 256429.3125\n",
      "backward\n",
      "forward\n",
      "iteration 20, loss 222918.453125\n",
      "backward\n",
      "forward\n",
      "iteration 21, loss 194993.03125\n",
      "backward\n",
      "forward\n",
      "iteration 22, loss 171448.359375\n",
      "backward\n",
      "forward\n",
      "iteration 23, loss 151335.4375\n",
      "backward\n",
      "forward\n",
      "iteration 24, loss 134009.578125\n",
      "backward\n",
      "forward\n",
      "iteration 25, loss 119005.1484375\n",
      "backward\n",
      "forward\n",
      "iteration 26, loss 105937.2109375\n",
      "backward\n",
      "forward\n",
      "iteration 27, loss 94514.484375\n",
      "backward\n",
      "forward\n",
      "iteration 28, loss 84500.359375\n",
      "backward\n",
      "forward\n",
      "iteration 29, loss 75694.21875\n",
      "backward\n",
      "forward\n",
      "iteration 30, loss 67919.359375\n",
      "backward\n",
      "forward\n",
      "iteration 31, loss 61041.03515625\n",
      "backward\n",
      "forward\n",
      "iteration 32, loss 54944.61328125\n",
      "backward\n",
      "forward\n",
      "iteration 33, loss 49534.62890625\n",
      "backward\n",
      "forward\n",
      "iteration 34, loss 44732.03515625\n",
      "backward\n",
      "forward\n",
      "iteration 35, loss 40454.265625\n",
      "backward\n",
      "forward\n",
      "iteration 36, loss 36630.9765625\n",
      "backward\n",
      "forward\n",
      "iteration 37, loss 33210.1171875\n",
      "backward\n",
      "forward\n",
      "iteration 38, loss 30144.36328125\n",
      "backward\n",
      "forward\n",
      "iteration 39, loss 27392.546875\n",
      "backward\n",
      "forward\n",
      "iteration 40, loss 24916.173828125\n",
      "backward\n",
      "forward\n",
      "iteration 41, loss 22684.841796875\n",
      "backward\n",
      "forward\n",
      "iteration 42, loss 20672.091796875\n",
      "backward\n",
      "forward\n",
      "iteration 43, loss 18854.59765625\n",
      "backward\n",
      "forward\n",
      "iteration 44, loss 17211.232421875\n",
      "backward\n",
      "forward\n",
      "iteration 45, loss 15724.3740234375\n",
      "backward\n",
      "forward\n",
      "iteration 46, loss 14376.529296875\n",
      "backward\n",
      "forward\n",
      "iteration 47, loss 13154.353515625\n",
      "backward\n",
      "forward\n",
      "iteration 48, loss 12044.7333984375\n",
      "backward\n",
      "forward\n",
      "iteration 49, loss 11036.599609375\n",
      "backward\n",
      "forward\n",
      "iteration 50, loss 10119.58203125\n",
      "backward\n",
      "forward\n",
      "iteration 51, loss 9284.7880859375\n",
      "backward\n",
      "forward\n",
      "iteration 52, loss 8523.86328125\n",
      "backward\n",
      "forward\n",
      "iteration 53, loss 7829.9970703125\n",
      "backward\n",
      "forward\n",
      "iteration 54, loss 7197.08984375\n",
      "backward\n",
      "forward\n",
      "iteration 55, loss 6619.01708984375\n",
      "backward\n",
      "forward\n",
      "iteration 56, loss 6090.49755859375\n",
      "backward\n",
      "forward\n",
      "iteration 57, loss 5607.10302734375\n",
      "backward\n",
      "forward\n",
      "iteration 58, loss 5164.6298828125\n",
      "backward\n",
      "forward\n",
      "iteration 59, loss 4759.5869140625\n",
      "backward\n",
      "forward\n",
      "iteration 60, loss 4388.4453125\n",
      "backward\n",
      "forward\n",
      "iteration 61, loss 4048.013916015625\n",
      "backward\n",
      "forward\n",
      "iteration 62, loss 3735.63916015625\n",
      "backward\n",
      "forward\n",
      "iteration 63, loss 3449.001953125\n",
      "backward\n",
      "forward\n",
      "iteration 64, loss 3185.639892578125\n",
      "backward\n",
      "forward\n",
      "iteration 65, loss 2943.5126953125\n",
      "backward\n",
      "forward\n",
      "iteration 66, loss 2720.87158203125\n",
      "backward\n",
      "forward\n",
      "iteration 67, loss 2516.1669921875\n",
      "backward\n",
      "forward\n",
      "iteration 68, loss 2327.81298828125\n",
      "backward\n",
      "forward\n",
      "iteration 69, loss 2154.248779296875\n",
      "backward\n",
      "forward\n",
      "iteration 70, loss 1994.3607177734375\n",
      "backward\n",
      "forward\n",
      "iteration 71, loss 1847.099365234375\n",
      "backward\n",
      "forward\n",
      "iteration 72, loss 1711.3055419921875\n",
      "backward\n",
      "forward\n",
      "iteration 73, loss 1585.8074951171875\n",
      "backward\n",
      "forward\n",
      "iteration 74, loss 1469.9642333984375\n",
      "backward\n",
      "forward\n",
      "iteration 75, loss 1363.0689697265625\n",
      "backward\n",
      "forward\n",
      "iteration 76, loss 1264.3116455078125\n",
      "backward\n",
      "forward\n",
      "iteration 77, loss 1173.1031494140625\n",
      "backward\n",
      "forward\n",
      "iteration 78, loss 1088.74951171875\n",
      "backward\n",
      "forward\n",
      "iteration 79, loss 1010.790771484375\n",
      "backward\n",
      "forward\n",
      "iteration 80, loss 938.6450805664062\n",
      "backward\n",
      "forward\n",
      "iteration 81, loss 871.9315795898438\n",
      "backward\n",
      "forward\n",
      "iteration 82, loss 810.1688842773438\n",
      "backward\n",
      "forward\n",
      "iteration 83, loss 752.9923706054688\n",
      "backward\n",
      "forward\n",
      "iteration 84, loss 699.9998168945312\n",
      "backward\n",
      "forward\n",
      "iteration 85, loss 650.8971557617188\n",
      "backward\n",
      "forward\n",
      "iteration 86, loss 605.4077758789062\n",
      "backward\n",
      "forward\n",
      "iteration 87, loss 563.2374267578125\n",
      "backward\n",
      "forward\n",
      "iteration 88, loss 524.1199340820312\n",
      "backward\n",
      "forward\n",
      "iteration 89, loss 487.82354736328125\n",
      "backward\n",
      "forward\n",
      "iteration 90, loss 454.1596984863281\n",
      "backward\n",
      "forward\n",
      "iteration 91, loss 422.9012451171875\n",
      "backward\n",
      "forward\n",
      "iteration 92, loss 393.87713623046875\n",
      "backward\n",
      "forward\n",
      "iteration 93, loss 366.9296569824219\n",
      "backward\n",
      "forward\n",
      "iteration 94, loss 341.899169921875\n",
      "backward\n",
      "forward\n",
      "iteration 95, loss 318.6325378417969\n",
      "backward\n",
      "forward\n",
      "iteration 96, loss 297.0182189941406\n",
      "backward\n",
      "forward\n",
      "iteration 97, loss 276.9307556152344\n",
      "backward\n",
      "forward\n",
      "iteration 98, loss 258.26031494140625\n",
      "backward\n",
      "forward\n",
      "iteration 99, loss 240.9109344482422\n",
      "backward\n",
      "forward\n",
      "iteration 100, loss 224.77017211914062\n",
      "backward\n",
      "forward\n",
      "iteration 101, loss 209.74807739257812\n",
      "backward\n",
      "forward\n",
      "iteration 102, loss 195.76333618164062\n",
      "backward\n",
      "forward\n",
      "iteration 103, loss 182.74002075195312\n",
      "backward\n",
      "forward\n",
      "iteration 104, loss 170.616455078125\n",
      "backward\n",
      "forward\n",
      "iteration 105, loss 159.31956481933594\n",
      "backward\n",
      "forward\n",
      "iteration 106, loss 148.79954528808594\n",
      "backward\n",
      "forward\n",
      "iteration 107, loss 138.99627685546875\n",
      "backward\n",
      "forward\n",
      "iteration 108, loss 129.8555450439453\n",
      "backward\n",
      "forward\n",
      "iteration 109, loss 121.33879852294922\n",
      "backward\n",
      "forward\n",
      "iteration 110, loss 113.4044189453125\n",
      "backward\n",
      "forward\n",
      "iteration 111, loss 106.0051498413086\n",
      "backward\n",
      "forward\n",
      "iteration 112, loss 99.10552215576172\n",
      "backward\n",
      "forward\n",
      "iteration 113, loss 92.67107391357422\n",
      "backward\n",
      "forward\n",
      "iteration 114, loss 86.66548919677734\n",
      "backward\n",
      "forward\n",
      "iteration 115, loss 81.05856323242188\n",
      "backward\n",
      "forward\n",
      "iteration 116, loss 75.8262939453125\n",
      "backward\n",
      "forward\n",
      "iteration 117, loss 70.9428939819336\n",
      "backward\n",
      "forward\n",
      "iteration 118, loss 66.3809585571289\n",
      "backward\n",
      "forward\n",
      "iteration 119, loss 62.11951446533203\n",
      "backward\n",
      "forward\n",
      "iteration 120, loss 58.14784622192383\n",
      "backward\n",
      "forward\n",
      "iteration 121, loss 54.44123458862305\n",
      "backward\n",
      "forward\n",
      "iteration 122, loss 50.978111267089844\n",
      "backward\n",
      "forward\n",
      "iteration 123, loss 47.74113082885742\n",
      "backward\n",
      "forward\n",
      "iteration 124, loss 44.71290588378906\n",
      "backward\n",
      "forward\n",
      "iteration 125, loss 41.88331604003906\n",
      "backward\n",
      "forward\n",
      "iteration 126, loss 39.23863220214844\n",
      "backward\n",
      "forward\n",
      "iteration 127, loss 36.76336669921875\n",
      "backward\n",
      "forward\n",
      "iteration 128, loss 34.44865036010742\n",
      "backward\n",
      "forward\n",
      "iteration 129, loss 32.283592224121094\n",
      "backward\n",
      "forward\n",
      "iteration 130, loss 30.25768280029297\n",
      "backward\n",
      "forward\n",
      "iteration 131, loss 28.36199378967285\n",
      "backward\n",
      "forward\n",
      "iteration 132, loss 26.588144302368164\n",
      "backward\n",
      "forward\n",
      "iteration 133, loss 24.92715072631836\n",
      "backward\n",
      "forward\n",
      "iteration 134, loss 23.371925354003906\n",
      "backward\n",
      "forward\n",
      "iteration 135, loss 21.917354583740234\n",
      "backward\n",
      "forward\n",
      "iteration 136, loss 20.5556583404541\n",
      "backward\n",
      "forward\n",
      "iteration 137, loss 19.27918815612793\n",
      "backward\n",
      "forward\n",
      "iteration 138, loss 18.084087371826172\n",
      "backward\n",
      "forward\n",
      "iteration 139, loss 16.964691162109375\n",
      "backward\n",
      "forward\n",
      "iteration 140, loss 15.915849685668945\n",
      "backward\n",
      "forward\n",
      "iteration 141, loss 14.933550834655762\n",
      "backward\n",
      "forward\n",
      "iteration 142, loss 14.013168334960938\n",
      "backward\n",
      "forward\n",
      "iteration 143, loss 13.151505470275879\n",
      "backward\n",
      "forward\n",
      "iteration 144, loss 12.343201637268066\n",
      "backward\n",
      "forward\n",
      "iteration 145, loss 11.585521697998047\n",
      "backward\n",
      "forward\n",
      "iteration 146, loss 10.875405311584473\n",
      "backward\n",
      "forward\n",
      "iteration 147, loss 10.209906578063965\n",
      "backward\n",
      "forward\n",
      "iteration 148, loss 9.584824562072754\n",
      "backward\n",
      "forward\n",
      "iteration 149, loss 8.999505996704102\n",
      "backward\n",
      "forward\n",
      "iteration 150, loss 8.450705528259277\n",
      "backward\n",
      "forward\n",
      "iteration 151, loss 7.936354637145996\n",
      "backward\n",
      "forward\n",
      "iteration 152, loss 7.4533305168151855\n",
      "backward\n",
      "forward\n",
      "iteration 153, loss 7.000330924987793\n",
      "backward\n",
      "forward\n",
      "iteration 154, loss 6.575444221496582\n",
      "backward\n",
      "forward\n",
      "iteration 155, loss 6.177171230316162\n",
      "backward\n",
      "forward\n",
      "iteration 156, loss 5.803046226501465\n",
      "backward\n",
      "forward\n",
      "iteration 157, loss 5.45203161239624\n",
      "backward\n",
      "forward\n",
      "iteration 158, loss 5.1223249435424805\n",
      "backward\n",
      "forward\n",
      "iteration 159, loss 4.812784671783447\n",
      "backward\n",
      "forward\n",
      "iteration 160, loss 4.523118495941162\n",
      "backward\n",
      "forward\n",
      "iteration 161, loss 4.250956058502197\n",
      "backward\n",
      "forward\n",
      "iteration 162, loss 3.9949588775634766\n",
      "backward\n",
      "forward\n",
      "iteration 163, loss 3.754709005355835\n",
      "backward\n",
      "forward\n",
      "iteration 164, loss 3.529115915298462\n",
      "backward\n",
      "forward\n",
      "iteration 165, loss 3.3177614212036133\n",
      "backward\n",
      "forward\n",
      "iteration 166, loss 3.1187562942504883\n",
      "backward\n",
      "forward\n",
      "iteration 167, loss 2.9321634769439697\n",
      "backward\n",
      "forward\n",
      "iteration 168, loss 2.75669527053833\n",
      "backward\n",
      "forward\n",
      "iteration 169, loss 2.5921523571014404\n",
      "backward\n",
      "forward\n",
      "iteration 170, loss 2.437256336212158\n",
      "backward\n",
      "forward\n",
      "iteration 171, loss 2.2922651767730713\n",
      "backward\n",
      "forward\n",
      "iteration 172, loss 2.1555447578430176\n",
      "backward\n",
      "forward\n",
      "iteration 173, loss 2.027158498764038\n",
      "backward\n",
      "forward\n",
      "iteration 174, loss 1.9067312479019165\n",
      "backward\n",
      "forward\n",
      "iteration 175, loss 1.7934247255325317\n",
      "backward\n",
      "forward\n",
      "iteration 176, loss 1.6869596242904663\n",
      "backward\n",
      "forward\n",
      "iteration 177, loss 1.5867116451263428\n",
      "backward\n",
      "forward\n",
      "iteration 178, loss 1.4926977157592773\n",
      "backward\n",
      "forward\n",
      "iteration 179, loss 1.4043478965759277\n",
      "backward\n",
      "forward\n",
      "iteration 180, loss 1.321141242980957\n",
      "backward\n",
      "forward\n",
      "iteration 181, loss 1.2432993650436401\n",
      "backward\n",
      "forward\n",
      "iteration 182, loss 1.1697410345077515\n",
      "backward\n",
      "forward\n",
      "iteration 183, loss 1.1006709337234497\n",
      "backward\n",
      "forward\n",
      "iteration 184, loss 1.0356965065002441\n",
      "backward\n",
      "forward\n",
      "iteration 185, loss 0.9747525453567505\n",
      "backward\n",
      "forward\n",
      "iteration 186, loss 0.9173706769943237\n",
      "backward\n",
      "forward\n",
      "iteration 187, loss 0.8633849620819092\n",
      "backward\n",
      "forward\n",
      "iteration 188, loss 0.8126658797264099\n",
      "backward\n",
      "forward\n",
      "iteration 189, loss 0.7648434638977051\n",
      "backward\n",
      "forward\n",
      "iteration 190, loss 0.7199686169624329\n",
      "backward\n",
      "forward\n",
      "iteration 191, loss 0.6777294874191284\n",
      "backward\n",
      "forward\n",
      "iteration 192, loss 0.6379900574684143\n",
      "backward\n",
      "forward\n",
      "iteration 193, loss 0.6007270812988281\n",
      "backward\n",
      "forward\n",
      "iteration 194, loss 0.5654918551445007\n",
      "backward\n",
      "forward\n",
      "iteration 195, loss 0.5323559045791626\n",
      "backward\n",
      "forward\n",
      "iteration 196, loss 0.5012341141700745\n",
      "backward\n",
      "forward\n",
      "iteration 197, loss 0.47196078300476074\n",
      "backward\n",
      "forward\n",
      "iteration 198, loss 0.4443911612033844\n",
      "backward\n",
      "forward\n",
      "iteration 199, loss 0.41847115755081177\n",
      "backward\n",
      "forward\n",
      "iteration 200, loss 0.3940940797328949\n",
      "backward\n",
      "forward\n",
      "iteration 201, loss 0.3711068630218506\n",
      "backward\n",
      "forward\n",
      "iteration 202, loss 0.3495311141014099\n",
      "backward\n",
      "forward\n",
      "iteration 203, loss 0.3292061388492584\n",
      "backward\n",
      "forward\n",
      "iteration 204, loss 0.3100251257419586\n",
      "backward\n",
      "forward\n",
      "iteration 205, loss 0.2920439839363098\n",
      "backward\n",
      "forward\n",
      "iteration 206, loss 0.27509114146232605\n",
      "backward\n",
      "forward\n",
      "iteration 207, loss 0.2590617835521698\n",
      "backward\n",
      "forward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 208, loss 0.24402643740177155\n",
      "backward\n",
      "forward\n",
      "iteration 209, loss 0.22987858951091766\n",
      "backward\n",
      "forward\n",
      "iteration 210, loss 0.21658504009246826\n",
      "backward\n",
      "forward\n",
      "iteration 211, loss 0.20405171811580658\n",
      "backward\n",
      "forward\n",
      "iteration 212, loss 0.19222751259803772\n",
      "backward\n",
      "forward\n",
      "iteration 213, loss 0.18113575875759125\n",
      "backward\n",
      "forward\n",
      "iteration 214, loss 0.17070727050304413\n",
      "backward\n",
      "forward\n",
      "iteration 215, loss 0.1608772873878479\n",
      "backward\n",
      "forward\n",
      "iteration 216, loss 0.15155600011348724\n",
      "backward\n",
      "forward\n",
      "iteration 217, loss 0.14279896020889282\n",
      "backward\n",
      "forward\n",
      "iteration 218, loss 0.13458096981048584\n",
      "backward\n",
      "forward\n",
      "iteration 219, loss 0.12685002386569977\n",
      "backward\n",
      "forward\n",
      "iteration 220, loss 0.11957568675279617\n",
      "backward\n",
      "forward\n",
      "iteration 221, loss 0.11264494806528091\n",
      "backward\n",
      "forward\n",
      "iteration 222, loss 0.1062159314751625\n",
      "backward\n",
      "forward\n",
      "iteration 223, loss 0.10008803009986877\n",
      "backward\n",
      "forward\n",
      "iteration 224, loss 0.09433910995721817\n",
      "backward\n",
      "forward\n",
      "iteration 225, loss 0.08893819898366928\n",
      "backward\n",
      "forward\n",
      "iteration 226, loss 0.08380552381277084\n",
      "backward\n",
      "forward\n",
      "iteration 227, loss 0.07899519801139832\n",
      "backward\n",
      "forward\n",
      "iteration 228, loss 0.07446341216564178\n",
      "backward\n",
      "forward\n",
      "iteration 229, loss 0.07018586248159409\n",
      "backward\n",
      "forward\n",
      "iteration 230, loss 0.066181980073452\n",
      "backward\n",
      "forward\n",
      "iteration 231, loss 0.062400054186582565\n",
      "backward\n",
      "forward\n",
      "iteration 232, loss 0.05882115662097931\n",
      "backward\n",
      "forward\n",
      "iteration 233, loss 0.055472008883953094\n",
      "backward\n",
      "forward\n",
      "iteration 234, loss 0.0523005910217762\n",
      "backward\n",
      "forward\n",
      "iteration 235, loss 0.04932025447487831\n",
      "backward\n",
      "forward\n",
      "iteration 236, loss 0.046515196561813354\n",
      "backward\n",
      "forward\n",
      "iteration 237, loss 0.04384421557188034\n",
      "backward\n",
      "forward\n",
      "iteration 238, loss 0.041352562606334686\n",
      "backward\n",
      "forward\n",
      "iteration 239, loss 0.03899354115128517\n",
      "backward\n",
      "forward\n",
      "iteration 240, loss 0.03675183281302452\n",
      "backward\n",
      "forward\n",
      "iteration 241, loss 0.034664638340473175\n",
      "backward\n",
      "forward\n",
      "iteration 242, loss 0.03270368650555611\n",
      "backward\n",
      "forward\n",
      "iteration 243, loss 0.030848108232021332\n",
      "backward\n",
      "forward\n",
      "iteration 244, loss 0.029081672430038452\n",
      "backward\n",
      "forward\n",
      "iteration 245, loss 0.027446584776043892\n",
      "backward\n",
      "forward\n",
      "iteration 246, loss 0.02587307244539261\n",
      "backward\n",
      "forward\n",
      "iteration 247, loss 0.02440868876874447\n",
      "backward\n",
      "forward\n",
      "iteration 248, loss 0.023014508187770844\n",
      "backward\n",
      "forward\n",
      "iteration 249, loss 0.021716849878430367\n",
      "backward\n",
      "forward\n",
      "iteration 250, loss 0.020494680851697922\n",
      "backward\n",
      "forward\n",
      "iteration 251, loss 0.019329464063048363\n",
      "backward\n",
      "forward\n",
      "iteration 252, loss 0.01824495941400528\n",
      "backward\n",
      "forward\n",
      "iteration 253, loss 0.017229124903678894\n",
      "backward\n",
      "forward\n",
      "iteration 254, loss 0.01626177318394184\n",
      "backward\n",
      "forward\n",
      "iteration 255, loss 0.015337667427957058\n",
      "backward\n",
      "forward\n",
      "iteration 256, loss 0.014477564021945\n",
      "backward\n",
      "forward\n",
      "iteration 257, loss 0.013666440732777119\n",
      "backward\n",
      "forward\n",
      "iteration 258, loss 0.012902284972369671\n",
      "backward\n",
      "forward\n",
      "iteration 259, loss 0.012178092263638973\n",
      "backward\n",
      "forward\n",
      "iteration 260, loss 0.011501064524054527\n",
      "backward\n",
      "forward\n",
      "iteration 261, loss 0.010853402316570282\n",
      "backward\n",
      "forward\n",
      "iteration 262, loss 0.010250391438603401\n",
      "backward\n",
      "forward\n",
      "iteration 263, loss 0.009678089991211891\n",
      "backward\n",
      "forward\n",
      "iteration 264, loss 0.009141317568719387\n",
      "backward\n",
      "forward\n",
      "iteration 265, loss 0.008637976832687855\n",
      "backward\n",
      "forward\n",
      "iteration 266, loss 0.00815940834581852\n",
      "backward\n",
      "forward\n",
      "iteration 267, loss 0.007707435172051191\n",
      "backward\n",
      "forward\n",
      "iteration 268, loss 0.007281103171408176\n",
      "backward\n",
      "forward\n",
      "iteration 269, loss 0.006889597047120333\n",
      "backward\n",
      "forward\n",
      "iteration 270, loss 0.006512387655675411\n",
      "backward\n",
      "forward\n",
      "iteration 271, loss 0.0061569819226861\n",
      "backward\n",
      "forward\n",
      "iteration 272, loss 0.005817973520606756\n",
      "backward\n",
      "forward\n",
      "iteration 273, loss 0.005504386033862829\n",
      "backward\n",
      "forward\n",
      "iteration 274, loss 0.00520468270406127\n",
      "backward\n",
      "forward\n",
      "iteration 275, loss 0.0049294824711978436\n",
      "backward\n",
      "forward\n",
      "iteration 276, loss 0.004662852268666029\n",
      "backward\n",
      "forward\n",
      "iteration 277, loss 0.004414777271449566\n",
      "backward\n",
      "forward\n",
      "iteration 278, loss 0.00417845044285059\n",
      "backward\n",
      "forward\n",
      "iteration 279, loss 0.003957946784794331\n",
      "backward\n",
      "forward\n",
      "iteration 280, loss 0.003753061406314373\n",
      "backward\n",
      "forward\n",
      "iteration 281, loss 0.0035554433707147837\n",
      "backward\n",
      "forward\n",
      "iteration 282, loss 0.0033707143738865852\n",
      "backward\n",
      "forward\n",
      "iteration 283, loss 0.0031955379527062178\n",
      "backward\n",
      "forward\n",
      "iteration 284, loss 0.003030479187145829\n",
      "backward\n",
      "forward\n",
      "iteration 285, loss 0.002876220503821969\n",
      "backward\n",
      "forward\n",
      "iteration 286, loss 0.002729087835177779\n",
      "backward\n",
      "forward\n",
      "iteration 287, loss 0.0025884772185236216\n",
      "backward\n",
      "forward\n",
      "iteration 288, loss 0.0024609407410025597\n",
      "backward\n",
      "forward\n",
      "iteration 289, loss 0.0023364180233329535\n",
      "backward\n",
      "forward\n",
      "iteration 290, loss 0.0022178185172379017\n",
      "backward\n",
      "forward\n",
      "iteration 291, loss 0.0021063112653791904\n",
      "backward\n",
      "forward\n",
      "iteration 292, loss 0.0020033896435052156\n",
      "backward\n",
      "forward\n",
      "iteration 293, loss 0.0019071431597694755\n",
      "backward\n",
      "forward\n",
      "iteration 294, loss 0.0018115274142473936\n",
      "backward\n",
      "forward\n",
      "iteration 295, loss 0.0017287799855694175\n",
      "backward\n",
      "forward\n",
      "iteration 296, loss 0.0016464688815176487\n",
      "backward\n",
      "forward\n",
      "iteration 297, loss 0.001568055828101933\n",
      "backward\n",
      "forward\n",
      "iteration 298, loss 0.0014939701650291681\n",
      "backward\n",
      "forward\n",
      "iteration 299, loss 0.0014244448393583298\n",
      "backward\n",
      "forward\n",
      "iteration 300, loss 0.0013616475043818355\n",
      "backward\n",
      "forward\n",
      "iteration 301, loss 0.0012983158230781555\n",
      "backward\n",
      "forward\n",
      "iteration 302, loss 0.0012421015417203307\n",
      "backward\n",
      "forward\n",
      "iteration 303, loss 0.001185095403343439\n",
      "backward\n",
      "forward\n",
      "iteration 304, loss 0.0011311349226161838\n",
      "backward\n",
      "forward\n",
      "iteration 305, loss 0.0010823369957506657\n",
      "backward\n",
      "forward\n",
      "iteration 306, loss 0.001037153066135943\n",
      "backward\n",
      "forward\n",
      "iteration 307, loss 0.0009916555136442184\n",
      "backward\n",
      "forward\n",
      "iteration 308, loss 0.0009487063507549465\n",
      "backward\n",
      "forward\n",
      "iteration 309, loss 0.0009075029520317912\n",
      "backward\n",
      "forward\n",
      "iteration 310, loss 0.0008696926524862647\n",
      "backward\n",
      "forward\n",
      "iteration 311, loss 0.0008329006959684193\n",
      "backward\n",
      "forward\n",
      "iteration 312, loss 0.0007998848450370133\n",
      "backward\n",
      "forward\n",
      "iteration 313, loss 0.0007672980427742004\n",
      "backward\n",
      "forward\n",
      "iteration 314, loss 0.0007352045504376292\n",
      "backward\n",
      "forward\n",
      "iteration 315, loss 0.0007069901912473142\n",
      "backward\n",
      "forward\n",
      "iteration 316, loss 0.0006777758826501667\n",
      "backward\n",
      "forward\n",
      "iteration 317, loss 0.0006505330093204975\n",
      "backward\n",
      "forward\n",
      "iteration 318, loss 0.0006255917251110077\n",
      "backward\n",
      "forward\n",
      "iteration 319, loss 0.0006017386331222951\n",
      "backward\n",
      "forward\n",
      "iteration 320, loss 0.0005787131376564503\n",
      "backward\n",
      "forward\n",
      "iteration 321, loss 0.0005557126714847982\n",
      "backward\n",
      "forward\n",
      "iteration 322, loss 0.0005344654200598598\n",
      "backward\n",
      "forward\n",
      "iteration 323, loss 0.0005157229606993496\n",
      "backward\n",
      "forward\n",
      "iteration 324, loss 0.0004961897502653301\n",
      "backward\n",
      "forward\n",
      "iteration 325, loss 0.0004791399114765227\n",
      "backward\n",
      "forward\n",
      "iteration 326, loss 0.0004608916933648288\n",
      "backward\n",
      "forward\n",
      "iteration 327, loss 0.0004444803053047508\n",
      "backward\n",
      "forward\n",
      "iteration 328, loss 0.00043053756235167384\n",
      "backward\n",
      "forward\n",
      "iteration 329, loss 0.0004147578729316592\n",
      "backward\n",
      "forward\n",
      "iteration 330, loss 0.0004013415891677141\n",
      "backward\n",
      "forward\n",
      "iteration 331, loss 0.00038611216587014496\n",
      "backward\n",
      "forward\n",
      "iteration 332, loss 0.00037262486875988543\n",
      "backward\n",
      "forward\n",
      "iteration 333, loss 0.0003603651712182909\n",
      "backward\n",
      "forward\n",
      "iteration 334, loss 0.0003487133944872767\n",
      "backward\n",
      "forward\n",
      "iteration 335, loss 0.0003384874726179987\n",
      "backward\n",
      "forward\n",
      "iteration 336, loss 0.00032663013553246856\n",
      "backward\n",
      "forward\n",
      "iteration 337, loss 0.00031575121101923287\n",
      "backward\n",
      "forward\n",
      "iteration 338, loss 0.00030630777473561466\n",
      "backward\n",
      "forward\n",
      "iteration 339, loss 0.00029610187630169094\n",
      "backward\n",
      "forward\n",
      "iteration 340, loss 0.0002866628346964717\n",
      "backward\n",
      "forward\n",
      "iteration 341, loss 0.0002778739435598254\n",
      "backward\n",
      "forward\n",
      "iteration 342, loss 0.0002696208830457181\n",
      "backward\n",
      "forward\n",
      "iteration 343, loss 0.0002619432925712317\n",
      "backward\n",
      "forward\n",
      "iteration 344, loss 0.00025372180971316993\n",
      "backward\n",
      "forward\n",
      "iteration 345, loss 0.0002461144176777452\n",
      "backward\n",
      "forward\n",
      "iteration 346, loss 0.00023990421323105693\n",
      "backward\n",
      "forward\n",
      "iteration 347, loss 0.00023306779621634632\n",
      "backward\n",
      "forward\n",
      "iteration 348, loss 0.00022588805586565286\n",
      "backward\n",
      "forward\n",
      "iteration 349, loss 0.00022036976588424295\n",
      "backward\n",
      "forward\n",
      "iteration 350, loss 0.0002132524678017944\n",
      "backward\n",
      "forward\n",
      "iteration 351, loss 0.0002073337382171303\n",
      "backward\n",
      "forward\n",
      "iteration 352, loss 0.0002014764177147299\n",
      "backward\n",
      "forward\n",
      "iteration 353, loss 0.00019563898968044668\n",
      "backward\n",
      "forward\n",
      "iteration 354, loss 0.00018996195285581052\n",
      "backward\n",
      "forward\n",
      "iteration 355, loss 0.0001856256858445704\n",
      "backward\n",
      "forward\n",
      "iteration 356, loss 0.0001810416579246521\n",
      "backward\n",
      "forward\n",
      "iteration 357, loss 0.0001755064877215773\n",
      "backward\n",
      "forward\n",
      "iteration 358, loss 0.00017096094961743802\n",
      "backward\n",
      "forward\n",
      "iteration 359, loss 0.0001666089374339208\n",
      "backward\n",
      "forward\n",
      "iteration 360, loss 0.0001625049626454711\n",
      "backward\n",
      "forward\n",
      "iteration 361, loss 0.00015837588580325246\n",
      "backward\n",
      "forward\n",
      "iteration 362, loss 0.00015443727897945791\n",
      "backward\n",
      "forward\n",
      "iteration 363, loss 0.0001508486457169056\n",
      "backward\n",
      "forward\n",
      "iteration 364, loss 0.00014730125258211046\n",
      "backward\n",
      "forward\n",
      "iteration 365, loss 0.00014343972725328058\n",
      "backward\n",
      "forward\n",
      "iteration 366, loss 0.00013968089479021728\n",
      "backward\n",
      "forward\n",
      "iteration 367, loss 0.00013622168626170605\n",
      "backward\n",
      "forward\n",
      "iteration 368, loss 0.00013334467075765133\n",
      "backward\n",
      "forward\n",
      "iteration 369, loss 0.00013031928392592818\n",
      "backward\n",
      "forward\n",
      "iteration 370, loss 0.00012737528595607728\n",
      "backward\n",
      "forward\n",
      "iteration 371, loss 0.00012396674719639122\n",
      "backward\n",
      "forward\n",
      "iteration 372, loss 0.0001212642528116703\n",
      "backward\n",
      "forward\n",
      "iteration 373, loss 0.00011886846914421767\n",
      "backward\n",
      "forward\n",
      "iteration 374, loss 0.00011633245594566688\n",
      "backward\n",
      "forward\n",
      "iteration 375, loss 0.0001137056460720487\n",
      "backward\n",
      "forward\n",
      "iteration 376, loss 0.00011071349581470713\n",
      "backward\n",
      "forward\n",
      "iteration 377, loss 0.00010788394865812734\n",
      "backward\n",
      "forward\n",
      "iteration 378, loss 0.00010587230644887313\n",
      "backward\n",
      "forward\n",
      "iteration 379, loss 0.0001034052693285048\n",
      "backward\n",
      "forward\n",
      "iteration 380, loss 0.00010113058669958264\n",
      "backward\n",
      "forward\n",
      "iteration 381, loss 9.924545156536624e-05\n",
      "backward\n",
      "forward\n",
      "iteration 382, loss 9.68090898822993e-05\n",
      "backward\n",
      "forward\n",
      "iteration 383, loss 9.501982276560739e-05\n",
      "backward\n",
      "forward\n",
      "iteration 384, loss 9.314191265730187e-05\n",
      "backward\n",
      "forward\n",
      "iteration 385, loss 9.130178659688681e-05\n",
      "backward\n",
      "forward\n",
      "iteration 386, loss 8.945545414462686e-05\n",
      "backward\n",
      "forward\n",
      "iteration 387, loss 8.752064604777843e-05\n",
      "backward\n",
      "forward\n",
      "iteration 388, loss 8.560626156395301e-05\n",
      "backward\n",
      "forward\n",
      "iteration 389, loss 8.383001113543287e-05\n",
      "backward\n",
      "forward\n",
      "iteration 390, loss 8.219524897867814e-05\n",
      "backward\n",
      "forward\n",
      "iteration 391, loss 8.068021270446479e-05\n",
      "backward\n",
      "forward\n",
      "iteration 392, loss 7.92620558058843e-05\n",
      "backward\n",
      "forward\n",
      "iteration 393, loss 7.791736425133422e-05\n",
      "backward\n",
      "forward\n",
      "iteration 394, loss 7.618474774062634e-05\n",
      "backward\n",
      "forward\n",
      "iteration 395, loss 7.485369860660285e-05\n",
      "backward\n",
      "forward\n",
      "iteration 396, loss 7.322047895286232e-05\n",
      "backward\n",
      "forward\n",
      "iteration 397, loss 7.168871525209397e-05\n",
      "backward\n",
      "forward\n",
      "iteration 398, loss 7.023623766144738e-05\n",
      "backward\n",
      "forward\n",
      "iteration 399, loss 6.918291910551488e-05\n",
      "backward\n",
      "forward\n",
      "iteration 400, loss 6.772561027901247e-05\n",
      "backward\n",
      "forward\n",
      "iteration 401, loss 6.653855234617367e-05\n",
      "backward\n",
      "forward\n",
      "iteration 402, loss 6.553106504725292e-05\n",
      "backward\n",
      "forward\n",
      "iteration 403, loss 6.419032433768734e-05\n",
      "backward\n",
      "forward\n",
      "iteration 404, loss 6.33111922070384e-05\n",
      "backward\n",
      "forward\n",
      "iteration 405, loss 6.213296728674322e-05\n",
      "backward\n",
      "forward\n",
      "iteration 406, loss 6.124049832578748e-05\n",
      "backward\n",
      "forward\n",
      "iteration 407, loss 6.0124755691504106e-05\n",
      "backward\n",
      "forward\n",
      "iteration 408, loss 5.8830613852478564e-05\n",
      "backward\n",
      "forward\n",
      "iteration 409, loss 5.792370575363748e-05\n",
      "backward\n",
      "forward\n",
      "iteration 410, loss 5.685456926585175e-05\n",
      "backward\n",
      "forward\n",
      "iteration 411, loss 5.624907498713583e-05\n",
      "backward\n",
      "forward\n",
      "iteration 412, loss 5.539527046494186e-05\n",
      "backward\n",
      "forward\n",
      "iteration 413, loss 5.4236366850091144e-05\n",
      "backward\n",
      "forward\n",
      "iteration 414, loss 5.331907595973462e-05\n",
      "backward\n",
      "forward\n",
      "iteration 415, loss 5.243680425337516e-05\n",
      "backward\n",
      "forward\n",
      "iteration 416, loss 5.180183143238537e-05\n",
      "backward\n",
      "forward\n",
      "iteration 417, loss 5.07610930071678e-05\n",
      "backward\n",
      "forward\n",
      "iteration 418, loss 5.026093640481122e-05\n",
      "backward\n",
      "forward\n",
      "iteration 419, loss 4.9332986236549914e-05\n",
      "backward\n",
      "forward\n",
      "iteration 420, loss 4.850994446314871e-05\n",
      "backward\n",
      "forward\n",
      "iteration 421, loss 4.7845936933299527e-05\n",
      "backward\n",
      "forward\n",
      "iteration 422, loss 4.731058652396314e-05\n",
      "backward\n",
      "forward\n",
      "iteration 423, loss 4.644005457521416e-05\n",
      "backward\n",
      "forward\n",
      "iteration 424, loss 4.554753832053393e-05\n",
      "backward\n",
      "forward\n",
      "iteration 425, loss 4.492074003792368e-05\n",
      "backward\n",
      "forward\n",
      "iteration 426, loss 4.4408163375919685e-05\n",
      "backward\n",
      "forward\n",
      "iteration 427, loss 4.3761770939454436e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward\n",
      "forward\n",
      "iteration 428, loss 4.316696504247375e-05\n",
      "backward\n",
      "forward\n",
      "iteration 429, loss 4.232087667332962e-05\n",
      "backward\n",
      "forward\n",
      "iteration 430, loss 4.16866059822496e-05\n",
      "backward\n",
      "forward\n",
      "iteration 431, loss 4.123483449802734e-05\n",
      "backward\n",
      "forward\n",
      "iteration 432, loss 4.047932452522218e-05\n",
      "backward\n",
      "forward\n",
      "iteration 433, loss 3.9842248952481896e-05\n",
      "backward\n",
      "forward\n",
      "iteration 434, loss 3.942669718526304e-05\n",
      "backward\n",
      "forward\n",
      "iteration 435, loss 3.888586434186436e-05\n",
      "backward\n",
      "forward\n",
      "iteration 436, loss 3.839027704088949e-05\n",
      "backward\n",
      "forward\n",
      "iteration 437, loss 3.787412788369693e-05\n",
      "backward\n",
      "forward\n",
      "iteration 438, loss 3.748030212591402e-05\n",
      "backward\n",
      "forward\n",
      "iteration 439, loss 3.668001954792999e-05\n",
      "backward\n",
      "forward\n",
      "iteration 440, loss 3.620635834522545e-05\n",
      "backward\n",
      "forward\n",
      "iteration 441, loss 3.583406942198053e-05\n",
      "backward\n",
      "forward\n",
      "iteration 442, loss 3.554209979483858e-05\n",
      "backward\n",
      "forward\n",
      "iteration 443, loss 3.4972879802808166e-05\n",
      "backward\n",
      "forward\n",
      "iteration 444, loss 3.449690848356113e-05\n",
      "backward\n",
      "forward\n",
      "iteration 445, loss 3.420569919398986e-05\n",
      "backward\n",
      "forward\n",
      "iteration 446, loss 3.3815373171819374e-05\n",
      "backward\n",
      "forward\n",
      "iteration 447, loss 3.325661236885935e-05\n",
      "backward\n",
      "forward\n",
      "iteration 448, loss 3.2779276807559654e-05\n",
      "backward\n",
      "forward\n",
      "iteration 449, loss 3.225937325623818e-05\n",
      "backward\n",
      "forward\n",
      "iteration 450, loss 3.196691250195727e-05\n",
      "backward\n",
      "forward\n",
      "iteration 451, loss 3.1421321182278916e-05\n",
      "backward\n",
      "forward\n",
      "iteration 452, loss 3.105522773694247e-05\n",
      "backward\n",
      "forward\n",
      "iteration 453, loss 3.060560266021639e-05\n",
      "backward\n",
      "forward\n",
      "iteration 454, loss 3.0375702408491634e-05\n",
      "backward\n",
      "forward\n",
      "iteration 455, loss 3.009402280440554e-05\n",
      "backward\n",
      "forward\n",
      "iteration 456, loss 2.9694585464312695e-05\n",
      "backward\n",
      "forward\n",
      "iteration 457, loss 2.9366670787567273e-05\n",
      "backward\n",
      "forward\n",
      "iteration 458, loss 2.8844766347901896e-05\n",
      "backward\n",
      "forward\n",
      "iteration 459, loss 2.8570191716426052e-05\n",
      "backward\n",
      "forward\n",
      "iteration 460, loss 2.8309410481597297e-05\n",
      "backward\n",
      "forward\n",
      "iteration 461, loss 2.8055679649696685e-05\n",
      "backward\n",
      "forward\n",
      "iteration 462, loss 2.7712283554137684e-05\n",
      "backward\n",
      "forward\n",
      "iteration 463, loss 2.73007190116914e-05\n",
      "backward\n",
      "forward\n",
      "iteration 464, loss 2.6905692720902152e-05\n",
      "backward\n",
      "forward\n",
      "iteration 465, loss 2.661676444404293e-05\n",
      "backward\n",
      "forward\n",
      "iteration 466, loss 2.6240140869049355e-05\n",
      "backward\n",
      "forward\n",
      "iteration 467, loss 2.601375490485225e-05\n",
      "backward\n",
      "forward\n",
      "iteration 468, loss 2.5723140424815938e-05\n",
      "backward\n",
      "forward\n",
      "iteration 469, loss 2.5291237761848606e-05\n",
      "backward\n",
      "forward\n",
      "iteration 470, loss 2.5160103177768178e-05\n",
      "backward\n",
      "forward\n",
      "iteration 471, loss 2.500818118278403e-05\n",
      "backward\n",
      "forward\n",
      "iteration 472, loss 2.4799479433568195e-05\n",
      "backward\n",
      "forward\n",
      "iteration 473, loss 2.4375371140195057e-05\n",
      "backward\n",
      "forward\n",
      "iteration 474, loss 2.4198163373512216e-05\n",
      "backward\n",
      "forward\n",
      "iteration 475, loss 2.3926684662001207e-05\n",
      "backward\n",
      "forward\n",
      "iteration 476, loss 2.3720933313597925e-05\n",
      "backward\n",
      "forward\n",
      "iteration 477, loss 2.3366590539808385e-05\n",
      "backward\n",
      "forward\n",
      "iteration 478, loss 2.314150879101362e-05\n",
      "backward\n",
      "forward\n",
      "iteration 479, loss 2.2837928554508835e-05\n",
      "backward\n",
      "forward\n",
      "iteration 480, loss 2.2615928173763677e-05\n",
      "backward\n",
      "forward\n",
      "iteration 481, loss 2.2391905076801777e-05\n",
      "backward\n",
      "forward\n",
      "iteration 482, loss 2.2029693354852498e-05\n",
      "backward\n",
      "forward\n",
      "iteration 483, loss 2.180381488869898e-05\n",
      "backward\n",
      "forward\n",
      "iteration 484, loss 2.1620360712404363e-05\n",
      "backward\n",
      "forward\n",
      "iteration 485, loss 2.1457835828186944e-05\n",
      "backward\n",
      "forward\n",
      "iteration 486, loss 2.1148442101548426e-05\n",
      "backward\n",
      "forward\n",
      "iteration 487, loss 2.1013494915678166e-05\n",
      "backward\n",
      "forward\n",
      "iteration 488, loss 2.077861972793471e-05\n",
      "backward\n",
      "forward\n",
      "iteration 489, loss 2.0548315660562366e-05\n",
      "backward\n",
      "forward\n",
      "iteration 490, loss 2.0353056243038736e-05\n",
      "backward\n",
      "forward\n",
      "iteration 491, loss 2.0171937649138272e-05\n",
      "backward\n",
      "forward\n",
      "iteration 492, loss 1.9856377548421733e-05\n",
      "backward\n",
      "forward\n",
      "iteration 493, loss 1.9800350855803117e-05\n",
      "backward\n",
      "forward\n",
      "iteration 494, loss 1.9556704501155764e-05\n",
      "backward\n",
      "forward\n",
      "iteration 495, loss 1.937384695338551e-05\n",
      "backward\n",
      "forward\n",
      "iteration 496, loss 1.918935595313087e-05\n",
      "backward\n",
      "forward\n",
      "iteration 497, loss 1.910102946567349e-05\n",
      "backward\n",
      "forward\n",
      "iteration 498, loss 1.889605300675612e-05\n",
      "backward\n",
      "forward\n",
      "iteration 499, loss 1.873385554063134e-05\n",
      "backward\n"
     ]
    }
   ],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        relu = input.clamp(min=0)\n",
    "        ctx.save_for_backward(relu)\n",
    "        return relu\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        relu, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[relu <= 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\")\n",
    "datatype = torch.float\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "W1 = torch.randn(D_in, H, dtype=datatype, device=device, requires_grad=True)\n",
    "W2 = torch.randn(H, D_out, dtype=datatype, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for iteration in np.arange(500):\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    z = x.mm(W1)\n",
    "    h = relu(z) # ReLU(z)\n",
    "    y_pred = h.mm(W2)\n",
    "    \n",
    "    loss = (y - y_pred).pow(2).sum()\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss.item()))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        \n",
    "        W2.grad.zero_()\n",
    "        W1.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
