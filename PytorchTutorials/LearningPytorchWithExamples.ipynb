{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial URL: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#examples-download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PyTorch with Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "datatype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Warm up: numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 46103921.24431324\n",
      "iteration 1, loss 55335319.40049319\n",
      "iteration 2, loss 64856416.68490945\n",
      "iteration 3, loss 56328213.97213833\n",
      "iteration 4, loss 30249422.78934653\n",
      "iteration 5, loss 10635371.68525007\n",
      "iteration 6, loss 3797737.360140309\n",
      "iteration 7, loss 2065809.7523898603\n",
      "iteration 8, loss 1506268.13934818\n",
      "iteration 9, loss 1212992.608010401\n",
      "iteration 10, loss 1006364.8889980961\n",
      "iteration 11, loss 845641.5291087683\n",
      "iteration 12, loss 716724.4384063184\n",
      "iteration 13, loss 611770.0729005195\n",
      "iteration 14, loss 525327.9287492852\n",
      "iteration 15, loss 453639.09952636133\n",
      "iteration 16, loss 393640.84285191656\n",
      "iteration 17, loss 343196.92708472384\n",
      "iteration 18, loss 300501.17207822256\n",
      "iteration 19, loss 264173.5857252701\n",
      "iteration 20, loss 233045.55894569898\n",
      "iteration 21, loss 206274.50088595736\n",
      "iteration 22, loss 183197.1375950476\n",
      "iteration 23, loss 163167.06907626428\n",
      "iteration 24, loss 145735.27821881382\n",
      "iteration 25, loss 130510.27222634724\n",
      "iteration 26, loss 117160.8232068207\n",
      "iteration 27, loss 105341.11256777763\n",
      "iteration 28, loss 94900.55581612414\n",
      "iteration 29, loss 85663.83073479976\n",
      "iteration 30, loss 77463.49500810177\n",
      "iteration 31, loss 70174.65338859808\n",
      "iteration 32, loss 63672.05263169226\n",
      "iteration 33, loss 57859.499419977205\n",
      "iteration 34, loss 52659.83389912059\n",
      "iteration 35, loss 47984.852247253875\n",
      "iteration 36, loss 43784.09003862516\n",
      "iteration 37, loss 40000.12476572477\n",
      "iteration 38, loss 36585.481474608474\n",
      "iteration 39, loss 33498.238511296266\n",
      "iteration 40, loss 30703.732952934348\n",
      "iteration 41, loss 28170.534857616396\n",
      "iteration 42, loss 25871.289981778114\n",
      "iteration 43, loss 23782.134611053934\n",
      "iteration 44, loss 21880.143606258593\n",
      "iteration 45, loss 20146.65942839876\n",
      "iteration 46, loss 18565.35494837277\n",
      "iteration 47, loss 17121.13918578973\n",
      "iteration 48, loss 15800.945051129367\n",
      "iteration 49, loss 14594.305482325797\n",
      "iteration 50, loss 13488.826346703248\n",
      "iteration 51, loss 12475.066828509356\n",
      "iteration 52, loss 11544.738399764006\n",
      "iteration 53, loss 10690.085937309945\n",
      "iteration 54, loss 9904.464678041639\n",
      "iteration 55, loss 9182.368814065027\n",
      "iteration 56, loss 8517.944524567474\n",
      "iteration 57, loss 7905.806923389471\n",
      "iteration 58, loss 7341.126960078318\n",
      "iteration 59, loss 6820.1480761282455\n",
      "iteration 60, loss 6338.99472898499\n",
      "iteration 61, loss 5894.341777100922\n",
      "iteration 62, loss 5483.350663750641\n",
      "iteration 63, loss 5103.225758705718\n",
      "iteration 64, loss 4751.452389482372\n",
      "iteration 65, loss 4425.810473598221\n",
      "iteration 66, loss 4124.053171809001\n",
      "iteration 67, loss 3844.2642136757067\n",
      "iteration 68, loss 3584.6465064289914\n",
      "iteration 69, loss 3343.771246323704\n",
      "iteration 70, loss 3119.9636288943357\n",
      "iteration 71, loss 2912.03731438823\n",
      "iteration 72, loss 2718.899596871272\n",
      "iteration 73, loss 2539.3446339096545\n",
      "iteration 74, loss 2372.3577479769256\n",
      "iteration 75, loss 2216.9661875145885\n",
      "iteration 76, loss 2072.3431286492623\n",
      "iteration 77, loss 1937.6603028141326\n",
      "iteration 78, loss 1812.1615737870939\n",
      "iteration 79, loss 1695.2169975941192\n",
      "iteration 80, loss 1586.2463923650107\n",
      "iteration 81, loss 1484.6664252189946\n",
      "iteration 82, loss 1390.0792577583293\n",
      "iteration 83, loss 1301.785623614862\n",
      "iteration 84, loss 1219.4081285748566\n",
      "iteration 85, loss 1142.4663912058954\n",
      "iteration 86, loss 1070.5876341796643\n",
      "iteration 87, loss 1003.4297441913427\n",
      "iteration 88, loss 940.6892801308746\n",
      "iteration 89, loss 882.0088042628724\n",
      "iteration 90, loss 827.172674440061\n",
      "iteration 91, loss 775.8901774224094\n",
      "iteration 92, loss 728.1369799144878\n",
      "iteration 93, loss 683.4547018672065\n",
      "iteration 94, loss 641.6161081736304\n",
      "iteration 95, loss 602.4462833801028\n",
      "iteration 96, loss 565.7639180915301\n",
      "iteration 97, loss 531.4177303304311\n",
      "iteration 98, loss 499.2367109691047\n",
      "iteration 99, loss 469.08754288326037\n",
      "iteration 100, loss 440.85320808719393\n",
      "iteration 101, loss 414.3591048423314\n",
      "iteration 102, loss 389.52063790374166\n",
      "iteration 103, loss 366.23308106576235\n",
      "iteration 104, loss 344.3841448919719\n",
      "iteration 105, loss 323.89013302802584\n",
      "iteration 106, loss 304.6622142839492\n",
      "iteration 107, loss 286.6157409241969\n",
      "iteration 108, loss 269.68013805907475\n",
      "iteration 109, loss 253.77410267395487\n",
      "iteration 110, loss 238.84249184128373\n",
      "iteration 111, loss 224.8159012045722\n",
      "iteration 112, loss 211.63871137575924\n",
      "iteration 113, loss 199.26313881512579\n",
      "iteration 114, loss 187.63094369789818\n",
      "iteration 115, loss 176.7024425834448\n",
      "iteration 116, loss 166.43172065006405\n",
      "iteration 117, loss 156.7804600495381\n",
      "iteration 118, loss 147.7043965821593\n",
      "iteration 119, loss 139.16911469319186\n",
      "iteration 120, loss 131.1437884179091\n",
      "iteration 121, loss 123.59434760267382\n",
      "iteration 122, loss 116.4928616586682\n",
      "iteration 123, loss 109.81042544097669\n",
      "iteration 124, loss 103.52414161163918\n",
      "iteration 125, loss 97.60785318504553\n",
      "iteration 126, loss 92.04108936713047\n",
      "iteration 127, loss 86.80108651583038\n",
      "iteration 128, loss 81.86636171410765\n",
      "iteration 129, loss 77.22104055391412\n",
      "iteration 130, loss 72.84661604108865\n",
      "iteration 131, loss 68.72757507291936\n",
      "iteration 132, loss 64.84704298036895\n",
      "iteration 133, loss 61.192246141148786\n",
      "iteration 134, loss 57.74993530888587\n",
      "iteration 135, loss 54.50821509451052\n",
      "iteration 136, loss 51.45019276563842\n",
      "iteration 137, loss 48.56904620774125\n",
      "iteration 138, loss 45.854225241217804\n",
      "iteration 139, loss 43.29489917267315\n",
      "iteration 140, loss 40.881529838676016\n",
      "iteration 141, loss 38.60727357335044\n",
      "iteration 142, loss 36.4624381502288\n",
      "iteration 143, loss 34.44023536646665\n",
      "iteration 144, loss 32.534314001683654\n",
      "iteration 145, loss 30.73556076441627\n",
      "iteration 146, loss 29.038136527038443\n",
      "iteration 147, loss 27.43704093959793\n",
      "iteration 148, loss 25.926732468023495\n",
      "iteration 149, loss 24.501906837413355\n",
      "iteration 150, loss 23.15682985162998\n",
      "iteration 151, loss 21.887674569666643\n",
      "iteration 152, loss 20.68949854719168\n",
      "iteration 153, loss 19.55922104787644\n",
      "iteration 154, loss 18.49231120493691\n",
      "iteration 155, loss 17.484661515104563\n",
      "iteration 156, loss 16.533025724023993\n",
      "iteration 157, loss 15.634431264222279\n",
      "iteration 158, loss 14.786112985871775\n",
      "iteration 159, loss 13.984828324338046\n",
      "iteration 160, loss 13.22838446359041\n",
      "iteration 161, loss 12.514507215886914\n",
      "iteration 162, loss 11.839865883584807\n",
      "iteration 163, loss 11.202889169872325\n",
      "iteration 164, loss 10.60129205256513\n",
      "iteration 165, loss 10.032391615605562\n",
      "iteration 166, loss 9.494593607102818\n",
      "iteration 167, loss 8.98625619138969\n",
      "iteration 168, loss 8.505896137679962\n",
      "iteration 169, loss 8.051938489604975\n",
      "iteration 170, loss 7.622640161335223\n",
      "iteration 171, loss 7.216707525389875\n",
      "iteration 172, loss 6.832931107044745\n",
      "iteration 173, loss 6.4701118494010235\n",
      "iteration 174, loss 6.12702308281631\n",
      "iteration 175, loss 5.802569207764558\n",
      "iteration 176, loss 5.495540101372244\n",
      "iteration 177, loss 5.205150107418407\n",
      "iteration 178, loss 4.930476798658349\n",
      "iteration 179, loss 4.670607153242106\n",
      "iteration 180, loss 4.424721217878834\n",
      "iteration 181, loss 4.192027002097578\n",
      "iteration 182, loss 3.9719298208719973\n",
      "iteration 183, loss 3.7635930747865274\n",
      "iteration 184, loss 3.566401971222201\n",
      "iteration 185, loss 3.3797741783889843\n",
      "iteration 186, loss 3.2031983746297943\n",
      "iteration 187, loss 3.035985617050814\n",
      "iteration 188, loss 2.8776948003585945\n",
      "iteration 189, loss 2.7277995800518444\n",
      "iteration 190, loss 2.5858681485984496\n",
      "iteration 191, loss 2.451504691929107\n",
      "iteration 192, loss 2.3242915503788932\n",
      "iteration 193, loss 2.203806643610695\n",
      "iteration 194, loss 2.0896842114689047\n",
      "iteration 195, loss 1.9816071533542896\n",
      "iteration 196, loss 1.8792766254353737\n",
      "iteration 197, loss 1.7823426913678326\n",
      "iteration 198, loss 1.6904638505062544\n",
      "iteration 199, loss 1.6034250880632925\n",
      "iteration 200, loss 1.5209408501357842\n",
      "iteration 201, loss 1.4427919657712884\n",
      "iteration 202, loss 1.3687544225948134\n",
      "iteration 203, loss 1.2986004761450385\n",
      "iteration 204, loss 1.232089714641726\n",
      "iteration 205, loss 1.1690530245920638\n",
      "iteration 206, loss 1.1093112649048043\n",
      "iteration 207, loss 1.052677831635026\n",
      "iteration 208, loss 0.9990270966736596\n",
      "iteration 209, loss 0.9481535024616848\n",
      "iteration 210, loss 0.8999063717993638\n",
      "iteration 211, loss 0.8541573750321675\n",
      "iteration 212, loss 0.810788442989318\n",
      "iteration 213, loss 0.7696664941928283\n",
      "iteration 214, loss 0.7306687753561159\n",
      "iteration 215, loss 0.693674483156475\n",
      "iteration 216, loss 0.6585923277530015\n",
      "iteration 217, loss 0.6253213536893403\n",
      "iteration 218, loss 0.5937671774594612\n",
      "iteration 219, loss 0.5638285752647414\n",
      "iteration 220, loss 0.5354399257054476\n",
      "iteration 221, loss 0.5084992704308307\n",
      "iteration 222, loss 0.48293373703514453\n",
      "iteration 223, loss 0.45868555749721895\n",
      "iteration 224, loss 0.43567779137701795\n",
      "iteration 225, loss 0.41384876806022297\n",
      "iteration 226, loss 0.393124406631828\n",
      "iteration 227, loss 0.37346108556541135\n",
      "iteration 228, loss 0.35479809638877524\n",
      "iteration 229, loss 0.3370863324309264\n",
      "iteration 230, loss 0.3202711701586344\n",
      "iteration 231, loss 0.30430839443153646\n",
      "iteration 232, loss 0.2891707952366779\n",
      "iteration 233, loss 0.2747850436978249\n",
      "iteration 234, loss 0.2611282249096174\n",
      "iteration 235, loss 0.24816321877194952\n",
      "iteration 236, loss 0.2358559319077683\n",
      "iteration 237, loss 0.22416940788358425\n",
      "iteration 238, loss 0.21306787387822645\n",
      "iteration 239, loss 0.20252702815322232\n",
      "iteration 240, loss 0.19251699008238188\n",
      "iteration 241, loss 0.18300997284365894\n",
      "iteration 242, loss 0.17398096701504873\n",
      "iteration 243, loss 0.16540394906193845\n",
      "iteration 244, loss 0.1572642861600153\n",
      "iteration 245, loss 0.14952503356219216\n",
      "iteration 246, loss 0.14217526059049948\n",
      "iteration 247, loss 0.13519250551286974\n",
      "iteration 248, loss 0.1285573880890739\n",
      "iteration 249, loss 0.12225266865308915\n",
      "iteration 250, loss 0.11626280986721071\n",
      "iteration 251, loss 0.1105715240030149\n",
      "iteration 252, loss 0.10516244409221362\n",
      "iteration 253, loss 0.10002325265006251\n",
      "iteration 254, loss 0.09514090131295205\n",
      "iteration 255, loss 0.09049930208900962\n",
      "iteration 256, loss 0.08608968436824119\n",
      "iteration 257, loss 0.08189701027626645\n",
      "iteration 258, loss 0.07791174762026755\n",
      "iteration 259, loss 0.07412365638302061\n",
      "iteration 260, loss 0.07052191425659822\n",
      "iteration 261, loss 0.06709760335750628\n",
      "iteration 262, loss 0.06384230082871752\n",
      "iteration 263, loss 0.06074757880679274\n",
      "iteration 264, loss 0.057804856170908754\n",
      "iteration 265, loss 0.05500703389844744\n",
      "iteration 266, loss 0.05234633910382064\n",
      "iteration 267, loss 0.04981659223758127\n",
      "iteration 268, loss 0.047412015661899794\n",
      "iteration 269, loss 0.04512521132450699\n",
      "iteration 270, loss 0.04294966334994956\n",
      "iteration 271, loss 0.04088051768395096\n",
      "iteration 272, loss 0.038912599701741574\n",
      "iteration 273, loss 0.03704046782624625\n",
      "iteration 274, loss 0.035259674754628684\n",
      "iteration 275, loss 0.03356581816297306\n",
      "iteration 276, loss 0.03195461135363401\n",
      "iteration 277, loss 0.03042207292422024\n",
      "iteration 278, loss 0.028963797052279593\n",
      "iteration 279, loss 0.027576596028799104\n",
      "iteration 280, loss 0.026256755110678265\n",
      "iteration 281, loss 0.025001603337925164\n",
      "iteration 282, loss 0.023806994134295646\n",
      "iteration 283, loss 0.022670053848838342\n",
      "iteration 284, loss 0.021588478989386213\n",
      "iteration 285, loss 0.020559157223715575\n",
      "iteration 286, loss 0.019579097136496353\n",
      "iteration 287, loss 0.01864658674322328\n",
      "iteration 288, loss 0.01775908939410862\n",
      "iteration 289, loss 0.0169143896832942\n",
      "iteration 290, loss 0.01611038107363461\n",
      "iteration 291, loss 0.015345056887738991\n",
      "iteration 292, loss 0.014616704076285883\n",
      "iteration 293, loss 0.013923383350930057\n",
      "iteration 294, loss 0.013263702897742612\n",
      "iteration 295, loss 0.012635514102166675\n",
      "iteration 296, loss 0.012037258515600651\n",
      "iteration 297, loss 0.011467690537848022\n",
      "iteration 298, loss 0.010925356710182075\n",
      "iteration 299, loss 0.010409051114760464\n",
      "iteration 300, loss 0.009917463867948208\n",
      "iteration 301, loss 0.009449342406551424\n",
      "iteration 302, loss 0.009003650792955162\n",
      "iteration 303, loss 0.008579226510247989\n",
      "iteration 304, loss 0.008175030235470494\n",
      "iteration 305, loss 0.0077901350640305405\n",
      "iteration 306, loss 0.00742365678162723\n",
      "iteration 307, loss 0.00707487719592283\n",
      "iteration 308, loss 0.006742356416953952\n",
      "iteration 309, loss 0.006425637169533272\n",
      "iteration 310, loss 0.006124041016691582\n",
      "iteration 311, loss 0.005836700679170956\n",
      "iteration 312, loss 0.005563031709705648\n",
      "iteration 313, loss 0.005302354492232153\n",
      "iteration 314, loss 0.005054017205186205\n",
      "iteration 315, loss 0.00481750713799462\n",
      "iteration 316, loss 0.004592251842290911\n",
      "iteration 317, loss 0.004377532204906033\n",
      "iteration 318, loss 0.004173004095192445\n",
      "iteration 319, loss 0.003978218868863062\n",
      "iteration 320, loss 0.003792630505584731\n",
      "iteration 321, loss 0.0036157073889481195\n",
      "iteration 322, loss 0.0034470964872854682\n",
      "iteration 323, loss 0.0032864516585054283\n",
      "iteration 324, loss 0.00313338202915471\n",
      "iteration 325, loss 0.0029874998430838738\n",
      "iteration 326, loss 0.0028485042674552706\n",
      "iteration 327, loss 0.002716031601356735\n",
      "iteration 328, loss 0.0025897943676420317\n",
      "iteration 329, loss 0.0024694827705365797\n",
      "iteration 330, loss 0.0023548136078846756\n",
      "iteration 331, loss 0.002245539818678449\n",
      "iteration 332, loss 0.0021414539902819977\n",
      "iteration 333, loss 0.00204219311613093\n",
      "iteration 334, loss 0.0019475692417392665\n",
      "iteration 335, loss 0.0018573556660386214\n",
      "iteration 336, loss 0.0017713642840805042\n",
      "iteration 337, loss 0.0016894089014631189\n",
      "iteration 338, loss 0.0016112633943668611\n",
      "iteration 339, loss 0.0015367775488714895\n",
      "iteration 340, loss 0.0014657747363816516\n",
      "iteration 341, loss 0.0013980703141834652\n",
      "iteration 342, loss 0.0013335351153106688\n",
      "iteration 343, loss 0.0012720088757995607\n",
      "iteration 344, loss 0.0012133467727123144\n",
      "iteration 345, loss 0.0011574503268445486\n",
      "iteration 346, loss 0.0011041372282439782\n",
      "iteration 347, loss 0.0010532789700860637\n",
      "iteration 348, loss 0.0010048278636389273\n",
      "iteration 349, loss 0.0009585950162014094\n",
      "iteration 350, loss 0.000914498486876843\n",
      "iteration 351, loss 0.0008724586371814255\n",
      "iteration 352, loss 0.0008323619233906788\n",
      "iteration 353, loss 0.000794123043096461\n",
      "iteration 354, loss 0.0007576620144408019\n",
      "iteration 355, loss 0.000722885235854429\n",
      "iteration 356, loss 0.000689719729197917\n",
      "iteration 357, loss 0.0006581019432700046\n",
      "iteration 358, loss 0.0006279444384375609\n",
      "iteration 359, loss 0.0005991826362803804\n",
      "iteration 360, loss 0.0005717406591473446\n",
      "iteration 361, loss 0.0005455615541097058\n",
      "iteration 362, loss 0.0005205937670797647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 363, loss 0.0004967799457679184\n",
      "iteration 364, loss 0.00047405988826575705\n",
      "iteration 365, loss 0.00045239069052041916\n",
      "iteration 366, loss 0.00043171936892629186\n",
      "iteration 367, loss 0.00041199824130047364\n",
      "iteration 368, loss 0.0003931882880835756\n",
      "iteration 369, loss 0.00037524460574672974\n",
      "iteration 370, loss 0.0003581265908069001\n",
      "iteration 371, loss 0.00034179921744813366\n",
      "iteration 372, loss 0.0003262247682720681\n",
      "iteration 373, loss 0.00031135739268338706\n",
      "iteration 374, loss 0.00029717310405100346\n",
      "iteration 375, loss 0.0002836395529747798\n",
      "iteration 376, loss 0.00027072693385004324\n",
      "iteration 377, loss 0.0002584070009918406\n",
      "iteration 378, loss 0.00024665285800572976\n",
      "iteration 379, loss 0.00023543671765784212\n",
      "iteration 380, loss 0.00022474216982300982\n",
      "iteration 381, loss 0.0002145293607731752\n",
      "iteration 382, loss 0.00020478454062717276\n",
      "iteration 383, loss 0.0001954867437069086\n",
      "iteration 384, loss 0.00018661214244178854\n",
      "iteration 385, loss 0.00017814984613522846\n",
      "iteration 386, loss 0.0001700678437558481\n",
      "iteration 387, loss 0.0001623553561186278\n",
      "iteration 388, loss 0.00015499430799271636\n",
      "iteration 389, loss 0.00014796992988712208\n",
      "iteration 390, loss 0.00014126579041003463\n",
      "iteration 391, loss 0.00013486699329434804\n",
      "iteration 392, loss 0.0001287606395400371\n",
      "iteration 393, loss 0.00012293242184981002\n",
      "iteration 394, loss 0.00011736964766613533\n",
      "iteration 395, loss 0.00011206089539755683\n",
      "iteration 396, loss 0.00010699457152403338\n",
      "iteration 397, loss 0.00010215710169648007\n",
      "iteration 398, loss 9.75427061256908e-05\n",
      "iteration 399, loss 9.313696938921232e-05\n",
      "iteration 400, loss 8.892993527024721e-05\n",
      "iteration 401, loss 8.491425206767727e-05\n",
      "iteration 402, loss 8.108152942057414e-05\n",
      "iteration 403, loss 7.742230362943879e-05\n",
      "iteration 404, loss 7.39294839585463e-05\n",
      "iteration 405, loss 7.059533303749538e-05\n",
      "iteration 406, loss 6.741220786238152e-05\n",
      "iteration 407, loss 6.437344848179038e-05\n",
      "iteration 408, loss 6.147321907990146e-05\n",
      "iteration 409, loss 5.870452159176725e-05\n",
      "iteration 410, loss 5.60604831223781e-05\n",
      "iteration 411, loss 5.353723857665549e-05\n",
      "iteration 412, loss 5.112908313916719e-05\n",
      "iteration 413, loss 4.8829238715826474e-05\n",
      "iteration 414, loss 4.663243520006083e-05\n",
      "iteration 415, loss 4.453515628906266e-05\n",
      "iteration 416, loss 4.25325354220211e-05\n",
      "iteration 417, loss 4.062044378333018e-05\n",
      "iteration 418, loss 3.8794993335065876e-05\n",
      "iteration 419, loss 3.705184137947234e-05\n",
      "iteration 420, loss 3.5387435269406515e-05\n",
      "iteration 421, loss 3.3798384870678795e-05\n",
      "iteration 422, loss 3.22812191919535e-05\n",
      "iteration 423, loss 3.083244532282384e-05\n",
      "iteration 424, loss 2.944902738852155e-05\n",
      "iteration 425, loss 2.812876909156681e-05\n",
      "iteration 426, loss 2.686707740061413e-05\n",
      "iteration 427, loss 2.5662250848526642e-05\n",
      "iteration 428, loss 2.4511896244919415e-05\n",
      "iteration 429, loss 2.3413225607311107e-05\n",
      "iteration 430, loss 2.236405900559133e-05\n",
      "iteration 431, loss 2.136220116306318e-05\n",
      "iteration 432, loss 2.0405436991972886e-05\n",
      "iteration 433, loss 1.9491699892949545e-05\n",
      "iteration 434, loss 1.8619059639490492e-05\n",
      "iteration 435, loss 1.77857919699788e-05\n",
      "iteration 436, loss 1.6990147190392966e-05\n",
      "iteration 437, loss 1.6230084761170043e-05\n",
      "iteration 438, loss 1.550463625897813e-05\n",
      "iteration 439, loss 1.4811396166085044e-05\n",
      "iteration 440, loss 1.4149199189000088e-05\n",
      "iteration 441, loss 1.3516737498755771e-05\n",
      "iteration 442, loss 1.2912739637754307e-05\n",
      "iteration 443, loss 1.2335783415581364e-05\n",
      "iteration 444, loss 1.1784713036670853e-05\n",
      "iteration 445, loss 1.1258446226770252e-05\n",
      "iteration 446, loss 1.0756023374995837e-05\n",
      "iteration 447, loss 1.027592993350829e-05\n",
      "iteration 448, loss 9.817286646324972e-06\n",
      "iteration 449, loss 9.379264488637939e-06\n",
      "iteration 450, loss 8.96088771170299e-06\n",
      "iteration 451, loss 8.561355187380718e-06\n",
      "iteration 452, loss 8.179661058817794e-06\n",
      "iteration 453, loss 7.814942546555084e-06\n",
      "iteration 454, loss 7.466533648796503e-06\n",
      "iteration 455, loss 7.133751364833591e-06\n",
      "iteration 456, loss 6.815878848813869e-06\n",
      "iteration 457, loss 6.512188778086569e-06\n",
      "iteration 458, loss 6.222087724189106e-06\n",
      "iteration 459, loss 5.944999783440547e-06\n",
      "iteration 460, loss 5.680287836261866e-06\n",
      "iteration 461, loss 5.42738868878903e-06\n",
      "iteration 462, loss 5.185803214612988e-06\n",
      "iteration 463, loss 4.955048273861592e-06\n",
      "iteration 464, loss 4.734696296164714e-06\n",
      "iteration 465, loss 4.524138552307677e-06\n",
      "iteration 466, loss 4.322914133295749e-06\n",
      "iteration 467, loss 4.130669635853038e-06\n",
      "iteration 468, loss 3.946991308297589e-06\n",
      "iteration 469, loss 3.771516312096623e-06\n",
      "iteration 470, loss 3.6038936706347182e-06\n",
      "iteration 471, loss 3.4437273014231865e-06\n",
      "iteration 472, loss 3.2907039034455444e-06\n",
      "iteration 473, loss 3.1445119696615567e-06\n",
      "iteration 474, loss 3.004845860424037e-06\n",
      "iteration 475, loss 2.871389207376005e-06\n",
      "iteration 476, loss 2.7438862260629937e-06\n",
      "iteration 477, loss 2.622088541761827e-06\n",
      "iteration 478, loss 2.5057909808191644e-06\n",
      "iteration 479, loss 2.394588245169534e-06\n",
      "iteration 480, loss 2.288396837204292e-06\n",
      "iteration 481, loss 2.18687590785489e-06\n",
      "iteration 482, loss 2.0898603399330406e-06\n",
      "iteration 483, loss 1.9971633960525985e-06\n",
      "iteration 484, loss 1.9085925736414444e-06\n",
      "iteration 485, loss 1.8239700151430787e-06\n",
      "iteration 486, loss 1.7431032879555991e-06\n",
      "iteration 487, loss 1.665835591962125e-06\n",
      "iteration 488, loss 1.5920049403697533e-06\n",
      "iteration 489, loss 1.5214597958868868e-06\n",
      "iteration 490, loss 1.4540480579069899e-06\n",
      "iteration 491, loss 1.3896764339397119e-06\n",
      "iteration 492, loss 1.3281433626901425e-06\n",
      "iteration 493, loss 1.26932323838624e-06\n",
      "iteration 494, loss 1.2131143162955333e-06\n",
      "iteration 495, loss 1.1594015909326865e-06\n",
      "iteration 496, loss 1.108077698248712e-06\n",
      "iteration 497, loss 1.0590320238893378e-06\n",
      "iteration 498, loss 1.012162224262999e-06\n",
      "iteration 499, loss 9.673711140236228e-07\n"
     ]
    }
   ],
   "source": [
    "# This is a good exercise to create a simple network, forward pass and backward pass all in numpy\n",
    "\n",
    "# We will use a fully-connected ReLU network as our running example. \n",
    "# The network will have a single hidden layer, and will be trained with gradient descent to fit random data \n",
    "# by minimizing the Euclidean distance between the network output and the true output.\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "W1 = np.random.randn(D_in, H)\n",
    "W2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    z = np.matmul(x,W1)\n",
    "    h = np.maximum(z,0) # ReLU(z)\n",
    "    y_pred = np.matmul(h, W2)\n",
    "    \n",
    "    loss = np.square(y - y_pred).sum()\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss))\n",
    "    \n",
    "    # Backward pass\n",
    "    dy_pred = 2*(y_pred - y)\n",
    "    dW2 = np.matmul(h.T, dy_pred)\n",
    "    dh = np.matmul(dy_pred, W2.T)\n",
    "    dz = np.copy(dh)\n",
    "    dz[h <= 0] = 0 # elementwise signum function\n",
    "    dW1 = np.matmul(x.T, dz)\n",
    "    \n",
    "    # Upgrade weights\n",
    "    W2 -= learning_rate * dW2\n",
    "    W1 -= learning_rate * dW1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pytorch: tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 26021376.0\n",
      "iteration 1, loss 22584786.0\n",
      "iteration 2, loss 23596730.0\n",
      "iteration 3, loss 25752796.0\n",
      "iteration 4, loss 26340710.0\n",
      "iteration 5, loss 23204560.0\n",
      "iteration 6, loss 17169966.0\n",
      "iteration 7, loss 10711865.0\n",
      "iteration 8, loss 6015136.0\n",
      "iteration 9, loss 3280649.5\n",
      "iteration 10, loss 1872432.875\n",
      "iteration 11, loss 1166895.875\n",
      "iteration 12, loss 804179.25\n",
      "iteration 13, loss 603233.375\n",
      "iteration 14, loss 481085.71875\n",
      "iteration 15, loss 399086.03125\n",
      "iteration 16, loss 339233.5\n",
      "iteration 17, loss 292814.34375\n",
      "iteration 18, loss 255302.78125\n",
      "iteration 19, loss 224220.34375\n",
      "iteration 20, loss 198039.953125\n",
      "iteration 21, loss 175718.421875\n",
      "iteration 22, loss 156523.4375\n",
      "iteration 23, loss 139903.5\n",
      "iteration 24, loss 125466.1015625\n",
      "iteration 25, loss 112852.5234375\n",
      "iteration 26, loss 101801.0703125\n",
      "iteration 27, loss 92082.0703125\n",
      "iteration 28, loss 83487.4609375\n",
      "iteration 29, loss 75852.46875\n",
      "iteration 30, loss 69059.671875\n",
      "iteration 31, loss 63027.48828125\n",
      "iteration 32, loss 57626.30859375\n",
      "iteration 33, loss 52776.48046875\n",
      "iteration 34, loss 48412.73828125\n",
      "iteration 35, loss 44487.828125\n",
      "iteration 36, loss 40940.25\n",
      "iteration 37, loss 37728.34375\n",
      "iteration 38, loss 34812.859375\n",
      "iteration 39, loss 32164.638671875\n",
      "iteration 40, loss 29756.212890625\n",
      "iteration 41, loss 27560.05859375\n",
      "iteration 42, loss 25558.2109375\n",
      "iteration 43, loss 23729.673828125\n",
      "iteration 44, loss 22054.123046875\n",
      "iteration 45, loss 20516.728515625\n",
      "iteration 46, loss 19103.580078125\n",
      "iteration 47, loss 17803.05859375\n",
      "iteration 48, loss 16605.23828125\n",
      "iteration 49, loss 15503.9560546875\n",
      "iteration 50, loss 14487.630859375\n",
      "iteration 51, loss 13549.537109375\n",
      "iteration 52, loss 12680.84375\n",
      "iteration 53, loss 11877.884765625\n",
      "iteration 54, loss 11133.3984375\n",
      "iteration 55, loss 10441.8193359375\n",
      "iteration 56, loss 9799.1884765625\n",
      "iteration 57, loss 9201.32421875\n",
      "iteration 58, loss 8645.0849609375\n",
      "iteration 59, loss 8126.6962890625\n",
      "iteration 60, loss 7643.65576171875\n",
      "iteration 61, loss 7192.83837890625\n",
      "iteration 62, loss 6771.828125\n",
      "iteration 63, loss 6378.5048828125\n",
      "iteration 64, loss 6010.65673828125\n",
      "iteration 65, loss 5666.630859375\n",
      "iteration 66, loss 5345.115234375\n",
      "iteration 67, loss 5045.35888671875\n",
      "iteration 68, loss 4764.52587890625\n",
      "iteration 69, loss 4501.67333984375\n",
      "iteration 70, loss 4255.0361328125\n",
      "iteration 71, loss 4023.627197265625\n",
      "iteration 72, loss 3806.208740234375\n",
      "iteration 73, loss 3601.8564453125\n",
      "iteration 74, loss 3409.77587890625\n",
      "iteration 75, loss 3229.115234375\n",
      "iteration 76, loss 3059.059814453125\n",
      "iteration 77, loss 2898.94384765625\n",
      "iteration 78, loss 2748.1279296875\n",
      "iteration 79, loss 2605.997314453125\n",
      "iteration 80, loss 2472.026611328125\n",
      "iteration 81, loss 2345.663818359375\n",
      "iteration 82, loss 2226.408935546875\n",
      "iteration 83, loss 2113.884033203125\n",
      "iteration 84, loss 2007.5970458984375\n",
      "iteration 85, loss 1907.2022705078125\n",
      "iteration 86, loss 1812.3646240234375\n",
      "iteration 87, loss 1722.65478515625\n",
      "iteration 88, loss 1637.8165283203125\n",
      "iteration 89, loss 1557.679443359375\n",
      "iteration 90, loss 1481.864013671875\n",
      "iteration 91, loss 1410.1005859375\n",
      "iteration 92, loss 1342.084716796875\n",
      "iteration 93, loss 1277.5242919921875\n",
      "iteration 94, loss 1216.359130859375\n",
      "iteration 95, loss 1158.3934326171875\n",
      "iteration 96, loss 1103.4312744140625\n",
      "iteration 97, loss 1051.3184814453125\n",
      "iteration 98, loss 1001.8626708984375\n",
      "iteration 99, loss 954.9427490234375\n",
      "iteration 100, loss 910.4024047851562\n",
      "iteration 101, loss 868.1124267578125\n",
      "iteration 102, loss 827.9581909179688\n",
      "iteration 103, loss 789.8209838867188\n",
      "iteration 104, loss 753.5826416015625\n",
      "iteration 105, loss 719.1484375\n",
      "iteration 106, loss 686.3926391601562\n",
      "iteration 107, loss 655.2578125\n",
      "iteration 108, loss 625.6375732421875\n",
      "iteration 109, loss 597.4639892578125\n",
      "iteration 110, loss 570.6578979492188\n",
      "iteration 111, loss 545.152587890625\n",
      "iteration 112, loss 520.8607177734375\n",
      "iteration 113, loss 497.7305603027344\n",
      "iteration 114, loss 475.7090148925781\n",
      "iteration 115, loss 454.7278137207031\n",
      "iteration 116, loss 434.7370300292969\n",
      "iteration 117, loss 415.6904602050781\n",
      "iteration 118, loss 397.53741455078125\n",
      "iteration 119, loss 380.23284912109375\n",
      "iteration 120, loss 363.73638916015625\n",
      "iteration 121, loss 348.0024108886719\n",
      "iteration 122, loss 332.99688720703125\n",
      "iteration 123, loss 318.67095947265625\n",
      "iteration 124, loss 305.01031494140625\n",
      "iteration 125, loss 291.9695739746094\n",
      "iteration 126, loss 279.520263671875\n",
      "iteration 127, loss 267.6377868652344\n",
      "iteration 128, loss 256.29241943359375\n",
      "iteration 129, loss 245.4565887451172\n",
      "iteration 130, loss 235.1089630126953\n",
      "iteration 131, loss 225.22386169433594\n",
      "iteration 132, loss 215.77908325195312\n",
      "iteration 133, loss 206.7531280517578\n",
      "iteration 134, loss 198.13331604003906\n",
      "iteration 135, loss 189.8917236328125\n",
      "iteration 136, loss 182.0091094970703\n",
      "iteration 137, loss 174.4770965576172\n",
      "iteration 138, loss 167.2722625732422\n",
      "iteration 139, loss 160.38217163085938\n",
      "iteration 140, loss 153.79202270507812\n",
      "iteration 141, loss 147.48487854003906\n",
      "iteration 142, loss 141.45045471191406\n",
      "iteration 143, loss 135.6778564453125\n",
      "iteration 144, loss 130.15415954589844\n",
      "iteration 145, loss 124.86515808105469\n",
      "iteration 146, loss 119.80558013916016\n",
      "iteration 147, loss 114.95967102050781\n",
      "iteration 148, loss 110.32122802734375\n",
      "iteration 149, loss 105.87861633300781\n",
      "iteration 150, loss 101.62226104736328\n",
      "iteration 151, loss 97.54736328125\n",
      "iteration 152, loss 93.64498138427734\n",
      "iteration 153, loss 89.90534973144531\n",
      "iteration 154, loss 86.32090759277344\n",
      "iteration 155, loss 82.88752746582031\n",
      "iteration 156, loss 79.59647369384766\n",
      "iteration 157, loss 76.44290161132812\n",
      "iteration 158, loss 73.42025756835938\n",
      "iteration 159, loss 70.52305603027344\n",
      "iteration 160, loss 67.74415588378906\n",
      "iteration 161, loss 65.07984161376953\n",
      "iteration 162, loss 62.52952194213867\n",
      "iteration 163, loss 60.08842849731445\n",
      "iteration 164, loss 57.748958587646484\n",
      "iteration 165, loss 55.50319290161133\n",
      "iteration 166, loss 53.349971771240234\n",
      "iteration 167, loss 51.283199310302734\n",
      "iteration 168, loss 49.29983901977539\n",
      "iteration 169, loss 47.3971061706543\n",
      "iteration 170, loss 45.57057189941406\n",
      "iteration 171, loss 43.817161560058594\n",
      "iteration 172, loss 42.134769439697266\n",
      "iteration 173, loss 40.52011489868164\n",
      "iteration 174, loss 38.969451904296875\n",
      "iteration 175, loss 37.4808464050293\n",
      "iteration 176, loss 36.050621032714844\n",
      "iteration 177, loss 34.67768096923828\n",
      "iteration 178, loss 33.359676361083984\n",
      "iteration 179, loss 32.092864990234375\n",
      "iteration 180, loss 30.877017974853516\n",
      "iteration 181, loss 29.70870018005371\n",
      "iteration 182, loss 28.585914611816406\n",
      "iteration 183, loss 27.507814407348633\n",
      "iteration 184, loss 26.471323013305664\n",
      "iteration 185, loss 25.47530174255371\n",
      "iteration 186, loss 24.517921447753906\n",
      "iteration 187, loss 23.59874153137207\n",
      "iteration 188, loss 22.714540481567383\n",
      "iteration 189, loss 21.865070343017578\n",
      "iteration 190, loss 21.048622131347656\n",
      "iteration 191, loss 20.263195037841797\n",
      "iteration 192, loss 19.508625030517578\n",
      "iteration 193, loss 18.783294677734375\n",
      "iteration 194, loss 18.085777282714844\n",
      "iteration 195, loss 17.41462516784668\n",
      "iteration 196, loss 16.76922607421875\n",
      "iteration 197, loss 16.148332595825195\n",
      "iteration 198, loss 15.551713943481445\n",
      "iteration 199, loss 14.97763729095459\n",
      "iteration 200, loss 14.425623893737793\n",
      "iteration 201, loss 13.894266128540039\n",
      "iteration 202, loss 13.383293151855469\n",
      "iteration 203, loss 12.89164924621582\n",
      "iteration 204, loss 12.418715476989746\n",
      "iteration 205, loss 11.96377182006836\n",
      "iteration 206, loss 11.525762557983398\n",
      "iteration 207, loss 11.104409217834473\n",
      "iteration 208, loss 10.699228286743164\n",
      "iteration 209, loss 10.30887222290039\n",
      "iteration 210, loss 9.933095932006836\n",
      "iteration 211, loss 9.571722030639648\n",
      "iteration 212, loss 9.223299026489258\n",
      "iteration 213, loss 8.888310432434082\n",
      "iteration 214, loss 8.566118240356445\n",
      "iteration 215, loss 8.255465507507324\n",
      "iteration 216, loss 7.956564903259277\n",
      "iteration 217, loss 7.668734073638916\n",
      "iteration 218, loss 7.391599178314209\n",
      "iteration 219, loss 7.1248779296875\n",
      "iteration 220, loss 6.867933750152588\n",
      "iteration 221, loss 6.620260715484619\n",
      "iteration 222, loss 6.382245063781738\n",
      "iteration 223, loss 6.152726650238037\n",
      "iteration 224, loss 5.931341171264648\n",
      "iteration 225, loss 5.7186360359191895\n",
      "iteration 226, loss 5.513542652130127\n",
      "iteration 227, loss 5.316254615783691\n",
      "iteration 228, loss 5.125691890716553\n",
      "iteration 229, loss 4.9424309730529785\n",
      "iteration 230, loss 4.765864372253418\n",
      "iteration 231, loss 4.595701694488525\n",
      "iteration 232, loss 4.432043552398682\n",
      "iteration 233, loss 4.273916721343994\n",
      "iteration 234, loss 4.121645927429199\n",
      "iteration 235, loss 3.975010633468628\n",
      "iteration 236, loss 3.833840847015381\n",
      "iteration 237, loss 3.6976401805877686\n",
      "iteration 238, loss 3.5665066242218018\n",
      "iteration 239, loss 3.440014362335205\n",
      "iteration 240, loss 3.3181302547454834\n",
      "iteration 241, loss 3.2005648612976074\n",
      "iteration 242, loss 3.087374448776245\n",
      "iteration 243, loss 2.9781713485717773\n",
      "iteration 244, loss 2.87312650680542\n",
      "iteration 245, loss 2.7717697620391846\n",
      "iteration 246, loss 2.673833131790161\n",
      "iteration 247, loss 2.5796470642089844\n",
      "iteration 248, loss 2.488724946975708\n",
      "iteration 249, loss 2.401265859603882\n",
      "iteration 250, loss 2.3167266845703125\n",
      "iteration 251, loss 2.2353568077087402\n",
      "iteration 252, loss 2.1568377017974854\n",
      "iteration 253, loss 2.081223487854004\n",
      "iteration 254, loss 2.0082435607910156\n",
      "iteration 255, loss 1.9379276037216187\n",
      "iteration 256, loss 1.8699040412902832\n",
      "iteration 257, loss 1.8044391870498657\n",
      "iteration 258, loss 1.7412796020507812\n",
      "iteration 259, loss 1.6805298328399658\n",
      "iteration 260, loss 1.6217691898345947\n",
      "iteration 261, loss 1.5651881694793701\n",
      "iteration 262, loss 1.510603904724121\n",
      "iteration 263, loss 1.4579627513885498\n",
      "iteration 264, loss 1.4070669412612915\n",
      "iteration 265, loss 1.3580278158187866\n",
      "iteration 266, loss 1.3108339309692383\n",
      "iteration 267, loss 1.2652099132537842\n",
      "iteration 268, loss 1.221186637878418\n",
      "iteration 269, loss 1.178792953491211\n",
      "iteration 270, loss 1.1378464698791504\n",
      "iteration 271, loss 1.0982880592346191\n",
      "iteration 272, loss 1.0602376461029053\n",
      "iteration 273, loss 1.0235159397125244\n",
      "iteration 274, loss 0.9880214929580688\n",
      "iteration 275, loss 0.9537916779518127\n",
      "iteration 276, loss 0.9207680225372314\n",
      "iteration 277, loss 0.8888971209526062\n",
      "iteration 278, loss 0.8581997156143188\n",
      "iteration 279, loss 0.8285161256790161\n",
      "iteration 280, loss 0.7999064922332764\n",
      "iteration 281, loss 0.7723668217658997\n",
      "iteration 282, loss 0.7457073926925659\n",
      "iteration 283, loss 0.719967782497406\n",
      "iteration 284, loss 0.6951389312744141\n",
      "iteration 285, loss 0.6711808443069458\n",
      "iteration 286, loss 0.6480860114097595\n",
      "iteration 287, loss 0.6258158683776855\n",
      "iteration 288, loss 0.604240357875824\n",
      "iteration 289, loss 0.583469033241272\n",
      "iteration 290, loss 0.5633207559585571\n",
      "iteration 291, loss 0.5440149307250977\n",
      "iteration 292, loss 0.5253262519836426\n",
      "iteration 293, loss 0.5072916746139526\n",
      "iteration 294, loss 0.4898599684238434\n",
      "iteration 295, loss 0.4730376601219177\n",
      "iteration 296, loss 0.45683997869491577\n",
      "iteration 297, loss 0.4412134885787964\n",
      "iteration 298, loss 0.4260503649711609\n",
      "iteration 299, loss 0.41144147515296936\n",
      "iteration 300, loss 0.39738717675209045\n",
      "iteration 301, loss 0.3837859332561493\n",
      "iteration 302, loss 0.3706659972667694\n",
      "iteration 303, loss 0.35797715187072754\n",
      "iteration 304, loss 0.3457866907119751\n",
      "iteration 305, loss 0.3339410424232483\n",
      "iteration 306, loss 0.3225526809692383\n",
      "iteration 307, loss 0.31151083111763\n",
      "iteration 308, loss 0.3008996546268463\n",
      "iteration 309, loss 0.29063281416893005\n",
      "iteration 310, loss 0.2807544469833374\n",
      "iteration 311, loss 0.27118828892707825\n",
      "iteration 312, loss 0.2618982195854187\n",
      "iteration 313, loss 0.25298163294792175\n",
      "iteration 314, loss 0.24436591565608978\n",
      "iteration 315, loss 0.23606644570827484\n",
      "iteration 316, loss 0.22802987694740295\n",
      "iteration 317, loss 0.22026415169239044\n",
      "iteration 318, loss 0.21277105808258057\n",
      "iteration 319, loss 0.20551219582557678\n",
      "iteration 320, loss 0.19855564832687378\n",
      "iteration 321, loss 0.19181041419506073\n",
      "iteration 322, loss 0.1852743923664093\n",
      "iteration 323, loss 0.17901082336902618\n",
      "iteration 324, loss 0.17293024063110352\n",
      "iteration 325, loss 0.16706383228302002\n",
      "iteration 326, loss 0.161399707198143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 327, loss 0.1559273898601532\n",
      "iteration 328, loss 0.15061362087726593\n",
      "iteration 329, loss 0.14551061391830444\n",
      "iteration 330, loss 0.14060448110103607\n",
      "iteration 331, loss 0.13584403693675995\n",
      "iteration 332, loss 0.13124772906303406\n",
      "iteration 333, loss 0.12679897248744965\n",
      "iteration 334, loss 0.12249643355607986\n",
      "iteration 335, loss 0.11837176978588104\n",
      "iteration 336, loss 0.11436454951763153\n",
      "iteration 337, loss 0.11048740148544312\n",
      "iteration 338, loss 0.10675722360610962\n",
      "iteration 339, loss 0.10313135385513306\n",
      "iteration 340, loss 0.09967608004808426\n",
      "iteration 341, loss 0.09629126638174057\n",
      "iteration 342, loss 0.09302114695310593\n",
      "iteration 343, loss 0.08989352732896805\n",
      "iteration 344, loss 0.08687429130077362\n",
      "iteration 345, loss 0.08393412083387375\n",
      "iteration 346, loss 0.08110698312520981\n",
      "iteration 347, loss 0.07838107645511627\n",
      "iteration 348, loss 0.07575645297765732\n",
      "iteration 349, loss 0.07321449369192123\n",
      "iteration 350, loss 0.07072597742080688\n",
      "iteration 351, loss 0.06834151595830917\n",
      "iteration 352, loss 0.06604891270399094\n",
      "iteration 353, loss 0.06382373720407486\n",
      "iteration 354, loss 0.061690740287303925\n",
      "iteration 355, loss 0.05962047725915909\n",
      "iteration 356, loss 0.057623524218797684\n",
      "iteration 357, loss 0.055694788694381714\n",
      "iteration 358, loss 0.05381757393479347\n",
      "iteration 359, loss 0.052012018859386444\n",
      "iteration 360, loss 0.0502672977745533\n",
      "iteration 361, loss 0.0485847108066082\n",
      "iteration 362, loss 0.046945370733737946\n",
      "iteration 363, loss 0.04538717120885849\n",
      "iteration 364, loss 0.04385888949036598\n",
      "iteration 365, loss 0.042398806661367416\n",
      "iteration 366, loss 0.04097911715507507\n",
      "iteration 367, loss 0.03962017595767975\n",
      "iteration 368, loss 0.03829047083854675\n",
      "iteration 369, loss 0.037014156579971313\n",
      "iteration 370, loss 0.035774245858192444\n",
      "iteration 371, loss 0.034581661224365234\n",
      "iteration 372, loss 0.03343869000673294\n",
      "iteration 373, loss 0.0323314405977726\n",
      "iteration 374, loss 0.031248437240719795\n",
      "iteration 375, loss 0.030196640640497208\n",
      "iteration 376, loss 0.02919575944542885\n",
      "iteration 377, loss 0.028225982561707497\n",
      "iteration 378, loss 0.027297500520944595\n",
      "iteration 379, loss 0.026390990242362022\n",
      "iteration 380, loss 0.025512533262372017\n",
      "iteration 381, loss 0.024666229262948036\n",
      "iteration 382, loss 0.023846039548516273\n",
      "iteration 383, loss 0.023060640320181847\n",
      "iteration 384, loss 0.022295067086815834\n",
      "iteration 385, loss 0.021549275144934654\n",
      "iteration 386, loss 0.020839575678110123\n",
      "iteration 387, loss 0.020150117576122284\n",
      "iteration 388, loss 0.019494784995913506\n",
      "iteration 389, loss 0.018846068531274796\n",
      "iteration 390, loss 0.018224196508526802\n",
      "iteration 391, loss 0.017616255208849907\n",
      "iteration 392, loss 0.017039090394973755\n",
      "iteration 393, loss 0.016482878476381302\n",
      "iteration 394, loss 0.015941359102725983\n",
      "iteration 395, loss 0.015419462695717812\n",
      "iteration 396, loss 0.014910866506397724\n",
      "iteration 397, loss 0.01442319992929697\n",
      "iteration 398, loss 0.013945531100034714\n",
      "iteration 399, loss 0.013488449156284332\n",
      "iteration 400, loss 0.01304895244538784\n",
      "iteration 401, loss 0.012628333643078804\n",
      "iteration 402, loss 0.012212797068059444\n",
      "iteration 403, loss 0.011825191788375378\n",
      "iteration 404, loss 0.01143718883395195\n",
      "iteration 405, loss 0.011067880317568779\n",
      "iteration 406, loss 0.010707715526223183\n",
      "iteration 407, loss 0.010362776927649975\n",
      "iteration 408, loss 0.010024752467870712\n",
      "iteration 409, loss 0.009701517410576344\n",
      "iteration 410, loss 0.009392627514898777\n",
      "iteration 411, loss 0.009083794429898262\n",
      "iteration 412, loss 0.008797791786491871\n",
      "iteration 413, loss 0.008516180329024792\n",
      "iteration 414, loss 0.008244181051850319\n",
      "iteration 415, loss 0.007977158762514591\n",
      "iteration 416, loss 0.007725718896836042\n",
      "iteration 417, loss 0.00747661292552948\n",
      "iteration 418, loss 0.0072414688766002655\n",
      "iteration 419, loss 0.007015454117208719\n",
      "iteration 420, loss 0.006796484347432852\n",
      "iteration 421, loss 0.006584197748452425\n",
      "iteration 422, loss 0.006378272082656622\n",
      "iteration 423, loss 0.0061767627485096455\n",
      "iteration 424, loss 0.005985007621347904\n",
      "iteration 425, loss 0.005795326083898544\n",
      "iteration 426, loss 0.00562000647187233\n",
      "iteration 427, loss 0.005446468945592642\n",
      "iteration 428, loss 0.005277151241898537\n",
      "iteration 429, loss 0.005112804938107729\n",
      "iteration 430, loss 0.004959539044648409\n",
      "iteration 431, loss 0.004804524593055248\n",
      "iteration 432, loss 0.004659126978367567\n",
      "iteration 433, loss 0.0045151375234127045\n",
      "iteration 434, loss 0.0043769958429038525\n",
      "iteration 435, loss 0.00424680532887578\n",
      "iteration 436, loss 0.0041181910783052444\n",
      "iteration 437, loss 0.003995499573647976\n",
      "iteration 438, loss 0.003876173635944724\n",
      "iteration 439, loss 0.003761280095204711\n",
      "iteration 440, loss 0.0036465812008827925\n",
      "iteration 441, loss 0.003539727535098791\n",
      "iteration 442, loss 0.0034375376999378204\n",
      "iteration 443, loss 0.003333078697323799\n",
      "iteration 444, loss 0.0032370423432439566\n",
      "iteration 445, loss 0.003143325448036194\n",
      "iteration 446, loss 0.0030535960104316473\n",
      "iteration 447, loss 0.002964368788525462\n",
      "iteration 448, loss 0.0028781124856323004\n",
      "iteration 449, loss 0.0027975859120488167\n",
      "iteration 450, loss 0.002715464448556304\n",
      "iteration 451, loss 0.0026389292906969786\n",
      "iteration 452, loss 0.0025633596815168858\n",
      "iteration 453, loss 0.0024908678606152534\n",
      "iteration 454, loss 0.0024227439425885677\n",
      "iteration 455, loss 0.0023545653093606234\n",
      "iteration 456, loss 0.002289523370563984\n",
      "iteration 457, loss 0.0022249130997806787\n",
      "iteration 458, loss 0.0021644297521561384\n",
      "iteration 459, loss 0.002103708451613784\n",
      "iteration 460, loss 0.0020479722879827023\n",
      "iteration 461, loss 0.001991990953683853\n",
      "iteration 462, loss 0.0019362927414476871\n",
      "iteration 463, loss 0.0018858304247260094\n",
      "iteration 464, loss 0.0018329961458221078\n",
      "iteration 465, loss 0.0017851295415312052\n",
      "iteration 466, loss 0.0017366408137604594\n",
      "iteration 467, loss 0.001691216486506164\n",
      "iteration 468, loss 0.0016470304690301418\n",
      "iteration 469, loss 0.0016039814800024033\n",
      "iteration 470, loss 0.0015600003534927964\n",
      "iteration 471, loss 0.0015177951427176595\n",
      "iteration 472, loss 0.0014791208086535335\n",
      "iteration 473, loss 0.001441493513993919\n",
      "iteration 474, loss 0.0014043233823031187\n",
      "iteration 475, loss 0.0013684091391041875\n",
      "iteration 476, loss 0.0013326775515452027\n",
      "iteration 477, loss 0.0012986839283257723\n",
      "iteration 478, loss 0.0012658715713769197\n",
      "iteration 479, loss 0.0012342784320935607\n",
      "iteration 480, loss 0.0012041786685585976\n",
      "iteration 481, loss 0.0011727383825927973\n",
      "iteration 482, loss 0.0011441524839028716\n",
      "iteration 483, loss 0.0011155008105561137\n",
      "iteration 484, loss 0.0010876930318772793\n",
      "iteration 485, loss 0.001061547314748168\n",
      "iteration 486, loss 0.001036637811921537\n",
      "iteration 487, loss 0.0010093614691868424\n",
      "iteration 488, loss 0.0009867837652564049\n",
      "iteration 489, loss 0.0009627443505451083\n",
      "iteration 490, loss 0.0009403228177689016\n",
      "iteration 491, loss 0.0009177624597214162\n",
      "iteration 492, loss 0.0008961381972767413\n",
      "iteration 493, loss 0.0008750908891670406\n",
      "iteration 494, loss 0.0008556350949220359\n",
      "iteration 495, loss 0.00083512207493186\n",
      "iteration 496, loss 0.0008157145930454135\n",
      "iteration 497, loss 0.0007970228907652199\n",
      "iteration 498, loss 0.0007801217143423855\n",
      "iteration 499, loss 0.0007616226794198155\n"
     ]
    }
   ],
   "source": [
    "# implementing the same example in torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "W1 = torch.randn(D_in, H, dtype=datatype, device=device)\n",
    "W2 = torch.randn(H, D_out, dtype=datatype, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    z = x.mm(W1)\n",
    "    h = z.clamp(min=0) # ReLU(z)\n",
    "    y_pred = h.mm(W2)\n",
    "    \n",
    "    loss = (y - y_pred).pow(2).sum().item()\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss))\n",
    "    \n",
    "    # Backward pass\n",
    "    dy_pred = 2*(y_pred - y)\n",
    "    dW2 = h.t().mm(dy_pred)\n",
    "    dh = dy_pred.mm(W2.t())\n",
    "    dz = dh.clone()\n",
    "    dz[h <= 0] = 0 # elementwise signum function\n",
    "    dW1 = x.t().mm(dz)\n",
    "    \n",
    "    # Upgrade weights\n",
    "    W2 -= learning_rate * dW2\n",
    "    W1 -= learning_rate * dW1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PyTorch: Tensors and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 31663520.0\n",
      "iteration 1, loss 27991946.0\n",
      "iteration 2, loss 28222632.0\n",
      "iteration 3, loss 27743500.0\n",
      "iteration 4, loss 23999806.0\n",
      "iteration 5, loss 17425660.0\n",
      "iteration 6, loss 10814874.0\n",
      "iteration 7, loss 6106819.0\n",
      "iteration 8, loss 3432409.25\n",
      "iteration 9, loss 2053820.5\n",
      "iteration 10, loss 1353646.875\n",
      "iteration 11, loss 977970.6875\n",
      "iteration 12, loss 756797.8125\n",
      "iteration 13, loss 612187.25\n",
      "iteration 14, loss 508853.96875\n",
      "iteration 15, loss 430070.71875\n",
      "iteration 16, loss 367380.75\n",
      "iteration 17, loss 316221.15625\n",
      "iteration 18, loss 273791.03125\n",
      "iteration 19, loss 238221.375\n",
      "iteration 20, loss 208145.65625\n",
      "iteration 21, loss 182562.703125\n",
      "iteration 22, loss 160715.484375\n",
      "iteration 23, loss 141991.375\n",
      "iteration 24, loss 125828.609375\n",
      "iteration 25, loss 111829.5234375\n",
      "iteration 26, loss 99659.0859375\n",
      "iteration 27, loss 89026.8515625\n",
      "iteration 28, loss 79714.890625\n",
      "iteration 29, loss 71538.0859375\n",
      "iteration 30, loss 64334.84765625\n",
      "iteration 31, loss 57972.1328125\n",
      "iteration 32, loss 52338.06640625\n",
      "iteration 33, loss 47334.7109375\n",
      "iteration 34, loss 42883.6171875\n",
      "iteration 35, loss 38915.890625\n",
      "iteration 36, loss 35367.83984375\n",
      "iteration 37, loss 32189.921875\n",
      "iteration 38, loss 29337.56640625\n",
      "iteration 39, loss 26778.41015625\n",
      "iteration 40, loss 24473.419921875\n",
      "iteration 41, loss 22392.955078125\n",
      "iteration 42, loss 20513.390625\n",
      "iteration 43, loss 18813.37109375\n",
      "iteration 44, loss 17271.060546875\n",
      "iteration 45, loss 15870.7412109375\n",
      "iteration 46, loss 14598.7880859375\n",
      "iteration 47, loss 13443.263671875\n",
      "iteration 48, loss 12389.1962890625\n",
      "iteration 49, loss 11427.5166015625\n",
      "iteration 50, loss 10548.630859375\n",
      "iteration 51, loss 9744.884765625\n",
      "iteration 52, loss 9008.794921875\n",
      "iteration 53, loss 8333.9453125\n",
      "iteration 54, loss 7715.0693359375\n",
      "iteration 55, loss 7147.208984375\n",
      "iteration 56, loss 6625.298828125\n",
      "iteration 57, loss 6145.12255859375\n",
      "iteration 58, loss 5703.203125\n",
      "iteration 59, loss 5295.73828125\n",
      "iteration 60, loss 4920.4970703125\n",
      "iteration 61, loss 4574.1240234375\n",
      "iteration 62, loss 4254.76318359375\n",
      "iteration 63, loss 3959.353759765625\n",
      "iteration 64, loss 3686.2197265625\n",
      "iteration 65, loss 3433.521484375\n",
      "iteration 66, loss 3199.61572265625\n",
      "iteration 67, loss 2982.95263671875\n",
      "iteration 68, loss 2782.265625\n",
      "iteration 69, loss 2596.2177734375\n",
      "iteration 70, loss 2423.619384765625\n",
      "iteration 71, loss 2263.39794921875\n",
      "iteration 72, loss 2114.532958984375\n",
      "iteration 73, loss 1976.1968994140625\n",
      "iteration 74, loss 1847.4130859375\n",
      "iteration 75, loss 1727.638427734375\n",
      "iteration 76, loss 1616.3031005859375\n",
      "iteration 77, loss 1512.6944580078125\n",
      "iteration 78, loss 1416.0885009765625\n",
      "iteration 79, loss 1326.0179443359375\n",
      "iteration 80, loss 1242.0869140625\n",
      "iteration 81, loss 1163.81494140625\n",
      "iteration 82, loss 1090.7005615234375\n",
      "iteration 83, loss 1022.46337890625\n",
      "iteration 84, loss 958.7726440429688\n",
      "iteration 85, loss 899.2522583007812\n",
      "iteration 86, loss 843.6319580078125\n",
      "iteration 87, loss 791.6369018554688\n",
      "iteration 88, loss 743.0153198242188\n",
      "iteration 89, loss 697.5232543945312\n",
      "iteration 90, loss 654.9407348632812\n",
      "iteration 91, loss 615.105224609375\n",
      "iteration 92, loss 577.8304443359375\n",
      "iteration 93, loss 542.9169311523438\n",
      "iteration 94, loss 510.20928955078125\n",
      "iteration 95, loss 479.5595703125\n",
      "iteration 96, loss 450.8553466796875\n",
      "iteration 97, loss 423.942138671875\n",
      "iteration 98, loss 398.6986389160156\n",
      "iteration 99, loss 375.0305480957031\n",
      "iteration 100, loss 352.83453369140625\n",
      "iteration 101, loss 331.9945068359375\n",
      "iteration 102, loss 312.43743896484375\n",
      "iteration 103, loss 294.079345703125\n",
      "iteration 104, loss 276.8601379394531\n",
      "iteration 105, loss 260.6806640625\n",
      "iteration 106, loss 245.4823455810547\n",
      "iteration 107, loss 231.21290588378906\n",
      "iteration 108, loss 217.81260681152344\n",
      "iteration 109, loss 205.2072296142578\n",
      "iteration 110, loss 193.36346435546875\n",
      "iteration 111, loss 182.23025512695312\n",
      "iteration 112, loss 171.76333618164062\n",
      "iteration 113, loss 161.91928100585938\n",
      "iteration 114, loss 152.652099609375\n",
      "iteration 115, loss 143.93804931640625\n",
      "iteration 116, loss 135.74102783203125\n",
      "iteration 117, loss 128.0221710205078\n",
      "iteration 118, loss 120.75773620605469\n",
      "iteration 119, loss 113.92736053466797\n",
      "iteration 120, loss 107.4921875\n",
      "iteration 121, loss 101.43383026123047\n",
      "iteration 122, loss 95.72727966308594\n",
      "iteration 123, loss 90.35404968261719\n",
      "iteration 124, loss 85.29203796386719\n",
      "iteration 125, loss 80.51946258544922\n",
      "iteration 126, loss 76.0230484008789\n",
      "iteration 127, loss 71.78785705566406\n",
      "iteration 128, loss 67.79426574707031\n",
      "iteration 129, loss 64.03202819824219\n",
      "iteration 130, loss 60.48379135131836\n",
      "iteration 131, loss 57.13860321044922\n",
      "iteration 132, loss 53.98189926147461\n",
      "iteration 133, loss 51.005977630615234\n",
      "iteration 134, loss 48.19916534423828\n",
      "iteration 135, loss 45.5522346496582\n",
      "iteration 136, loss 43.05381774902344\n",
      "iteration 137, loss 40.69600296020508\n",
      "iteration 138, loss 38.47059631347656\n",
      "iteration 139, loss 36.372066497802734\n",
      "iteration 140, loss 34.389747619628906\n",
      "iteration 141, loss 32.5190315246582\n",
      "iteration 142, loss 30.752944946289062\n",
      "iteration 143, loss 29.085702896118164\n",
      "iteration 144, loss 27.510290145874023\n",
      "iteration 145, loss 26.02349090576172\n",
      "iteration 146, loss 24.61829376220703\n",
      "iteration 147, loss 23.29203224182129\n",
      "iteration 148, loss 22.0379581451416\n",
      "iteration 149, loss 20.853303909301758\n",
      "iteration 150, loss 19.73370933532715\n",
      "iteration 151, loss 18.677013397216797\n",
      "iteration 152, loss 17.677024841308594\n",
      "iteration 153, loss 16.731996536254883\n",
      "iteration 154, loss 15.837964057922363\n",
      "iteration 155, loss 14.994571685791016\n",
      "iteration 156, loss 14.196133613586426\n",
      "iteration 157, loss 13.440958023071289\n",
      "iteration 158, loss 12.727604866027832\n",
      "iteration 159, loss 12.052752494812012\n",
      "iteration 160, loss 11.414819717407227\n",
      "iteration 161, loss 10.81142520904541\n",
      "iteration 162, loss 10.240175247192383\n",
      "iteration 163, loss 9.700557708740234\n",
      "iteration 164, loss 9.189424514770508\n",
      "iteration 165, loss 8.705938339233398\n",
      "iteration 166, loss 8.248597145080566\n",
      "iteration 167, loss 7.8162031173706055\n",
      "iteration 168, loss 7.406139850616455\n",
      "iteration 169, loss 7.019251823425293\n",
      "iteration 170, loss 6.65392541885376\n",
      "iteration 171, loss 6.308075428009033\n",
      "iteration 172, loss 5.980283260345459\n",
      "iteration 173, loss 5.670199871063232\n",
      "iteration 174, loss 5.376194477081299\n",
      "iteration 175, loss 5.097930908203125\n",
      "iteration 176, loss 4.834532260894775\n",
      "iteration 177, loss 4.585125923156738\n",
      "iteration 178, loss 4.348464012145996\n",
      "iteration 179, loss 4.124988555908203\n",
      "iteration 180, loss 3.9129650592803955\n",
      "iteration 181, loss 3.712061882019043\n",
      "iteration 182, loss 3.5215532779693604\n",
      "iteration 183, loss 3.3413374423980713\n",
      "iteration 184, loss 3.1701622009277344\n",
      "iteration 185, loss 3.00801157951355\n",
      "iteration 186, loss 2.854531764984131\n",
      "iteration 187, loss 2.708818197250366\n",
      "iteration 188, loss 2.5708765983581543\n",
      "iteration 189, loss 2.440091371536255\n",
      "iteration 190, loss 2.316037893295288\n",
      "iteration 191, loss 2.1984896659851074\n",
      "iteration 192, loss 2.086919069290161\n",
      "iteration 193, loss 1.9811816215515137\n",
      "iteration 194, loss 1.8809891939163208\n",
      "iteration 195, loss 1.7858853340148926\n",
      "iteration 196, loss 1.695708155632019\n",
      "iteration 197, loss 1.6101316213607788\n",
      "iteration 198, loss 1.5290498733520508\n",
      "iteration 199, loss 1.452046513557434\n",
      "iteration 200, loss 1.3790520429611206\n",
      "iteration 201, loss 1.309824824333191\n",
      "iteration 202, loss 1.2440440654754639\n",
      "iteration 203, loss 1.181709885597229\n",
      "iteration 204, loss 1.1225234270095825\n",
      "iteration 205, loss 1.066343069076538\n",
      "iteration 206, loss 1.0131381750106812\n",
      "iteration 207, loss 0.9624803066253662\n",
      "iteration 208, loss 0.9144824743270874\n",
      "iteration 209, loss 0.8689194917678833\n",
      "iteration 210, loss 0.8255695700645447\n",
      "iteration 211, loss 0.7846388816833496\n",
      "iteration 212, loss 0.7456256747245789\n",
      "iteration 213, loss 0.7086471915245056\n",
      "iteration 214, loss 0.6734672784805298\n",
      "iteration 215, loss 0.6401062607765198\n",
      "iteration 216, loss 0.6084336042404175\n",
      "iteration 217, loss 0.5783517360687256\n",
      "iteration 218, loss 0.5497480034828186\n",
      "iteration 219, loss 0.52262943983078\n",
      "iteration 220, loss 0.49684247374534607\n",
      "iteration 221, loss 0.4723106920719147\n",
      "iteration 222, loss 0.44910311698913574\n",
      "iteration 223, loss 0.4270317554473877\n",
      "iteration 224, loss 0.40603211522102356\n",
      "iteration 225, loss 0.38609686493873596\n",
      "iteration 226, loss 0.3671764135360718\n",
      "iteration 227, loss 0.3491798937320709\n",
      "iteration 228, loss 0.3320690989494324\n",
      "iteration 229, loss 0.31582608819007874\n",
      "iteration 230, loss 0.3003675043582916\n",
      "iteration 231, loss 0.28571149706840515\n",
      "iteration 232, loss 0.2717733681201935\n",
      "iteration 233, loss 0.25851333141326904\n",
      "iteration 234, loss 0.24589982628822327\n",
      "iteration 235, loss 0.23393401503562927\n",
      "iteration 236, loss 0.22259999811649323\n",
      "iteration 237, loss 0.21178776025772095\n",
      "iteration 238, loss 0.20150303840637207\n",
      "iteration 239, loss 0.19168438017368317\n",
      "iteration 240, loss 0.18239180743694305\n",
      "iteration 241, loss 0.1735554039478302\n",
      "iteration 242, loss 0.16516990959644318\n",
      "iteration 243, loss 0.15717406570911407\n",
      "iteration 244, loss 0.14959944784641266\n",
      "iteration 245, loss 0.1423625499010086\n",
      "iteration 246, loss 0.13549034297466278\n",
      "iteration 247, loss 0.12898847460746765\n",
      "iteration 248, loss 0.12275637686252594\n",
      "iteration 249, loss 0.1168510764837265\n",
      "iteration 250, loss 0.11120285838842392\n",
      "iteration 251, loss 0.10585086047649384\n",
      "iteration 252, loss 0.10079235583543777\n",
      "iteration 253, loss 0.09595537930727005\n",
      "iteration 254, loss 0.09136278927326202\n",
      "iteration 255, loss 0.08697933703660965\n",
      "iteration 256, loss 0.08281102776527405\n",
      "iteration 257, loss 0.07884593307971954\n",
      "iteration 258, loss 0.07506955415010452\n",
      "iteration 259, loss 0.07148180902004242\n",
      "iteration 260, loss 0.0680965781211853\n",
      "iteration 261, loss 0.06482300162315369\n",
      "iteration 262, loss 0.06174200028181076\n",
      "iteration 263, loss 0.058818429708480835\n",
      "iteration 264, loss 0.05601281672716141\n",
      "iteration 265, loss 0.053347982466220856\n",
      "iteration 266, loss 0.05080486088991165\n",
      "iteration 267, loss 0.048402801156044006\n",
      "iteration 268, loss 0.04609883204102516\n",
      "iteration 269, loss 0.043925605714321136\n",
      "iteration 270, loss 0.04185115918517113\n",
      "iteration 271, loss 0.03986860066652298\n",
      "iteration 272, loss 0.03799647092819214\n",
      "iteration 273, loss 0.03620081767439842\n",
      "iteration 274, loss 0.03449489548802376\n",
      "iteration 275, loss 0.03287136182188988\n",
      "iteration 276, loss 0.03132273629307747\n",
      "iteration 277, loss 0.029849613085389137\n",
      "iteration 278, loss 0.028461210429668427\n",
      "iteration 279, loss 0.027117522433400154\n",
      "iteration 280, loss 0.0258470606058836\n",
      "iteration 281, loss 0.024631716310977936\n",
      "iteration 282, loss 0.023488827049732208\n",
      "iteration 283, loss 0.02239362895488739\n",
      "iteration 284, loss 0.021347224712371826\n",
      "iteration 285, loss 0.020347753539681435\n",
      "iteration 286, loss 0.019402367994189262\n",
      "iteration 287, loss 0.018501577898859978\n",
      "iteration 288, loss 0.017646266147494316\n",
      "iteration 289, loss 0.016831960529088974\n",
      "iteration 290, loss 0.01604113169014454\n",
      "iteration 291, loss 0.015302837826311588\n",
      "iteration 292, loss 0.014599348418414593\n",
      "iteration 293, loss 0.013928033411502838\n",
      "iteration 294, loss 0.013286441564559937\n",
      "iteration 295, loss 0.012677849270403385\n",
      "iteration 296, loss 0.012101318687200546\n",
      "iteration 297, loss 0.011541079729795456\n",
      "iteration 298, loss 0.011017716489732265\n",
      "iteration 299, loss 0.010522935539484024\n",
      "iteration 300, loss 0.010041109286248684\n",
      "iteration 301, loss 0.00959093403071165\n",
      "iteration 302, loss 0.009157597087323666\n",
      "iteration 303, loss 0.008752977475523949\n",
      "iteration 304, loss 0.008350789546966553\n",
      "iteration 305, loss 0.007971680723130703\n",
      "iteration 306, loss 0.007604648824781179\n",
      "iteration 307, loss 0.007276797201484442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 308, loss 0.006953850854188204\n",
      "iteration 309, loss 0.006643050350248814\n",
      "iteration 310, loss 0.006347417365759611\n",
      "iteration 311, loss 0.006067201029509306\n",
      "iteration 312, loss 0.005804762244224548\n",
      "iteration 313, loss 0.005549834109842777\n",
      "iteration 314, loss 0.0053060175850987434\n",
      "iteration 315, loss 0.005072643514722586\n",
      "iteration 316, loss 0.004850757773965597\n",
      "iteration 317, loss 0.0046408153139054775\n",
      "iteration 318, loss 0.004441520664840937\n",
      "iteration 319, loss 0.004253089893609285\n",
      "iteration 320, loss 0.0040689678862690926\n",
      "iteration 321, loss 0.003894977970048785\n",
      "iteration 322, loss 0.0037302093114703894\n",
      "iteration 323, loss 0.0035708374343812466\n",
      "iteration 324, loss 0.003420985070988536\n",
      "iteration 325, loss 0.0032768857199698687\n",
      "iteration 326, loss 0.003140539862215519\n",
      "iteration 327, loss 0.003007710911333561\n",
      "iteration 328, loss 0.0028847353532910347\n",
      "iteration 329, loss 0.0027661144267767668\n",
      "iteration 330, loss 0.0026554809883236885\n",
      "iteration 331, loss 0.002545753261074424\n",
      "iteration 332, loss 0.0024425797164440155\n",
      "iteration 333, loss 0.0023442727979272604\n",
      "iteration 334, loss 0.0022521798964589834\n",
      "iteration 335, loss 0.002162782009691\n",
      "iteration 336, loss 0.002073760377243161\n",
      "iteration 337, loss 0.001993653830140829\n",
      "iteration 338, loss 0.0019160817610099912\n",
      "iteration 339, loss 0.0018410400953143835\n",
      "iteration 340, loss 0.0017705305945128202\n",
      "iteration 341, loss 0.0017011762829497457\n",
      "iteration 342, loss 0.0016373316757380962\n",
      "iteration 343, loss 0.001574792666360736\n",
      "iteration 344, loss 0.0015164942014962435\n",
      "iteration 345, loss 0.001460052328184247\n",
      "iteration 346, loss 0.0014060394605621696\n",
      "iteration 347, loss 0.001351496553979814\n",
      "iteration 348, loss 0.0013052141293883324\n",
      "iteration 349, loss 0.0012564788339659572\n",
      "iteration 350, loss 0.001209982088766992\n",
      "iteration 351, loss 0.0011656810529530048\n",
      "iteration 352, loss 0.001127085997723043\n",
      "iteration 353, loss 0.0010866083903238177\n",
      "iteration 354, loss 0.0010472832946106791\n",
      "iteration 355, loss 0.001011402579024434\n",
      "iteration 356, loss 0.0009755084756761789\n",
      "iteration 357, loss 0.0009428355842828751\n",
      "iteration 358, loss 0.000911865325178951\n",
      "iteration 359, loss 0.0008796459878794849\n",
      "iteration 360, loss 0.000849930802360177\n",
      "iteration 361, loss 0.0008220616728067398\n",
      "iteration 362, loss 0.0007931060972623527\n",
      "iteration 363, loss 0.0007672212668694556\n",
      "iteration 364, loss 0.0007423224742524326\n",
      "iteration 365, loss 0.0007175793871283531\n",
      "iteration 366, loss 0.0006946984212845564\n",
      "iteration 367, loss 0.0006729604210704565\n",
      "iteration 368, loss 0.0006513265543617308\n",
      "iteration 369, loss 0.0006304204580374062\n",
      "iteration 370, loss 0.0006105622160248458\n",
      "iteration 371, loss 0.0005912113701924682\n",
      "iteration 372, loss 0.0005732022691518068\n",
      "iteration 373, loss 0.0005552966031245887\n",
      "iteration 374, loss 0.0005386067205108702\n",
      "iteration 375, loss 0.0005221624160185456\n",
      "iteration 376, loss 0.0005065996083430946\n",
      "iteration 377, loss 0.0004928515991196036\n",
      "iteration 378, loss 0.00047754347906447947\n",
      "iteration 379, loss 0.00046386453323066235\n",
      "iteration 380, loss 0.00044998020166531205\n",
      "iteration 381, loss 0.00043737064697779715\n",
      "iteration 382, loss 0.00042503507575020194\n",
      "iteration 383, loss 0.00041306173079647124\n",
      "iteration 384, loss 0.00040252917096950114\n",
      "iteration 385, loss 0.000391528446925804\n",
      "iteration 386, loss 0.0003804200969170779\n",
      "iteration 387, loss 0.0003704449627548456\n",
      "iteration 388, loss 0.00035959293018095195\n",
      "iteration 389, loss 0.00034940202021971345\n",
      "iteration 390, loss 0.00034052698174491525\n",
      "iteration 391, loss 0.0003319307288620621\n",
      "iteration 392, loss 0.00032291843672282994\n",
      "iteration 393, loss 0.00031505629885941744\n",
      "iteration 394, loss 0.00030644965590909123\n",
      "iteration 395, loss 0.0002988681080751121\n",
      "iteration 396, loss 0.000290463533019647\n",
      "iteration 397, loss 0.00028421133174560964\n",
      "iteration 398, loss 0.0002768845297396183\n",
      "iteration 399, loss 0.0002696339797694236\n",
      "iteration 400, loss 0.00026248922222293913\n",
      "iteration 401, loss 0.00025608547730371356\n",
      "iteration 402, loss 0.0002502487041056156\n",
      "iteration 403, loss 0.0002444733108859509\n",
      "iteration 404, loss 0.00023827998666092753\n",
      "iteration 405, loss 0.0002328204136574641\n",
      "iteration 406, loss 0.00022741439170204103\n",
      "iteration 407, loss 0.00022216410434339195\n",
      "iteration 408, loss 0.0002173397078877315\n",
      "iteration 409, loss 0.00021189230028539896\n",
      "iteration 410, loss 0.00020710502576548606\n",
      "iteration 411, loss 0.00020186589972581714\n",
      "iteration 412, loss 0.0001977489300770685\n",
      "iteration 413, loss 0.00019335707474965602\n",
      "iteration 414, loss 0.00018864826415665448\n",
      "iteration 415, loss 0.00018572133558336645\n",
      "iteration 416, loss 0.0001815009891288355\n",
      "iteration 417, loss 0.00017686927458271384\n",
      "iteration 418, loss 0.00017313787247985601\n",
      "iteration 419, loss 0.00016920700727496296\n",
      "iteration 420, loss 0.00016601799870841205\n",
      "iteration 421, loss 0.00016235890507232398\n",
      "iteration 422, loss 0.0001587980950716883\n",
      "iteration 423, loss 0.00015623106446582824\n",
      "iteration 424, loss 0.00015304956468753517\n",
      "iteration 425, loss 0.0001494270545663312\n",
      "iteration 426, loss 0.00014691022806800902\n",
      "iteration 427, loss 0.00014372571604326367\n",
      "iteration 428, loss 0.00014059444947633892\n",
      "iteration 429, loss 0.00013794533151667565\n",
      "iteration 430, loss 0.00013533738092519343\n",
      "iteration 431, loss 0.0001326580677414313\n",
      "iteration 432, loss 0.00012989292736165226\n",
      "iteration 433, loss 0.00012755523493979126\n",
      "iteration 434, loss 0.00012493903341237456\n",
      "iteration 435, loss 0.0001224990701302886\n",
      "iteration 436, loss 0.00012032302765874192\n",
      "iteration 437, loss 0.00011820355575764552\n",
      "iteration 438, loss 0.00011597180855460465\n",
      "iteration 439, loss 0.00011417114001233131\n",
      "iteration 440, loss 0.00011208708747290075\n",
      "iteration 441, loss 0.00010997931531164795\n",
      "iteration 442, loss 0.00010806561476783827\n",
      "iteration 443, loss 0.00010589956946205348\n",
      "iteration 444, loss 0.00010401233157608658\n",
      "iteration 445, loss 0.00010255987581331283\n",
      "iteration 446, loss 0.00010080967331305146\n",
      "iteration 447, loss 9.925877384375781e-05\n",
      "iteration 448, loss 9.723626135382801e-05\n",
      "iteration 449, loss 9.534731361782178e-05\n",
      "iteration 450, loss 9.399684495292604e-05\n",
      "iteration 451, loss 9.253142343368381e-05\n",
      "iteration 452, loss 9.101513569476083e-05\n",
      "iteration 453, loss 8.931475895224139e-05\n",
      "iteration 454, loss 8.836227789288387e-05\n",
      "iteration 455, loss 8.673356933286414e-05\n",
      "iteration 456, loss 8.530859486199915e-05\n",
      "iteration 457, loss 8.404636173509061e-05\n",
      "iteration 458, loss 8.263895142590627e-05\n",
      "iteration 459, loss 8.14919694676064e-05\n",
      "iteration 460, loss 8.019832603167742e-05\n",
      "iteration 461, loss 7.859763718442991e-05\n",
      "iteration 462, loss 7.758538413327187e-05\n",
      "iteration 463, loss 7.647449820069596e-05\n",
      "iteration 464, loss 7.533597090514377e-05\n",
      "iteration 465, loss 7.415201980620623e-05\n",
      "iteration 466, loss 7.319711585296318e-05\n",
      "iteration 467, loss 7.19972958904691e-05\n",
      "iteration 468, loss 7.116803317330778e-05\n",
      "iteration 469, loss 6.993474380578846e-05\n",
      "iteration 470, loss 6.916947313584387e-05\n",
      "iteration 471, loss 6.794559885747731e-05\n",
      "iteration 472, loss 6.713990296702832e-05\n",
      "iteration 473, loss 6.616638711420819e-05\n",
      "iteration 474, loss 6.530760583700612e-05\n",
      "iteration 475, loss 6.433270027628168e-05\n",
      "iteration 476, loss 6.340242543956265e-05\n",
      "iteration 477, loss 6.273737380979583e-05\n",
      "iteration 478, loss 6.176732131280005e-05\n",
      "iteration 479, loss 6.086771099944599e-05\n",
      "iteration 480, loss 6.011981895426288e-05\n",
      "iteration 481, loss 5.921569390920922e-05\n",
      "iteration 482, loss 5.825797052239068e-05\n",
      "iteration 483, loss 5.737287938245572e-05\n",
      "iteration 484, loss 5.6621185649419203e-05\n",
      "iteration 485, loss 5.622286698780954e-05\n",
      "iteration 486, loss 5.5387143220286816e-05\n",
      "iteration 487, loss 5.466460424941033e-05\n",
      "iteration 488, loss 5.3891919378656894e-05\n",
      "iteration 489, loss 5.330070780473761e-05\n",
      "iteration 490, loss 5.273938222671859e-05\n",
      "iteration 491, loss 5.1955747039755806e-05\n",
      "iteration 492, loss 5.141691508470103e-05\n",
      "iteration 493, loss 5.0740247388603166e-05\n",
      "iteration 494, loss 5.006108767702244e-05\n",
      "iteration 495, loss 4.932101001031697e-05\n",
      "iteration 496, loss 4.886462556896731e-05\n",
      "iteration 497, loss 4.845149669563398e-05\n",
      "iteration 498, loss 4.7753146645845845e-05\n",
      "iteration 499, loss 4.730936052510515e-05\n"
     ]
    }
   ],
   "source": [
    "# implementing the same example in torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "W1 = torch.randn(D_in, H, dtype=datatype, device=device, requires_grad=True)\n",
    "W2 = torch.randn(H, D_out, dtype=datatype, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    z = x.mm(W1)\n",
    "    h = z.clamp(min=0) # ReLU(z)\n",
    "    y_pred = h.mm(W2)\n",
    "    \n",
    "    loss = (y - y_pred).pow(2).sum()\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss.item()))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        \n",
    "        W2.grad.zero_()\n",
    "        W1.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 26212858.0\n",
      "iteration 1, loss 21896194.0\n",
      "iteration 2, loss 21858198.0\n",
      "iteration 3, loss 23005750.0\n",
      "iteration 4, loss 22921930.0\n",
      "iteration 5, loss 20259130.0\n",
      "iteration 6, loss 15311140.0\n",
      "iteration 7, loss 10057202.0\n",
      "iteration 8, loss 5992724.0\n",
      "iteration 9, loss 3475756.5\n",
      "iteration 10, loss 2079067.0\n",
      "iteration 11, loss 1336380.75\n",
      "iteration 12, loss 934223.4375\n",
      "iteration 13, loss 703958.4375\n",
      "iteration 14, loss 561053.625\n",
      "iteration 15, loss 464512.1875\n",
      "iteration 16, loss 394344.03125\n",
      "iteration 17, loss 340167.8125\n",
      "iteration 18, loss 296615.0\n",
      "iteration 19, loss 260641.484375\n",
      "iteration 20, loss 230350.4375\n",
      "iteration 21, loss 204525.59375\n",
      "iteration 22, loss 182298.71875\n",
      "iteration 23, loss 163086.953125\n",
      "iteration 24, loss 146331.53125\n",
      "iteration 25, loss 131647.234375\n",
      "iteration 26, loss 118729.5625\n",
      "iteration 27, loss 107315.609375\n",
      "iteration 28, loss 97202.078125\n",
      "iteration 29, loss 88205.28125\n",
      "iteration 30, loss 80184.515625\n",
      "iteration 31, loss 73013.125\n",
      "iteration 32, loss 66587.6796875\n",
      "iteration 33, loss 60820.8046875\n",
      "iteration 34, loss 55638.796875\n",
      "iteration 35, loss 50966.8046875\n",
      "iteration 36, loss 46746.10546875\n",
      "iteration 37, loss 42926.87890625\n",
      "iteration 38, loss 39462.59765625\n",
      "iteration 39, loss 36316.78125\n",
      "iteration 40, loss 33458.46875\n",
      "iteration 41, loss 30862.900390625\n",
      "iteration 42, loss 28494.677734375\n",
      "iteration 43, loss 26331.94921875\n",
      "iteration 44, loss 24354.0859375\n",
      "iteration 45, loss 22542.77734375\n",
      "iteration 46, loss 20882.759765625\n",
      "iteration 47, loss 19358.798828125\n",
      "iteration 48, loss 17958.513671875\n",
      "iteration 49, loss 16670.376953125\n",
      "iteration 50, loss 15484.2392578125\n",
      "iteration 51, loss 14391.1474609375\n",
      "iteration 52, loss 13384.1015625\n",
      "iteration 53, loss 12456.33203125\n",
      "iteration 54, loss 11598.5966796875\n",
      "iteration 55, loss 10805.984375\n",
      "iteration 56, loss 10072.5400390625\n",
      "iteration 57, loss 9393.5263671875\n",
      "iteration 58, loss 8764.2119140625\n",
      "iteration 59, loss 8180.66552734375\n",
      "iteration 60, loss 7639.17431640625\n",
      "iteration 61, loss 7136.5029296875\n",
      "iteration 62, loss 6669.33251953125\n",
      "iteration 63, loss 6234.9755859375\n",
      "iteration 64, loss 5831.19384765625\n",
      "iteration 65, loss 5455.42626953125\n",
      "iteration 66, loss 5106.0986328125\n",
      "iteration 67, loss 4780.99853515625\n",
      "iteration 68, loss 4478.50048828125\n",
      "iteration 69, loss 4196.3544921875\n",
      "iteration 70, loss 3934.3134765625\n",
      "iteration 71, loss 3689.522216796875\n",
      "iteration 72, loss 3460.923828125\n",
      "iteration 73, loss 3247.3662109375\n",
      "iteration 74, loss 3047.777099609375\n",
      "iteration 75, loss 2861.215087890625\n",
      "iteration 76, loss 2686.7734375\n",
      "iteration 77, loss 2523.53173828125\n",
      "iteration 78, loss 2370.647705078125\n",
      "iteration 79, loss 2227.573974609375\n",
      "iteration 80, loss 2093.6376953125\n",
      "iteration 81, loss 1968.09326171875\n",
      "iteration 82, loss 1850.4742431640625\n",
      "iteration 83, loss 1740.187255859375\n",
      "iteration 84, loss 1636.83251953125\n",
      "iteration 85, loss 1539.974365234375\n",
      "iteration 86, loss 1449.0928955078125\n",
      "iteration 87, loss 1363.8045654296875\n",
      "iteration 88, loss 1283.74853515625\n",
      "iteration 89, loss 1208.6390380859375\n",
      "iteration 90, loss 1138.1358642578125\n",
      "iteration 91, loss 1071.9056396484375\n",
      "iteration 92, loss 1009.6883544921875\n",
      "iteration 93, loss 951.246826171875\n",
      "iteration 94, loss 896.2841186523438\n",
      "iteration 95, loss 844.630126953125\n",
      "iteration 96, loss 796.0435180664062\n",
      "iteration 97, loss 750.3582763671875\n",
      "iteration 98, loss 707.3839111328125\n",
      "iteration 99, loss 666.944091796875\n",
      "iteration 100, loss 628.9031982421875\n",
      "iteration 101, loss 593.0988159179688\n",
      "iteration 102, loss 559.3966064453125\n",
      "iteration 103, loss 527.6699829101562\n",
      "iteration 104, loss 497.8115234375\n",
      "iteration 105, loss 469.684326171875\n",
      "iteration 106, loss 443.2176818847656\n",
      "iteration 107, loss 418.2713928222656\n",
      "iteration 108, loss 394.7621154785156\n",
      "iteration 109, loss 372.60894775390625\n",
      "iteration 110, loss 351.7395324707031\n",
      "iteration 111, loss 332.0684509277344\n",
      "iteration 112, loss 313.524658203125\n",
      "iteration 113, loss 296.0472717285156\n",
      "iteration 114, loss 279.571044921875\n",
      "iteration 115, loss 264.03009033203125\n",
      "iteration 116, loss 249.37527465820312\n",
      "iteration 117, loss 235.5597686767578\n",
      "iteration 118, loss 222.52706909179688\n",
      "iteration 119, loss 210.238037109375\n",
      "iteration 120, loss 198.638427734375\n",
      "iteration 121, loss 187.6890869140625\n",
      "iteration 122, loss 177.35830688476562\n",
      "iteration 123, loss 167.61248779296875\n",
      "iteration 124, loss 158.41134643554688\n",
      "iteration 125, loss 149.7271728515625\n",
      "iteration 126, loss 141.52943420410156\n",
      "iteration 127, loss 133.78939819335938\n",
      "iteration 128, loss 126.4825668334961\n",
      "iteration 129, loss 119.58200073242188\n",
      "iteration 130, loss 113.0667495727539\n",
      "iteration 131, loss 106.91327667236328\n",
      "iteration 132, loss 101.10197448730469\n",
      "iteration 133, loss 95.6158676147461\n",
      "iteration 134, loss 90.4302978515625\n",
      "iteration 135, loss 85.53140258789062\n",
      "iteration 136, loss 80.900634765625\n",
      "iteration 137, loss 76.5270767211914\n",
      "iteration 138, loss 72.3923568725586\n",
      "iteration 139, loss 68.48603820800781\n",
      "iteration 140, loss 64.79393768310547\n",
      "iteration 141, loss 61.30500793457031\n",
      "iteration 142, loss 58.00706481933594\n",
      "iteration 143, loss 54.89202880859375\n",
      "iteration 144, loss 51.944053649902344\n",
      "iteration 145, loss 49.1579475402832\n",
      "iteration 146, loss 46.524871826171875\n",
      "iteration 147, loss 44.0346565246582\n",
      "iteration 148, loss 41.6785888671875\n",
      "iteration 149, loss 39.4511833190918\n",
      "iteration 150, loss 37.34483337402344\n",
      "iteration 151, loss 35.351924896240234\n",
      "iteration 152, loss 33.467529296875\n",
      "iteration 153, loss 31.685470581054688\n",
      "iteration 154, loss 29.998966217041016\n",
      "iteration 155, loss 28.403871536254883\n",
      "iteration 156, loss 26.894893646240234\n",
      "iteration 157, loss 25.466907501220703\n",
      "iteration 158, loss 24.11577606201172\n",
      "iteration 159, loss 22.83829689025879\n",
      "iteration 160, loss 21.628950119018555\n",
      "iteration 161, loss 20.485004425048828\n",
      "iteration 162, loss 19.401277542114258\n",
      "iteration 163, loss 18.376327514648438\n",
      "iteration 164, loss 17.406007766723633\n",
      "iteration 165, loss 16.488025665283203\n",
      "iteration 166, loss 15.618950843811035\n",
      "iteration 167, loss 14.796134948730469\n",
      "iteration 168, loss 14.016998291015625\n",
      "iteration 169, loss 13.278993606567383\n",
      "iteration 170, loss 12.581005096435547\n",
      "iteration 171, loss 11.920133590698242\n",
      "iteration 172, loss 11.294455528259277\n",
      "iteration 173, loss 10.702268600463867\n",
      "iteration 174, loss 10.141033172607422\n",
      "iteration 175, loss 9.609747886657715\n",
      "iteration 176, loss 9.106541633605957\n",
      "iteration 177, loss 8.629873275756836\n",
      "iteration 178, loss 8.178232192993164\n",
      "iteration 179, loss 7.751008033752441\n",
      "iteration 180, loss 7.345953464508057\n",
      "iteration 181, loss 6.96258020401001\n",
      "iteration 182, loss 6.5991411209106445\n",
      "iteration 183, loss 6.2551398277282715\n",
      "iteration 184, loss 5.929035663604736\n",
      "iteration 185, loss 5.620338439941406\n",
      "iteration 186, loss 5.328367233276367\n",
      "iteration 187, loss 5.051050186157227\n",
      "iteration 188, loss 4.788346767425537\n",
      "iteration 189, loss 4.539495468139648\n",
      "iteration 190, loss 4.303796291351318\n",
      "iteration 191, loss 4.080315113067627\n",
      "iteration 192, loss 3.868708610534668\n",
      "iteration 193, loss 3.668036937713623\n",
      "iteration 194, loss 3.4780220985412598\n",
      "iteration 195, loss 3.297821521759033\n",
      "iteration 196, loss 3.1270360946655273\n",
      "iteration 197, loss 2.965200662612915\n",
      "iteration 198, loss 2.8119547367095947\n",
      "iteration 199, loss 2.6665074825286865\n",
      "iteration 200, loss 2.5288333892822266\n",
      "iteration 201, loss 2.398313283920288\n",
      "iteration 202, loss 2.2744462490081787\n",
      "iteration 203, loss 2.1570944786071777\n",
      "iteration 204, loss 2.0459983348846436\n",
      "iteration 205, loss 1.9404969215393066\n",
      "iteration 206, loss 1.8405694961547852\n",
      "iteration 207, loss 1.7457373142242432\n",
      "iteration 208, loss 1.6559003591537476\n",
      "iteration 209, loss 1.5706841945648193\n",
      "iteration 210, loss 1.4898710250854492\n",
      "iteration 211, loss 1.4133280515670776\n",
      "iteration 212, loss 1.340714454650879\n",
      "iteration 213, loss 1.2717797756195068\n",
      "iteration 214, loss 1.206519365310669\n",
      "iteration 215, loss 1.144713044166565\n",
      "iteration 216, loss 1.085976481437683\n",
      "iteration 217, loss 1.0302846431732178\n",
      "iteration 218, loss 0.9774135947227478\n",
      "iteration 219, loss 0.927344024181366\n",
      "iteration 220, loss 0.8798681497573853\n",
      "iteration 221, loss 0.8348721265792847\n",
      "iteration 222, loss 0.7921640276908875\n",
      "iteration 223, loss 0.7515934109687805\n",
      "iteration 224, loss 0.7132071256637573\n",
      "iteration 225, loss 0.6767553091049194\n",
      "iteration 226, loss 0.6421712636947632\n",
      "iteration 227, loss 0.6093000173568726\n",
      "iteration 228, loss 0.5782838463783264\n",
      "iteration 229, loss 0.5487286448478699\n",
      "iteration 230, loss 0.5207196474075317\n",
      "iteration 231, loss 0.4941692352294922\n",
      "iteration 232, loss 0.4689558148384094\n",
      "iteration 233, loss 0.44515371322631836\n",
      "iteration 234, loss 0.422452837228775\n",
      "iteration 235, loss 0.400905966758728\n",
      "iteration 236, loss 0.3805214464664459\n",
      "iteration 237, loss 0.3611607551574707\n",
      "iteration 238, loss 0.34279823303222656\n",
      "iteration 239, loss 0.3253747224807739\n",
      "iteration 240, loss 0.30877870321273804\n",
      "iteration 241, loss 0.2930987775325775\n",
      "iteration 242, loss 0.27817443013191223\n",
      "iteration 243, loss 0.26403412222862244\n",
      "iteration 244, loss 0.2506490647792816\n",
      "iteration 245, loss 0.23791766166687012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 246, loss 0.22584763169288635\n",
      "iteration 247, loss 0.21440009772777557\n",
      "iteration 248, loss 0.20351772010326385\n",
      "iteration 249, loss 0.19321097433567047\n",
      "iteration 250, loss 0.1834266483783722\n",
      "iteration 251, loss 0.17413833737373352\n",
      "iteration 252, loss 0.16531012952327728\n",
      "iteration 253, loss 0.15696129202842712\n",
      "iteration 254, loss 0.14899130165576935\n",
      "iteration 255, loss 0.14145053923130035\n",
      "iteration 256, loss 0.13432398438453674\n",
      "iteration 257, loss 0.12749695777893066\n",
      "iteration 258, loss 0.12107862532138824\n",
      "iteration 259, loss 0.1149253249168396\n",
      "iteration 260, loss 0.10912556201219559\n",
      "iteration 261, loss 0.10361149162054062\n",
      "iteration 262, loss 0.09838104993104935\n",
      "iteration 263, loss 0.09342491626739502\n",
      "iteration 264, loss 0.08871405571699142\n",
      "iteration 265, loss 0.0842457115650177\n",
      "iteration 266, loss 0.07999363541603088\n",
      "iteration 267, loss 0.07596572488546371\n",
      "iteration 268, loss 0.07211719453334808\n",
      "iteration 269, loss 0.06848961114883423\n",
      "iteration 270, loss 0.0650365799665451\n",
      "iteration 271, loss 0.06176643446087837\n",
      "iteration 272, loss 0.05866916477680206\n",
      "iteration 273, loss 0.055729396641254425\n",
      "iteration 274, loss 0.05292503535747528\n",
      "iteration 275, loss 0.05025813728570938\n",
      "iteration 276, loss 0.04773058742284775\n",
      "iteration 277, loss 0.045320358127355576\n",
      "iteration 278, loss 0.04305417090654373\n",
      "iteration 279, loss 0.040903396904468536\n",
      "iteration 280, loss 0.03884866088628769\n",
      "iteration 281, loss 0.03689942508935928\n",
      "iteration 282, loss 0.03505221754312515\n",
      "iteration 283, loss 0.03328923135995865\n",
      "iteration 284, loss 0.0316222608089447\n",
      "iteration 285, loss 0.030037488788366318\n",
      "iteration 286, loss 0.02853764221072197\n",
      "iteration 287, loss 0.027110114693641663\n",
      "iteration 288, loss 0.025753911584615707\n",
      "iteration 289, loss 0.024477871134877205\n",
      "iteration 290, loss 0.02325383573770523\n",
      "iteration 291, loss 0.022095004096627235\n",
      "iteration 292, loss 0.02098441869020462\n",
      "iteration 293, loss 0.01994425803422928\n",
      "iteration 294, loss 0.018946461379528046\n",
      "iteration 295, loss 0.01800677925348282\n",
      "iteration 296, loss 0.01711299456655979\n",
      "iteration 297, loss 0.016258656978607178\n",
      "iteration 298, loss 0.015455619432032108\n",
      "iteration 299, loss 0.014687578193843365\n",
      "iteration 300, loss 0.013965018093585968\n",
      "iteration 301, loss 0.013273164629936218\n",
      "iteration 302, loss 0.012623811140656471\n",
      "iteration 303, loss 0.01200567651540041\n",
      "iteration 304, loss 0.011415259912610054\n",
      "iteration 305, loss 0.01085122860968113\n",
      "iteration 306, loss 0.010328137315809727\n",
      "iteration 307, loss 0.009820857085287571\n",
      "iteration 308, loss 0.0093446159735322\n",
      "iteration 309, loss 0.008886804804205894\n",
      "iteration 310, loss 0.008456823416054249\n",
      "iteration 311, loss 0.008046146482229233\n",
      "iteration 312, loss 0.007662738673388958\n",
      "iteration 313, loss 0.0072831884026527405\n",
      "iteration 314, loss 0.006931585259735584\n",
      "iteration 315, loss 0.006603776477277279\n",
      "iteration 316, loss 0.006288795731961727\n",
      "iteration 317, loss 0.005986940581351519\n",
      "iteration 318, loss 0.00569928390905261\n",
      "iteration 319, loss 0.005434991791844368\n",
      "iteration 320, loss 0.005175071302801371\n",
      "iteration 321, loss 0.004927304107695818\n",
      "iteration 322, loss 0.004699185956269503\n",
      "iteration 323, loss 0.00447933841496706\n",
      "iteration 324, loss 0.004269988741725683\n",
      "iteration 325, loss 0.004074861295521259\n",
      "iteration 326, loss 0.0038864233065396547\n",
      "iteration 327, loss 0.003708435222506523\n",
      "iteration 328, loss 0.003536779899150133\n",
      "iteration 329, loss 0.0033767332788556814\n",
      "iteration 330, loss 0.0032186703756451607\n",
      "iteration 331, loss 0.003076502587646246\n",
      "iteration 332, loss 0.0029391678981482983\n",
      "iteration 333, loss 0.0028056148439645767\n",
      "iteration 334, loss 0.0026832574512809515\n",
      "iteration 335, loss 0.0025626104325056076\n",
      "iteration 336, loss 0.002451414242386818\n",
      "iteration 337, loss 0.002344019478186965\n",
      "iteration 338, loss 0.002242312068119645\n",
      "iteration 339, loss 0.002145126461982727\n",
      "iteration 340, loss 0.002051307586953044\n",
      "iteration 341, loss 0.0019623637199401855\n",
      "iteration 342, loss 0.0018796651856973767\n",
      "iteration 343, loss 0.00180120300501585\n",
      "iteration 344, loss 0.0017258865991607308\n",
      "iteration 345, loss 0.0016550295986235142\n",
      "iteration 346, loss 0.0015871694777160883\n",
      "iteration 347, loss 0.0015203633811324835\n",
      "iteration 348, loss 0.0014590017963200808\n",
      "iteration 349, loss 0.0014000099617987871\n",
      "iteration 350, loss 0.00134277215693146\n",
      "iteration 351, loss 0.001290271757170558\n",
      "iteration 352, loss 0.0012386294547468424\n",
      "iteration 353, loss 0.0011888900771737099\n",
      "iteration 354, loss 0.0011418949579820037\n",
      "iteration 355, loss 0.0010992842726409435\n",
      "iteration 356, loss 0.0010569102596491575\n",
      "iteration 357, loss 0.0010163086699321866\n",
      "iteration 358, loss 0.0009781651897355914\n",
      "iteration 359, loss 0.000939438643399626\n",
      "iteration 360, loss 0.000904976564925164\n",
      "iteration 361, loss 0.0008716987795196474\n",
      "iteration 362, loss 0.0008401253144256771\n",
      "iteration 363, loss 0.0008093327051028609\n",
      "iteration 364, loss 0.0007794498233124614\n",
      "iteration 365, loss 0.0007519683567807078\n",
      "iteration 366, loss 0.0007249796763062477\n",
      "iteration 367, loss 0.0006995522417128086\n",
      "iteration 368, loss 0.0006754331407137215\n",
      "iteration 369, loss 0.0006520000169984996\n",
      "iteration 370, loss 0.000629092042800039\n",
      "iteration 371, loss 0.0006077763973735273\n",
      "iteration 372, loss 0.0005874395137652755\n",
      "iteration 373, loss 0.0005672218394465744\n",
      "iteration 374, loss 0.0005492469645105302\n",
      "iteration 375, loss 0.0005303291836753488\n",
      "iteration 376, loss 0.0005133236409164965\n",
      "iteration 377, loss 0.0004963675746694207\n",
      "iteration 378, loss 0.0004807106452062726\n",
      "iteration 379, loss 0.0004656985984183848\n",
      "iteration 380, loss 0.0004511837614700198\n",
      "iteration 381, loss 0.00043713836930692196\n",
      "iteration 382, loss 0.0004227626486681402\n",
      "iteration 383, loss 0.0004107225686311722\n",
      "iteration 384, loss 0.00039825279964134097\n",
      "iteration 385, loss 0.00038555735955014825\n",
      "iteration 386, loss 0.00037367368349805474\n",
      "iteration 387, loss 0.0003624561650212854\n",
      "iteration 388, loss 0.00035214907256886363\n",
      "iteration 389, loss 0.0003409251803532243\n",
      "iteration 390, loss 0.00033140790765173733\n",
      "iteration 391, loss 0.00032128446036949754\n",
      "iteration 392, loss 0.00031197804491966963\n",
      "iteration 393, loss 0.000302547006867826\n",
      "iteration 394, loss 0.00029427759000100195\n",
      "iteration 395, loss 0.000285694666672498\n",
      "iteration 396, loss 0.0002779781352728605\n",
      "iteration 397, loss 0.00027061018045060337\n",
      "iteration 398, loss 0.0002627018839120865\n",
      "iteration 399, loss 0.00025569333229213953\n",
      "iteration 400, loss 0.0002490858605597168\n",
      "iteration 401, loss 0.00024246587418019772\n",
      "iteration 402, loss 0.0002356481272727251\n",
      "iteration 403, loss 0.00022951490245759487\n",
      "iteration 404, loss 0.000222896778723225\n",
      "iteration 405, loss 0.00021759314404334873\n",
      "iteration 406, loss 0.00021221926726866513\n",
      "iteration 407, loss 0.00020625654724426568\n",
      "iteration 408, loss 0.000201760558411479\n",
      "iteration 409, loss 0.0001964591210708022\n",
      "iteration 410, loss 0.00019165182311553508\n",
      "iteration 411, loss 0.0001873158907983452\n",
      "iteration 412, loss 0.00018269495922140777\n",
      "iteration 413, loss 0.0001780728343874216\n",
      "iteration 414, loss 0.00017382721125613898\n",
      "iteration 415, loss 0.0001695369282970205\n",
      "iteration 416, loss 0.00016551509907003492\n",
      "iteration 417, loss 0.0001618546521058306\n",
      "iteration 418, loss 0.00015807599993422627\n",
      "iteration 419, loss 0.00015429499035235494\n",
      "iteration 420, loss 0.00015080075536388904\n",
      "iteration 421, loss 0.00014743964129593223\n",
      "iteration 422, loss 0.00014415547775570303\n",
      "iteration 423, loss 0.00014059210661798716\n",
      "iteration 424, loss 0.00013750533980783075\n",
      "iteration 425, loss 0.0001344976481050253\n",
      "iteration 426, loss 0.000131713502923958\n",
      "iteration 427, loss 0.00012886369950138032\n",
      "iteration 428, loss 0.00012606519157998264\n",
      "iteration 429, loss 0.00012301541573833674\n",
      "iteration 430, loss 0.00012047088239341974\n",
      "iteration 431, loss 0.00011791908764280379\n",
      "iteration 432, loss 0.00011572251241886988\n",
      "iteration 433, loss 0.00011294485739199445\n",
      "iteration 434, loss 0.00011089074541814625\n",
      "iteration 435, loss 0.00010859614121727645\n",
      "iteration 436, loss 0.0001065997639670968\n",
      "iteration 437, loss 0.00010450944682816043\n",
      "iteration 438, loss 0.000102165577118285\n",
      "iteration 439, loss 0.00010021110938396305\n",
      "iteration 440, loss 9.839014819590375e-05\n",
      "iteration 441, loss 9.652291191741824e-05\n",
      "iteration 442, loss 9.455240069655702e-05\n",
      "iteration 443, loss 9.277078788727522e-05\n",
      "iteration 444, loss 9.115310967899859e-05\n",
      "iteration 445, loss 8.949334733188152e-05\n",
      "iteration 446, loss 8.752530993660912e-05\n",
      "iteration 447, loss 8.56828992255032e-05\n",
      "iteration 448, loss 8.411408634856343e-05\n",
      "iteration 449, loss 8.27656258479692e-05\n",
      "iteration 450, loss 8.114388037938625e-05\n",
      "iteration 451, loss 7.966160774230957e-05\n",
      "iteration 452, loss 7.83175346441567e-05\n",
      "iteration 453, loss 7.695533713558689e-05\n",
      "iteration 454, loss 7.57498200982809e-05\n",
      "iteration 455, loss 7.430170080624521e-05\n",
      "iteration 456, loss 7.314389222301543e-05\n",
      "iteration 457, loss 7.18355513527058e-05\n",
      "iteration 458, loss 7.020663906587288e-05\n",
      "iteration 459, loss 6.931893585715443e-05\n",
      "iteration 460, loss 6.777595990570262e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 461, loss 6.669329013675451e-05\n",
      "iteration 462, loss 6.569574179593474e-05\n",
      "iteration 463, loss 6.437947740778327e-05\n",
      "iteration 464, loss 6.343035056488588e-05\n",
      "iteration 465, loss 6.227393896551803e-05\n",
      "iteration 466, loss 6.124771607574075e-05\n",
      "iteration 467, loss 6.0271864640526474e-05\n",
      "iteration 468, loss 5.928164318902418e-05\n",
      "iteration 469, loss 5.827002314617857e-05\n",
      "iteration 470, loss 5.722991409129463e-05\n",
      "iteration 471, loss 5.644732300424948e-05\n",
      "iteration 472, loss 5.552920265472494e-05\n",
      "iteration 473, loss 5.4650012316415086e-05\n",
      "iteration 474, loss 5.386986958910711e-05\n",
      "iteration 475, loss 5.295498704072088e-05\n",
      "iteration 476, loss 5.2064689953112975e-05\n",
      "iteration 477, loss 5.124606832396239e-05\n",
      "iteration 478, loss 5.049395622336306e-05\n",
      "iteration 479, loss 4.980440644430928e-05\n",
      "iteration 480, loss 4.898387487628497e-05\n",
      "iteration 481, loss 4.8342320951633155e-05\n",
      "iteration 482, loss 4.7586450818926096e-05\n",
      "iteration 483, loss 4.6991899580461904e-05\n",
      "iteration 484, loss 4.621305561158806e-05\n",
      "iteration 485, loss 4.548431752482429e-05\n",
      "iteration 486, loss 4.4922308006789535e-05\n",
      "iteration 487, loss 4.4264234020374715e-05\n",
      "iteration 488, loss 4.3665539124049246e-05\n",
      "iteration 489, loss 4.303138848626986e-05\n",
      "iteration 490, loss 4.233487925375812e-05\n",
      "iteration 491, loss 4.176686343271285e-05\n",
      "iteration 492, loss 4.118477590964176e-05\n",
      "iteration 493, loss 4.0488303056918085e-05\n",
      "iteration 494, loss 4.001634442829527e-05\n",
      "iteration 495, loss 3.954864223487675e-05\n",
      "iteration 496, loss 3.904602272086777e-05\n",
      "iteration 497, loss 3.851723886327818e-05\n",
      "iteration 498, loss 3.798571924562566e-05\n",
      "iteration 499, loss 3.734098936547525e-05\n"
     ]
    }
   ],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        relu = input.clamp(min=0)\n",
    "        ctx.save_for_backward(relu)\n",
    "        return relu\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        relu, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[relu <= 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "W1 = torch.randn(D_in, H, dtype=datatype, device=device, requires_grad=True)\n",
    "W2 = torch.randn(H, D_out, dtype=datatype, device=device, requires_grad=True)\n",
    "b1 = torch.randn(H, dtype=datatype, device=device, requires_grad=True)\n",
    "b2 = torch.randn(D_out, dtype=datatype, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for iteration in np.arange(500):\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    z = x.mm(W1) + b1\n",
    "    h = relu(z) # ReLU(z)\n",
    "    y_pred = h.mm(W2) + b2\n",
    "    \n",
    "    loss = (y - y_pred).pow(2).sum()\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss.item()))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        \n",
    "        W2.grad.zero_()\n",
    "        W1.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "        b1.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TensorFlow: Static Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not implementing this part of the tutorial because it just provides a contrast between Pytorch Autograd and Tensorflow. \n",
    "\n",
    "Essentially the key point to remember is that in tensorflow we create a graph first folowed by opening a tensorflow session to run the same. \n",
    "\n",
    "Ref: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#tensorflow-static-graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pytorch: nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 722.8011474609375\n",
      "iteration 1, loss 667.1231689453125\n",
      "iteration 2, loss 619.0684814453125\n",
      "iteration 3, loss 576.9715576171875\n",
      "iteration 4, loss 539.89404296875\n",
      "iteration 5, loss 506.85028076171875\n",
      "iteration 6, loss 476.9220886230469\n",
      "iteration 7, loss 449.794189453125\n",
      "iteration 8, loss 424.9374084472656\n",
      "iteration 9, loss 401.92108154296875\n",
      "iteration 10, loss 380.38409423828125\n",
      "iteration 11, loss 360.1080322265625\n",
      "iteration 12, loss 341.0399475097656\n",
      "iteration 13, loss 322.756103515625\n",
      "iteration 14, loss 305.42596435546875\n",
      "iteration 15, loss 289.02850341796875\n",
      "iteration 16, loss 273.399169921875\n",
      "iteration 17, loss 258.46533203125\n",
      "iteration 18, loss 244.1697540283203\n",
      "iteration 19, loss 230.5232696533203\n",
      "iteration 20, loss 217.54598999023438\n",
      "iteration 21, loss 205.13197326660156\n",
      "iteration 22, loss 193.27008056640625\n",
      "iteration 23, loss 181.9656982421875\n",
      "iteration 24, loss 171.16929626464844\n",
      "iteration 25, loss 160.922119140625\n",
      "iteration 26, loss 151.18582153320312\n",
      "iteration 27, loss 141.95420837402344\n",
      "iteration 28, loss 133.208984375\n",
      "iteration 29, loss 124.94905090332031\n",
      "iteration 30, loss 117.13352966308594\n",
      "iteration 31, loss 109.76118469238281\n",
      "iteration 32, loss 102.811767578125\n",
      "iteration 33, loss 96.26437377929688\n",
      "iteration 34, loss 90.10588073730469\n",
      "iteration 35, loss 84.29393005371094\n",
      "iteration 36, loss 78.8482666015625\n",
      "iteration 37, loss 73.74490356445312\n",
      "iteration 38, loss 68.97612762451172\n",
      "iteration 39, loss 64.51286315917969\n",
      "iteration 40, loss 60.34284210205078\n",
      "iteration 41, loss 56.440948486328125\n",
      "iteration 42, loss 52.798805236816406\n",
      "iteration 43, loss 49.4029541015625\n",
      "iteration 44, loss 46.23564910888672\n",
      "iteration 45, loss 43.280967712402344\n",
      "iteration 46, loss 40.52499771118164\n",
      "iteration 47, loss 37.95689392089844\n",
      "iteration 48, loss 35.56439971923828\n",
      "iteration 49, loss 33.330413818359375\n",
      "iteration 50, loss 31.248065948486328\n",
      "iteration 51, loss 29.306440353393555\n",
      "iteration 52, loss 27.495752334594727\n",
      "iteration 53, loss 25.806537628173828\n",
      "iteration 54, loss 24.230880737304688\n",
      "iteration 55, loss 22.762588500976562\n",
      "iteration 56, loss 21.391700744628906\n",
      "iteration 57, loss 20.112913131713867\n",
      "iteration 58, loss 18.92015266418457\n",
      "iteration 59, loss 17.805957794189453\n",
      "iteration 60, loss 16.765897750854492\n",
      "iteration 61, loss 15.795044898986816\n",
      "iteration 62, loss 14.88717269897461\n",
      "iteration 63, loss 14.037931442260742\n",
      "iteration 64, loss 13.242887496948242\n",
      "iteration 65, loss 12.500114440917969\n",
      "iteration 66, loss 11.808418273925781\n",
      "iteration 67, loss 11.159542083740234\n",
      "iteration 68, loss 10.54813003540039\n",
      "iteration 69, loss 9.97477912902832\n",
      "iteration 70, loss 9.436800003051758\n",
      "iteration 71, loss 8.932580947875977\n",
      "iteration 72, loss 8.4587984085083\n",
      "iteration 73, loss 8.01326847076416\n",
      "iteration 74, loss 7.59361457824707\n",
      "iteration 75, loss 7.199640274047852\n",
      "iteration 76, loss 6.8285322189331055\n",
      "iteration 77, loss 6.4788336753845215\n",
      "iteration 78, loss 6.148992538452148\n",
      "iteration 79, loss 5.838308334350586\n",
      "iteration 80, loss 5.545331001281738\n",
      "iteration 81, loss 5.2687273025512695\n",
      "iteration 82, loss 5.007639408111572\n",
      "iteration 83, loss 4.761054039001465\n",
      "iteration 84, loss 4.528296947479248\n",
      "iteration 85, loss 4.307975769042969\n",
      "iteration 86, loss 4.099577903747559\n",
      "iteration 87, loss 3.902449369430542\n",
      "iteration 88, loss 3.716125249862671\n",
      "iteration 89, loss 3.5396299362182617\n",
      "iteration 90, loss 3.3724429607391357\n",
      "iteration 91, loss 3.214146375656128\n",
      "iteration 92, loss 3.0641236305236816\n",
      "iteration 93, loss 2.9221701622009277\n",
      "iteration 94, loss 2.787431001663208\n",
      "iteration 95, loss 2.659611225128174\n",
      "iteration 96, loss 2.5382418632507324\n",
      "iteration 97, loss 2.423034191131592\n",
      "iteration 98, loss 2.3136942386627197\n",
      "iteration 99, loss 2.2098302841186523\n",
      "iteration 100, loss 2.1111392974853516\n",
      "iteration 101, loss 2.0174152851104736\n",
      "iteration 102, loss 1.928264856338501\n",
      "iteration 103, loss 1.8435072898864746\n",
      "iteration 104, loss 1.7629032135009766\n",
      "iteration 105, loss 1.6861937046051025\n",
      "iteration 106, loss 1.6131529808044434\n",
      "iteration 107, loss 1.54362154006958\n",
      "iteration 108, loss 1.477431297302246\n",
      "iteration 109, loss 1.414327621459961\n",
      "iteration 110, loss 1.354195475578308\n",
      "iteration 111, loss 1.2969143390655518\n",
      "iteration 112, loss 1.2423592805862427\n",
      "iteration 113, loss 1.1902592182159424\n",
      "iteration 114, loss 1.140641689300537\n",
      "iteration 115, loss 1.0933153629302979\n",
      "iteration 116, loss 1.0481435060501099\n",
      "iteration 117, loss 1.005061388015747\n",
      "iteration 118, loss 0.9639549255371094\n",
      "iteration 119, loss 0.9247121810913086\n",
      "iteration 120, loss 0.8872411251068115\n",
      "iteration 121, loss 0.8514301180839539\n",
      "iteration 122, loss 0.8172399997711182\n",
      "iteration 123, loss 0.7845621705055237\n",
      "iteration 124, loss 0.7534053921699524\n",
      "iteration 125, loss 0.7235995531082153\n",
      "iteration 126, loss 0.6950932741165161\n",
      "iteration 127, loss 0.6678565740585327\n",
      "iteration 128, loss 0.6418018341064453\n",
      "iteration 129, loss 0.6168705224990845\n",
      "iteration 130, loss 0.5930259227752686\n",
      "iteration 131, loss 0.5702186226844788\n",
      "iteration 132, loss 0.5483940839767456\n",
      "iteration 133, loss 0.5275092124938965\n",
      "iteration 134, loss 0.5074930191040039\n",
      "iteration 135, loss 0.48829811811447144\n",
      "iteration 136, loss 0.4699094295501709\n",
      "iteration 137, loss 0.45228707790374756\n",
      "iteration 138, loss 0.4353986978530884\n",
      "iteration 139, loss 0.4192051291465759\n",
      "iteration 140, loss 0.4036812484264374\n",
      "iteration 141, loss 0.38877975940704346\n",
      "iteration 142, loss 0.3744988441467285\n",
      "iteration 143, loss 0.3607901334762573\n",
      "iteration 144, loss 0.3476470410823822\n",
      "iteration 145, loss 0.33502382040023804\n",
      "iteration 146, loss 0.3229096829891205\n",
      "iteration 147, loss 0.31128764152526855\n",
      "iteration 148, loss 0.3001147508621216\n",
      "iteration 149, loss 0.2893891930580139\n",
      "iteration 150, loss 0.2790811061859131\n",
      "iteration 151, loss 0.2691805958747864\n",
      "iteration 152, loss 0.2596573233604431\n",
      "iteration 153, loss 0.2505069673061371\n",
      "iteration 154, loss 0.24171164631843567\n",
      "iteration 155, loss 0.2332495003938675\n",
      "iteration 156, loss 0.22511467337608337\n",
      "iteration 157, loss 0.21729451417922974\n",
      "iteration 158, loss 0.2097741961479187\n",
      "iteration 159, loss 0.2025439739227295\n",
      "iteration 160, loss 0.19557765126228333\n",
      "iteration 161, loss 0.18887117505073547\n",
      "iteration 162, loss 0.18242120742797852\n",
      "iteration 163, loss 0.17620086669921875\n",
      "iteration 164, loss 0.17021191120147705\n",
      "iteration 165, loss 0.16444844007492065\n",
      "iteration 166, loss 0.1588960587978363\n",
      "iteration 167, loss 0.1535458266735077\n",
      "iteration 168, loss 0.1483961045742035\n",
      "iteration 169, loss 0.14343291521072388\n",
      "iteration 170, loss 0.1386459320783615\n",
      "iteration 171, loss 0.1340375691652298\n",
      "iteration 172, loss 0.12959522008895874\n",
      "iteration 173, loss 0.12531234323978424\n",
      "iteration 174, loss 0.12118390202522278\n",
      "iteration 175, loss 0.11720416694879532\n",
      "iteration 176, loss 0.11336571723222733\n",
      "iteration 177, loss 0.10966268181800842\n",
      "iteration 178, loss 0.10609076172113419\n",
      "iteration 179, loss 0.10264277458190918\n",
      "iteration 180, loss 0.0993177592754364\n",
      "iteration 181, loss 0.0961073637008667\n",
      "iteration 182, loss 0.09301038086414337\n",
      "iteration 183, loss 0.09002114087343216\n",
      "iteration 184, loss 0.08713719248771667\n",
      "iteration 185, loss 0.0843493640422821\n",
      "iteration 186, loss 0.08166013658046722\n",
      "iteration 187, loss 0.07906416803598404\n",
      "iteration 188, loss 0.07655481994152069\n",
      "iteration 189, loss 0.07413332164287567\n",
      "iteration 190, loss 0.0717928409576416\n",
      "iteration 191, loss 0.06953185796737671\n",
      "iteration 192, loss 0.06734965741634369\n",
      "iteration 193, loss 0.06523999571800232\n",
      "iteration 194, loss 0.06320168077945709\n",
      "iteration 195, loss 0.06123172119259834\n",
      "iteration 196, loss 0.05932696908712387\n",
      "iteration 197, loss 0.05748629570007324\n",
      "iteration 198, loss 0.055706847459077835\n",
      "iteration 199, loss 0.05398595333099365\n",
      "iteration 200, loss 0.0523223839700222\n",
      "iteration 201, loss 0.05071433633565903\n",
      "iteration 202, loss 0.04915797337889671\n",
      "iteration 203, loss 0.047652535140514374\n",
      "iteration 204, loss 0.04619666188955307\n",
      "iteration 205, loss 0.04478917270898819\n",
      "iteration 206, loss 0.043426234275102615\n",
      "iteration 207, loss 0.04210805892944336\n",
      "iteration 208, loss 0.040831856429576874\n",
      "iteration 209, loss 0.039597101509571075\n",
      "iteration 210, loss 0.038402456790208817\n",
      "iteration 211, loss 0.03724564611911774\n",
      "iteration 212, loss 0.0361262708902359\n",
      "iteration 213, loss 0.03504571691155434\n",
      "iteration 214, loss 0.0339987613260746\n",
      "iteration 215, loss 0.032985832542181015\n",
      "iteration 216, loss 0.03200392425060272\n",
      "iteration 217, loss 0.031053073704242706\n",
      "iteration 218, loss 0.03013189323246479\n",
      "iteration 219, loss 0.02924012951552868\n",
      "iteration 220, loss 0.028376463800668716\n",
      "iteration 221, loss 0.0275395717471838\n",
      "iteration 222, loss 0.026728656142950058\n",
      "iteration 223, loss 0.025943033397197723\n",
      "iteration 224, loss 0.025182459503412247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 225, loss 0.024444621056318283\n",
      "iteration 226, loss 0.02372988685965538\n",
      "iteration 227, loss 0.023036973550915718\n",
      "iteration 228, loss 0.022365542128682137\n",
      "iteration 229, loss 0.02171441726386547\n",
      "iteration 230, loss 0.021082963794469833\n",
      "iteration 231, loss 0.020471083000302315\n",
      "iteration 232, loss 0.019878428429365158\n",
      "iteration 233, loss 0.01930391415953636\n",
      "iteration 234, loss 0.01874655857682228\n",
      "iteration 235, loss 0.018206072971224785\n",
      "iteration 236, loss 0.01768212951719761\n",
      "iteration 237, loss 0.017173517495393753\n",
      "iteration 238, loss 0.016680622473359108\n",
      "iteration 239, loss 0.016202203929424286\n",
      "iteration 240, loss 0.015738341957330704\n",
      "iteration 241, loss 0.015288172289729118\n",
      "iteration 242, loss 0.014851509593427181\n",
      "iteration 243, loss 0.014427940361201763\n",
      "iteration 244, loss 0.014016978442668915\n",
      "iteration 245, loss 0.013618243858218193\n",
      "iteration 246, loss 0.01323151495307684\n",
      "iteration 247, loss 0.012856277637183666\n",
      "iteration 248, loss 0.0124918632209301\n",
      "iteration 249, loss 0.012138307094573975\n",
      "iteration 250, loss 0.011795330792665482\n",
      "iteration 251, loss 0.01146239135414362\n",
      "iteration 252, loss 0.011139238253235817\n",
      "iteration 253, loss 0.01082564890384674\n",
      "iteration 254, loss 0.010521065443754196\n",
      "iteration 255, loss 0.010225378908216953\n",
      "iteration 256, loss 0.009938305243849754\n",
      "iteration 257, loss 0.009659613482654095\n",
      "iteration 258, loss 0.009389105252921581\n",
      "iteration 259, loss 0.009126494638621807\n",
      "iteration 260, loss 0.008871392346918583\n",
      "iteration 261, loss 0.008623634465038776\n",
      "iteration 262, loss 0.008383121341466904\n",
      "iteration 263, loss 0.008149526081979275\n",
      "iteration 264, loss 0.00792268943041563\n",
      "iteration 265, loss 0.007702475413680077\n",
      "iteration 266, loss 0.007488572970032692\n",
      "iteration 267, loss 0.00728072552010417\n",
      "iteration 268, loss 0.007078900001943111\n",
      "iteration 269, loss 0.006882878951728344\n",
      "iteration 270, loss 0.006692542228847742\n",
      "iteration 271, loss 0.006507575511932373\n",
      "iteration 272, loss 0.006327865645289421\n",
      "iteration 273, loss 0.006153319031000137\n",
      "iteration 274, loss 0.005983707495033741\n",
      "iteration 275, loss 0.005818871781229973\n",
      "iteration 276, loss 0.005658752750605345\n",
      "iteration 277, loss 0.0055031972005963326\n",
      "iteration 278, loss 0.0053520589135587215\n",
      "iteration 279, loss 0.005205201450735331\n",
      "iteration 280, loss 0.005062539130449295\n",
      "iteration 281, loss 0.004923861939460039\n",
      "iteration 282, loss 0.0047891587018966675\n",
      "iteration 283, loss 0.004658236168324947\n",
      "iteration 284, loss 0.004531023558229208\n",
      "iteration 285, loss 0.0044072954915463924\n",
      "iteration 286, loss 0.00428710225969553\n",
      "iteration 287, loss 0.004170293919742107\n",
      "iteration 288, loss 0.004056768491864204\n",
      "iteration 289, loss 0.003946397453546524\n",
      "iteration 290, loss 0.0038390234112739563\n",
      "iteration 291, loss 0.0037346661556512117\n",
      "iteration 292, loss 0.0036332965828478336\n",
      "iteration 293, loss 0.00353472912684083\n",
      "iteration 294, loss 0.0034388662315905094\n",
      "iteration 295, loss 0.003345681820064783\n",
      "iteration 296, loss 0.0032550878822803497\n",
      "iteration 297, loss 0.0031670730095356703\n",
      "iteration 298, loss 0.0030814800411462784\n",
      "iteration 299, loss 0.002998232375830412\n",
      "iteration 300, loss 0.0029172911308705807\n",
      "iteration 301, loss 0.002838578773662448\n",
      "iteration 302, loss 0.0027620773762464523\n",
      "iteration 303, loss 0.0026876810006797314\n",
      "iteration 304, loss 0.0026153279468417168\n",
      "iteration 305, loss 0.0025449718814343214\n",
      "iteration 306, loss 0.002476582769304514\n",
      "iteration 307, loss 0.0024100374430418015\n",
      "iteration 308, loss 0.0023453310132026672\n",
      "iteration 309, loss 0.002282447647303343\n",
      "iteration 310, loss 0.0022212928161025047\n",
      "iteration 311, loss 0.0021617833990603685\n",
      "iteration 312, loss 0.0021039012353867292\n",
      "iteration 313, loss 0.002047620015218854\n",
      "iteration 314, loss 0.0019928785040974617\n",
      "iteration 315, loss 0.0019396310672163963\n",
      "iteration 316, loss 0.0018878597766160965\n",
      "iteration 317, loss 0.0018374696373939514\n",
      "iteration 318, loss 0.0017884577391669154\n",
      "iteration 319, loss 0.0017407668055966496\n",
      "iteration 320, loss 0.0016943993978202343\n",
      "iteration 321, loss 0.0016492782160639763\n",
      "iteration 322, loss 0.0016053994186222553\n",
      "iteration 323, loss 0.001562729710713029\n",
      "iteration 324, loss 0.0015211640857160091\n",
      "iteration 325, loss 0.0014807838015258312\n",
      "iteration 326, loss 0.0014414838515222073\n",
      "iteration 327, loss 0.001403246307745576\n",
      "iteration 328, loss 0.0013660311233252287\n",
      "iteration 329, loss 0.0013298385310918093\n",
      "iteration 330, loss 0.0012946410570293665\n",
      "iteration 331, loss 0.0012603823561221361\n",
      "iteration 332, loss 0.0012270361185073853\n",
      "iteration 333, loss 0.0011945958249270916\n",
      "iteration 334, loss 0.0011630267836153507\n",
      "iteration 335, loss 0.0011323174694553018\n",
      "iteration 336, loss 0.0011024157283827662\n",
      "iteration 337, loss 0.001073331106454134\n",
      "iteration 338, loss 0.0010450173867866397\n",
      "iteration 339, loss 0.0010174757335335016\n",
      "iteration 340, loss 0.0009906680788844824\n",
      "iteration 341, loss 0.0009645891841500998\n",
      "iteration 342, loss 0.0009392056963406503\n",
      "iteration 343, loss 0.0009144959039986134\n",
      "iteration 344, loss 0.0008904582355171442\n",
      "iteration 345, loss 0.0008670485112816095\n",
      "iteration 346, loss 0.0008442895486950874\n",
      "iteration 347, loss 0.0008221318130381405\n",
      "iteration 348, loss 0.0008005629642866552\n",
      "iteration 349, loss 0.0007795643759891391\n",
      "iteration 350, loss 0.0007591377943754196\n",
      "iteration 351, loss 0.0007392617408186197\n",
      "iteration 352, loss 0.0007199009414762259\n",
      "iteration 353, loss 0.0007010557455942035\n",
      "iteration 354, loss 0.0006827111355960369\n",
      "iteration 355, loss 0.0006648676935583353\n",
      "iteration 356, loss 0.0006474945694208145\n",
      "iteration 357, loss 0.0006305841961875558\n",
      "iteration 358, loss 0.0006141120102256536\n",
      "iteration 359, loss 0.0005980782443657517\n",
      "iteration 360, loss 0.0005824794643558562\n",
      "iteration 361, loss 0.0005672939587384462\n",
      "iteration 362, loss 0.000552507524844259\n",
      "iteration 363, loss 0.0005381099763326347\n",
      "iteration 364, loss 0.0005241053877398372\n",
      "iteration 365, loss 0.0005104579031467438\n",
      "iteration 366, loss 0.0004971812013536692\n",
      "iteration 367, loss 0.00048424905980937183\n",
      "iteration 368, loss 0.00047166855074465275\n",
      "iteration 369, loss 0.00045941618736833334\n",
      "iteration 370, loss 0.0004474784363992512\n",
      "iteration 371, loss 0.00043586152605712414\n",
      "iteration 372, loss 0.0004245548043400049\n",
      "iteration 373, loss 0.00041354019776917994\n",
      "iteration 374, loss 0.00040281162364408374\n",
      "iteration 375, loss 0.00039238156750798225\n",
      "iteration 376, loss 0.000382209662348032\n",
      "iteration 377, loss 0.0003723149420693517\n",
      "iteration 378, loss 0.0003626768593676388\n",
      "iteration 379, loss 0.0003532920964062214\n",
      "iteration 380, loss 0.00034415684058330953\n",
      "iteration 381, loss 0.00033525744220241904\n",
      "iteration 382, loss 0.0003265960840508342\n",
      "iteration 383, loss 0.00031815958209335804\n",
      "iteration 384, loss 0.00030994147527962923\n",
      "iteration 385, loss 0.00030194310238584876\n",
      "iteration 386, loss 0.0002941527927760035\n",
      "iteration 387, loss 0.0002865635324269533\n",
      "iteration 388, loss 0.00027917855186387897\n",
      "iteration 389, loss 0.00027199037140235305\n",
      "iteration 390, loss 0.0002649808011483401\n",
      "iteration 391, loss 0.00025815347908064723\n",
      "iteration 392, loss 0.0002515149535611272\n",
      "iteration 393, loss 0.00024503885651938617\n",
      "iteration 394, loss 0.00023873455938883126\n",
      "iteration 395, loss 0.0002325919340364635\n",
      "iteration 396, loss 0.00022661556431557983\n",
      "iteration 397, loss 0.00022079376503825188\n",
      "iteration 398, loss 0.00021512393141165376\n",
      "iteration 399, loss 0.00020960281835868955\n",
      "iteration 400, loss 0.00020422387751750648\n",
      "iteration 401, loss 0.00019898427126463503\n",
      "iteration 402, loss 0.00019388255896046758\n",
      "iteration 403, loss 0.00018891115905717015\n",
      "iteration 404, loss 0.00018406874733045697\n",
      "iteration 405, loss 0.00017935855430550873\n",
      "iteration 406, loss 0.00017475991626270115\n",
      "iteration 407, loss 0.0001702906156424433\n",
      "iteration 408, loss 0.00016593249165453017\n",
      "iteration 409, loss 0.00016168970614671707\n",
      "iteration 410, loss 0.00015755509957671165\n",
      "iteration 411, loss 0.00015352688205894083\n",
      "iteration 412, loss 0.0001496058830525726\n",
      "iteration 413, loss 0.00014578494301531464\n",
      "iteration 414, loss 0.0001420605694875121\n",
      "iteration 415, loss 0.00013843222404830158\n",
      "iteration 416, loss 0.00013490200217347592\n",
      "iteration 417, loss 0.0001314629625994712\n",
      "iteration 418, loss 0.00012810772750526667\n",
      "iteration 419, loss 0.00012484195758588612\n",
      "iteration 420, loss 0.00012166221858933568\n",
      "iteration 421, loss 0.00011856258061015978\n",
      "iteration 422, loss 0.00011554344382602721\n",
      "iteration 423, loss 0.00011260263272561133\n",
      "iteration 424, loss 0.00010973835742333904\n",
      "iteration 425, loss 0.00010694760567275807\n",
      "iteration 426, loss 0.00010422716877656057\n",
      "iteration 427, loss 0.00010157930955756456\n",
      "iteration 428, loss 9.899840370053425e-05\n",
      "iteration 429, loss 9.648097329773009e-05\n",
      "iteration 430, loss 9.403271542396396e-05\n",
      "iteration 431, loss 9.164500806946307e-05\n",
      "iteration 432, loss 8.932008495321497e-05\n",
      "iteration 433, loss 8.705463551450521e-05\n",
      "iteration 434, loss 8.484696445520967e-05\n",
      "iteration 435, loss 8.269659883808345e-05\n",
      "iteration 436, loss 8.060091931838542e-05\n",
      "iteration 437, loss 7.855911098886281e-05\n",
      "iteration 438, loss 7.656966772628948e-05\n",
      "iteration 439, loss 7.46290388633497e-05\n",
      "iteration 440, loss 7.273965456988662e-05\n",
      "iteration 441, loss 7.089965947670862e-05\n",
      "iteration 442, loss 6.910386582603678e-05\n",
      "iteration 443, loss 6.735926581313834e-05\n",
      "iteration 444, loss 6.565681542269886e-05\n",
      "iteration 445, loss 6.399644917109981e-05\n",
      "iteration 446, loss 6.23822197667323e-05\n",
      "iteration 447, loss 6.080475577618927e-05\n",
      "iteration 448, loss 5.927092570345849e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 449, loss 5.777354090241715e-05\n",
      "iteration 450, loss 5.6318040151381865e-05\n",
      "iteration 451, loss 5.4898366215638816e-05\n",
      "iteration 452, loss 5.3513795137405396e-05\n",
      "iteration 453, loss 5.216568388277665e-05\n",
      "iteration 454, loss 5.084961230750196e-05\n",
      "iteration 455, loss 4.9568516260478646e-05\n",
      "iteration 456, loss 4.8323447117581964e-05\n",
      "iteration 457, loss 4.7105553676374257e-05\n",
      "iteration 458, loss 4.592053301166743e-05\n",
      "iteration 459, loss 4.476350295590237e-05\n",
      "iteration 460, loss 4.363871266832575e-05\n",
      "iteration 461, loss 4.2541920265648514e-05\n",
      "iteration 462, loss 4.147242725593969e-05\n",
      "iteration 463, loss 4.0430924855172634e-05\n",
      "iteration 464, loss 3.941497561754659e-05\n",
      "iteration 465, loss 3.8425343518611044e-05\n",
      "iteration 466, loss 3.7459820305230096e-05\n",
      "iteration 467, loss 3.6521632864605635e-05\n",
      "iteration 468, loss 3.5604309232439846e-05\n",
      "iteration 469, loss 3.4710305044427514e-05\n",
      "iteration 470, loss 3.384103183634579e-05\n",
      "iteration 471, loss 3.299372474430129e-05\n",
      "iteration 472, loss 3.2167044992093e-05\n",
      "iteration 473, loss 3.136121085844934e-05\n",
      "iteration 474, loss 3.0575789423892274e-05\n",
      "iteration 475, loss 2.9810484193149023e-05\n",
      "iteration 476, loss 2.9063947295071557e-05\n",
      "iteration 477, loss 2.8337912226561457e-05\n",
      "iteration 478, loss 2.7629388569039293e-05\n",
      "iteration 479, loss 2.6940317184198648e-05\n",
      "iteration 480, loss 2.6266610802849755e-05\n",
      "iteration 481, loss 2.56102121056756e-05\n",
      "iteration 482, loss 2.4968871002784e-05\n",
      "iteration 483, loss 2.4346580175915733e-05\n",
      "iteration 484, loss 2.3738899471936747e-05\n",
      "iteration 485, loss 2.3146036255639046e-05\n",
      "iteration 486, loss 2.2568639906239696e-05\n",
      "iteration 487, loss 2.2005735445418395e-05\n",
      "iteration 488, loss 2.14573519770056e-05\n",
      "iteration 489, loss 2.092220529448241e-05\n",
      "iteration 490, loss 2.0402416339493357e-05\n",
      "iteration 491, loss 1.9893435819540173e-05\n",
      "iteration 492, loss 1.9396959032746963e-05\n",
      "iteration 493, loss 1.891469582915306e-05\n",
      "iteration 494, loss 1.8444101442582905e-05\n",
      "iteration 495, loss 1.7984810256166384e-05\n",
      "iteration 496, loss 1.7538170141051523e-05\n",
      "iteration 497, loss 1.7101925550377928e-05\n",
      "iteration 498, loss 1.6676676750648767e-05\n",
      "iteration 499, loss 1.626275115995668e-05\n"
     ]
    }
   ],
   "source": [
    "# implementing the same example using torch.nn package\n",
    "\n",
    "# NOTE: \n",
    "# Learning rate has changed from previous examples. \n",
    "# This is motivated by the fact that loss is decreasing just not fast enough\n",
    "# Though it is not clear why the same model would warant different learning rate when implemented with nn package.\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out))\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model = model.cuda(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss.item()))\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pytorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 781.8439331054688\n",
      "iteration 1, loss 763.1244506835938\n",
      "iteration 2, loss 744.964111328125\n",
      "iteration 3, loss 727.3046875\n",
      "iteration 4, loss 710.059814453125\n",
      "iteration 5, loss 693.2385864257812\n",
      "iteration 6, loss 676.8621826171875\n",
      "iteration 7, loss 660.990478515625\n",
      "iteration 8, loss 645.5855712890625\n",
      "iteration 9, loss 630.6854858398438\n",
      "iteration 10, loss 616.1856689453125\n",
      "iteration 11, loss 602.0438232421875\n",
      "iteration 12, loss 588.224853515625\n",
      "iteration 13, loss 574.8394775390625\n",
      "iteration 14, loss 561.8868408203125\n",
      "iteration 15, loss 549.3050537109375\n",
      "iteration 16, loss 537.050048828125\n",
      "iteration 17, loss 525.126708984375\n",
      "iteration 18, loss 513.48095703125\n",
      "iteration 19, loss 502.1620178222656\n",
      "iteration 20, loss 491.121337890625\n",
      "iteration 21, loss 480.3432922363281\n",
      "iteration 22, loss 469.953369140625\n",
      "iteration 23, loss 459.876953125\n",
      "iteration 24, loss 450.0588684082031\n",
      "iteration 25, loss 440.4822692871094\n",
      "iteration 26, loss 431.1605529785156\n",
      "iteration 27, loss 422.0596923828125\n",
      "iteration 28, loss 413.176025390625\n",
      "iteration 29, loss 404.4803161621094\n",
      "iteration 30, loss 396.00286865234375\n",
      "iteration 31, loss 387.69525146484375\n",
      "iteration 32, loss 379.56591796875\n",
      "iteration 33, loss 371.63018798828125\n",
      "iteration 34, loss 363.82440185546875\n",
      "iteration 35, loss 356.1622314453125\n",
      "iteration 36, loss 348.6470031738281\n",
      "iteration 37, loss 341.25653076171875\n",
      "iteration 38, loss 333.99041748046875\n",
      "iteration 39, loss 326.85791015625\n",
      "iteration 40, loss 319.8709716796875\n",
      "iteration 41, loss 313.0251770019531\n",
      "iteration 42, loss 306.2835388183594\n",
      "iteration 43, loss 299.68585205078125\n",
      "iteration 44, loss 293.2066345214844\n",
      "iteration 45, loss 286.8839111328125\n",
      "iteration 46, loss 280.686767578125\n",
      "iteration 47, loss 274.58319091796875\n",
      "iteration 48, loss 268.62103271484375\n",
      "iteration 49, loss 262.7674560546875\n",
      "iteration 50, loss 257.0213928222656\n",
      "iteration 51, loss 251.3956756591797\n",
      "iteration 52, loss 245.86163330078125\n",
      "iteration 53, loss 240.41403198242188\n",
      "iteration 54, loss 235.0523223876953\n",
      "iteration 55, loss 229.78614807128906\n",
      "iteration 56, loss 224.62069702148438\n",
      "iteration 57, loss 219.55563354492188\n",
      "iteration 58, loss 214.56954956054688\n",
      "iteration 59, loss 209.67401123046875\n",
      "iteration 60, loss 204.86566162109375\n",
      "iteration 61, loss 200.14146423339844\n",
      "iteration 62, loss 195.49789428710938\n",
      "iteration 63, loss 190.94003295898438\n",
      "iteration 64, loss 186.459716796875\n",
      "iteration 65, loss 182.06478881835938\n",
      "iteration 66, loss 177.75503540039062\n",
      "iteration 67, loss 173.5185546875\n",
      "iteration 68, loss 169.35572814941406\n",
      "iteration 69, loss 165.27120971679688\n",
      "iteration 70, loss 161.26913452148438\n",
      "iteration 71, loss 157.34970092773438\n",
      "iteration 72, loss 153.50222778320312\n",
      "iteration 73, loss 149.72210693359375\n",
      "iteration 74, loss 146.0169219970703\n",
      "iteration 75, loss 142.38711547851562\n",
      "iteration 76, loss 138.82635498046875\n",
      "iteration 77, loss 135.33116149902344\n",
      "iteration 78, loss 131.9004364013672\n",
      "iteration 79, loss 128.52993774414062\n",
      "iteration 80, loss 125.2282943725586\n",
      "iteration 81, loss 121.98501586914062\n",
      "iteration 82, loss 118.80501556396484\n",
      "iteration 83, loss 115.68618774414062\n",
      "iteration 84, loss 112.62197875976562\n",
      "iteration 85, loss 109.62129211425781\n",
      "iteration 86, loss 106.68717956542969\n",
      "iteration 87, loss 103.81266021728516\n",
      "iteration 88, loss 100.99586486816406\n",
      "iteration 89, loss 98.23568725585938\n",
      "iteration 90, loss 95.53158569335938\n",
      "iteration 91, loss 92.88919067382812\n",
      "iteration 92, loss 90.30632781982422\n",
      "iteration 93, loss 87.77972412109375\n",
      "iteration 94, loss 85.3038101196289\n",
      "iteration 95, loss 82.88409423828125\n",
      "iteration 96, loss 80.52262878417969\n",
      "iteration 97, loss 78.21282958984375\n",
      "iteration 98, loss 75.95540618896484\n",
      "iteration 99, loss 73.75232696533203\n",
      "iteration 100, loss 71.59951782226562\n",
      "iteration 101, loss 69.49905395507812\n",
      "iteration 102, loss 67.44798278808594\n",
      "iteration 103, loss 65.44646453857422\n",
      "iteration 104, loss 63.494537353515625\n",
      "iteration 105, loss 61.58709716796875\n",
      "iteration 106, loss 59.72567367553711\n",
      "iteration 107, loss 57.915252685546875\n",
      "iteration 108, loss 56.14950180053711\n",
      "iteration 109, loss 54.429813385009766\n",
      "iteration 110, loss 52.754249572753906\n",
      "iteration 111, loss 51.123680114746094\n",
      "iteration 112, loss 49.53432083129883\n",
      "iteration 113, loss 47.98796081542969\n",
      "iteration 114, loss 46.47969055175781\n",
      "iteration 115, loss 45.010833740234375\n",
      "iteration 116, loss 43.581844329833984\n",
      "iteration 117, loss 42.192378997802734\n",
      "iteration 118, loss 40.837318420410156\n",
      "iteration 119, loss 39.518836975097656\n",
      "iteration 120, loss 38.240501403808594\n",
      "iteration 121, loss 36.99420928955078\n",
      "iteration 122, loss 35.78096389770508\n",
      "iteration 123, loss 34.60338592529297\n",
      "iteration 124, loss 33.45838928222656\n",
      "iteration 125, loss 32.345672607421875\n",
      "iteration 126, loss 31.26375961303711\n",
      "iteration 127, loss 30.21426773071289\n",
      "iteration 128, loss 29.195117950439453\n",
      "iteration 129, loss 28.20725440979004\n",
      "iteration 130, loss 27.24961280822754\n",
      "iteration 131, loss 26.318700790405273\n",
      "iteration 132, loss 25.41592025756836\n",
      "iteration 133, loss 24.54067611694336\n",
      "iteration 134, loss 23.69150161743164\n",
      "iteration 135, loss 22.869813919067383\n",
      "iteration 136, loss 22.072978973388672\n",
      "iteration 137, loss 21.301929473876953\n",
      "iteration 138, loss 20.553070068359375\n",
      "iteration 139, loss 19.82879638671875\n",
      "iteration 140, loss 19.12701416015625\n",
      "iteration 141, loss 18.44888687133789\n",
      "iteration 142, loss 17.7911434173584\n",
      "iteration 143, loss 17.15692138671875\n",
      "iteration 144, loss 16.543149948120117\n",
      "iteration 145, loss 15.948393821716309\n",
      "iteration 146, loss 15.373353958129883\n",
      "iteration 147, loss 14.817131042480469\n",
      "iteration 148, loss 14.278202056884766\n",
      "iteration 149, loss 13.757017135620117\n",
      "iteration 150, loss 13.25252628326416\n",
      "iteration 151, loss 12.765016555786133\n",
      "iteration 152, loss 12.293169021606445\n",
      "iteration 153, loss 11.83660888671875\n",
      "iteration 154, loss 11.394319534301758\n",
      "iteration 155, loss 10.966660499572754\n",
      "iteration 156, loss 10.553146362304688\n",
      "iteration 157, loss 10.152755737304688\n",
      "iteration 158, loss 9.766471862792969\n",
      "iteration 159, loss 9.393533706665039\n",
      "iteration 160, loss 9.032915115356445\n",
      "iteration 161, loss 8.685111999511719\n",
      "iteration 162, loss 8.349565505981445\n",
      "iteration 163, loss 8.025428771972656\n",
      "iteration 164, loss 7.712893486022949\n",
      "iteration 165, loss 7.411812782287598\n",
      "iteration 166, loss 7.121827125549316\n",
      "iteration 167, loss 6.842080116271973\n",
      "iteration 168, loss 6.572716236114502\n",
      "iteration 169, loss 6.312939167022705\n",
      "iteration 170, loss 6.063102722167969\n",
      "iteration 171, loss 5.8225812911987305\n",
      "iteration 172, loss 5.590780735015869\n",
      "iteration 173, loss 5.367875099182129\n",
      "iteration 174, loss 5.153444290161133\n",
      "iteration 175, loss 4.94678258895874\n",
      "iteration 176, loss 4.74811315536499\n",
      "iteration 177, loss 4.557111740112305\n",
      "iteration 178, loss 4.373236656188965\n",
      "iteration 179, loss 4.196382999420166\n",
      "iteration 180, loss 4.026340484619141\n",
      "iteration 181, loss 3.8629016876220703\n",
      "iteration 182, loss 3.705826759338379\n",
      "iteration 183, loss 3.554737091064453\n",
      "iteration 184, loss 3.409592628479004\n",
      "iteration 185, loss 3.2702584266662598\n",
      "iteration 186, loss 3.1363484859466553\n",
      "iteration 187, loss 3.0077619552612305\n",
      "iteration 188, loss 2.8842785358428955\n",
      "iteration 189, loss 2.7655885219573975\n",
      "iteration 190, loss 2.6514928340911865\n",
      "iteration 191, loss 2.5420031547546387\n",
      "iteration 192, loss 2.4368786811828613\n",
      "iteration 193, loss 2.335925579071045\n",
      "iteration 194, loss 2.2389872074127197\n",
      "iteration 195, loss 2.145890474319458\n",
      "iteration 196, loss 2.056572914123535\n",
      "iteration 197, loss 1.970832109451294\n",
      "iteration 198, loss 1.8884960412979126\n",
      "iteration 199, loss 1.8096169233322144\n",
      "iteration 200, loss 1.7337543964385986\n",
      "iteration 201, loss 1.6609992980957031\n",
      "iteration 202, loss 1.5911660194396973\n",
      "iteration 203, loss 1.5241618156433105\n",
      "iteration 204, loss 1.4598455429077148\n",
      "iteration 205, loss 1.3981577157974243\n",
      "iteration 206, loss 1.338957667350769\n",
      "iteration 207, loss 1.28218674659729\n",
      "iteration 208, loss 1.2277668714523315\n",
      "iteration 209, loss 1.1754852533340454\n",
      "iteration 210, loss 1.1253571510314941\n",
      "iteration 211, loss 1.0773234367370605\n",
      "iteration 212, loss 1.0312508344650269\n",
      "iteration 213, loss 0.9870699644088745\n",
      "iteration 214, loss 0.9447279572486877\n",
      "iteration 215, loss 0.9041687846183777\n",
      "iteration 216, loss 0.8652663230895996\n",
      "iteration 217, loss 0.8279649019241333\n",
      "iteration 218, loss 0.7922376394271851\n",
      "iteration 219, loss 0.7579842805862427\n",
      "iteration 220, loss 0.7251747250556946\n",
      "iteration 221, loss 0.6937795877456665\n",
      "iteration 222, loss 0.6637204885482788\n",
      "iteration 223, loss 0.634928286075592\n",
      "iteration 224, loss 0.6073519587516785\n",
      "iteration 225, loss 0.5809135437011719\n",
      "iteration 226, loss 0.5556094646453857\n",
      "iteration 227, loss 0.531378448009491\n",
      "iteration 228, loss 0.5081573724746704\n",
      "iteration 229, loss 0.48591333627700806\n",
      "iteration 230, loss 0.4646328389644623\n",
      "iteration 231, loss 0.444259375333786\n",
      "iteration 232, loss 0.4247533082962036\n",
      "iteration 233, loss 0.40608322620391846\n",
      "iteration 234, loss 0.3882092237472534\n",
      "iteration 235, loss 0.37110209465026855\n",
      "iteration 236, loss 0.35472607612609863\n",
      "iteration 237, loss 0.339053750038147\n",
      "iteration 238, loss 0.3240557312965393\n",
      "iteration 239, loss 0.3096987307071686\n",
      "iteration 240, loss 0.2959628701210022\n",
      "iteration 241, loss 0.2828235626220703\n",
      "iteration 242, loss 0.27025461196899414\n",
      "iteration 243, loss 0.25823307037353516\n",
      "iteration 244, loss 0.24673588573932648\n",
      "iteration 245, loss 0.23573046922683716\n",
      "iteration 246, loss 0.22520796954631805\n",
      "iteration 247, loss 0.21515247225761414\n",
      "iteration 248, loss 0.2055307924747467\n",
      "iteration 249, loss 0.19633184373378754\n",
      "iteration 250, loss 0.187536358833313\n",
      "iteration 251, loss 0.17912690341472626\n",
      "iteration 252, loss 0.1710875928401947\n",
      "iteration 253, loss 0.16340330243110657\n",
      "iteration 254, loss 0.15605829656124115\n",
      "iteration 255, loss 0.14903922379016876\n",
      "iteration 256, loss 0.1423298567533493\n",
      "iteration 257, loss 0.135922372341156\n",
      "iteration 258, loss 0.12979209423065186\n",
      "iteration 259, loss 0.12394064664840698\n",
      "iteration 260, loss 0.11834582686424255\n",
      "iteration 261, loss 0.1130041554570198\n",
      "iteration 262, loss 0.10791145265102386\n",
      "iteration 263, loss 0.10305090248584747\n",
      "iteration 264, loss 0.09840758889913559\n",
      "iteration 265, loss 0.09397175163030624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 266, loss 0.08973495662212372\n",
      "iteration 267, loss 0.08569058775901794\n",
      "iteration 268, loss 0.08182896673679352\n",
      "iteration 269, loss 0.0781402736902237\n",
      "iteration 270, loss 0.07461853325366974\n",
      "iteration 271, loss 0.07125508785247803\n",
      "iteration 272, loss 0.06804170459508896\n",
      "iteration 273, loss 0.0649736300110817\n",
      "iteration 274, loss 0.062043141573667526\n",
      "iteration 275, loss 0.059244997799396515\n",
      "iteration 276, loss 0.05657241493463516\n",
      "iteration 277, loss 0.05402098968625069\n",
      "iteration 278, loss 0.05158504843711853\n",
      "iteration 279, loss 0.04925819858908653\n",
      "iteration 280, loss 0.0470360592007637\n",
      "iteration 281, loss 0.04491424560546875\n",
      "iteration 282, loss 0.04288914427161217\n",
      "iteration 283, loss 0.04095599800348282\n",
      "iteration 284, loss 0.03910956159234047\n",
      "iteration 285, loss 0.037346579134464264\n",
      "iteration 286, loss 0.03566347062587738\n",
      "iteration 287, loss 0.034055862575769424\n",
      "iteration 288, loss 0.03252062574028969\n",
      "iteration 289, loss 0.031055081635713577\n",
      "iteration 290, loss 0.02965511381626129\n",
      "iteration 291, loss 0.028318431228399277\n",
      "iteration 292, loss 0.027042457833886147\n",
      "iteration 293, loss 0.0258238073438406\n",
      "iteration 294, loss 0.02466089278459549\n",
      "iteration 295, loss 0.023549210280179977\n",
      "iteration 296, loss 0.022488586604595184\n",
      "iteration 297, loss 0.021475892513990402\n",
      "iteration 298, loss 0.020508799701929092\n",
      "iteration 299, loss 0.01958513632416725\n",
      "iteration 300, loss 0.018703296780586243\n",
      "iteration 301, loss 0.017861027270555496\n",
      "iteration 302, loss 0.017056748270988464\n",
      "iteration 303, loss 0.01628868840634823\n",
      "iteration 304, loss 0.015555180609226227\n",
      "iteration 305, loss 0.014854779466986656\n",
      "iteration 306, loss 0.014185965061187744\n",
      "iteration 307, loss 0.01354735903441906\n",
      "iteration 308, loss 0.012937040999531746\n",
      "iteration 309, loss 0.01235444936901331\n",
      "iteration 310, loss 0.01179804839193821\n",
      "iteration 311, loss 0.011266691610217094\n",
      "iteration 312, loss 0.010759145952761173\n",
      "iteration 313, loss 0.01027450617402792\n",
      "iteration 314, loss 0.009811402298510075\n",
      "iteration 315, loss 0.009369293227791786\n",
      "iteration 316, loss 0.008947072550654411\n",
      "iteration 317, loss 0.008543615229427814\n",
      "iteration 318, loss 0.008158456534147263\n",
      "iteration 319, loss 0.007790790405124426\n",
      "iteration 320, loss 0.007440061774104834\n",
      "iteration 321, loss 0.007105143740773201\n",
      "iteration 322, loss 0.006785118021070957\n",
      "iteration 323, loss 0.006479545496404171\n",
      "iteration 324, loss 0.006187614984810352\n",
      "iteration 325, loss 0.0059087928384542465\n",
      "iteration 326, loss 0.00564234796911478\n",
      "iteration 327, loss 0.005387832410633564\n",
      "iteration 328, loss 0.005144766066223383\n",
      "iteration 329, loss 0.0049125924706459045\n",
      "iteration 330, loss 0.004690688569098711\n",
      "iteration 331, loss 0.004478737711906433\n",
      "iteration 332, loss 0.00427636131644249\n",
      "iteration 333, loss 0.004082928877323866\n",
      "iteration 334, loss 0.003898209659382701\n",
      "iteration 335, loss 0.00372174265794456\n",
      "iteration 336, loss 0.0035531697794795036\n",
      "iteration 337, loss 0.0033921278081834316\n",
      "iteration 338, loss 0.003238269127905369\n",
      "iteration 339, loss 0.003091296646744013\n",
      "iteration 340, loss 0.00295091001316905\n",
      "iteration 341, loss 0.0028168093413114548\n",
      "iteration 342, loss 0.0026887005660682917\n",
      "iteration 343, loss 0.002566348994150758\n",
      "iteration 344, loss 0.0024494014214724302\n",
      "iteration 345, loss 0.002337767044082284\n",
      "iteration 346, loss 0.002231116406619549\n",
      "iteration 347, loss 0.0021292190067470074\n",
      "iteration 348, loss 0.0020319290924817324\n",
      "iteration 349, loss 0.0019390038214623928\n",
      "iteration 350, loss 0.0018502408638596535\n",
      "iteration 351, loss 0.0017654441762715578\n",
      "iteration 352, loss 0.0016844798810780048\n",
      "iteration 353, loss 0.0016071388963609934\n",
      "iteration 354, loss 0.0015332711627706885\n",
      "iteration 355, loss 0.0014627280179411173\n",
      "iteration 356, loss 0.0013953896705061197\n",
      "iteration 357, loss 0.0013310378417372704\n",
      "iteration 358, loss 0.0012696015182882547\n",
      "iteration 359, loss 0.001210943330079317\n",
      "iteration 360, loss 0.001154918922111392\n",
      "iteration 361, loss 0.0011014363262802362\n",
      "iteration 362, loss 0.0010503778466954827\n",
      "iteration 363, loss 0.0010016104206442833\n",
      "iteration 364, loss 0.0009550685063004494\n",
      "iteration 365, loss 0.0009106253273785114\n",
      "iteration 366, loss 0.000868201837874949\n",
      "iteration 367, loss 0.0008276914013549685\n",
      "iteration 368, loss 0.0007890431443229318\n",
      "iteration 369, loss 0.0007521422812715173\n",
      "iteration 370, loss 0.0007169240852817893\n",
      "iteration 371, loss 0.0006833176594227552\n",
      "iteration 372, loss 0.0006512359832413495\n",
      "iteration 373, loss 0.0006206159014254808\n",
      "iteration 374, loss 0.0005913967615924776\n",
      "iteration 375, loss 0.0005635203560814261\n",
      "iteration 376, loss 0.0005369159043766558\n",
      "iteration 377, loss 0.0005115442909300327\n",
      "iteration 378, loss 0.00048732879804447293\n",
      "iteration 379, loss 0.00046422419836744666\n",
      "iteration 380, loss 0.0004421910853125155\n",
      "iteration 381, loss 0.0004211735213175416\n",
      "iteration 382, loss 0.0004011253477074206\n",
      "iteration 383, loss 0.00038200203562155366\n",
      "iteration 384, loss 0.00036376158823259175\n",
      "iteration 385, loss 0.0003463705361355096\n",
      "iteration 386, loss 0.00032978696981444955\n",
      "iteration 387, loss 0.00031397020211443305\n",
      "iteration 388, loss 0.00029889086727052927\n",
      "iteration 389, loss 0.00028450891841202974\n",
      "iteration 390, loss 0.0002708013344090432\n",
      "iteration 391, loss 0.0002577329578343779\n",
      "iteration 392, loss 0.0002452780317980796\n",
      "iteration 393, loss 0.00023340214102063328\n",
      "iteration 394, loss 0.00022208652808330953\n",
      "iteration 395, loss 0.00021130408276803792\n",
      "iteration 396, loss 0.0002010274911299348\n",
      "iteration 397, loss 0.00019123370293527842\n",
      "iteration 398, loss 0.00018190803530160338\n",
      "iteration 399, loss 0.00017301979823969305\n",
      "iteration 400, loss 0.0001645617448957637\n",
      "iteration 401, loss 0.0001564838457852602\n",
      "iteration 402, loss 0.00014879861555527896\n",
      "iteration 403, loss 0.00014148178161121905\n",
      "iteration 404, loss 0.0001345127820968628\n",
      "iteration 405, loss 0.0001278785348404199\n",
      "iteration 406, loss 0.00012155772128608078\n",
      "iteration 407, loss 0.00011554301454452798\n",
      "iteration 408, loss 0.00010981447121594101\n",
      "iteration 409, loss 0.00010436000593472272\n",
      "iteration 410, loss 9.916616545524448e-05\n",
      "iteration 411, loss 9.422526636626571e-05\n",
      "iteration 412, loss 8.952336065704003e-05\n",
      "iteration 413, loss 8.504528523189947e-05\n",
      "iteration 414, loss 8.07843534857966e-05\n",
      "iteration 415, loss 7.673290383536369e-05\n",
      "iteration 416, loss 7.287594780791551e-05\n",
      "iteration 417, loss 6.920711894053966e-05\n",
      "iteration 418, loss 6.57183991279453e-05\n",
      "iteration 419, loss 6.239961658138782e-05\n",
      "iteration 420, loss 5.9241145208943635e-05\n",
      "iteration 421, loss 5.6237495300592855e-05\n",
      "iteration 422, loss 5.338128175935708e-05\n",
      "iteration 423, loss 5.066846642876044e-05\n",
      "iteration 424, loss 4.808573430636898e-05\n",
      "iteration 425, loss 4.563291440717876e-05\n",
      "iteration 426, loss 4.330007141106762e-05\n",
      "iteration 427, loss 4.108203938812949e-05\n",
      "iteration 428, loss 3.8973674236331135e-05\n",
      "iteration 429, loss 3.6973095120629296e-05\n",
      "iteration 430, loss 3.507008295855485e-05\n",
      "iteration 431, loss 3.326013029436581e-05\n",
      "iteration 432, loss 3.1541683711111546e-05\n",
      "iteration 433, loss 2.9908502256148495e-05\n",
      "iteration 434, loss 2.8359489078866318e-05\n",
      "iteration 435, loss 2.6885303668677807e-05\n",
      "iteration 436, loss 2.548621705500409e-05\n",
      "iteration 437, loss 2.4159093300113454e-05\n",
      "iteration 438, loss 2.2895697838976048e-05\n",
      "iteration 439, loss 2.170100560761057e-05\n",
      "iteration 440, loss 2.0564002625178546e-05\n",
      "iteration 441, loss 1.948420685948804e-05\n",
      "iteration 442, loss 1.8460543287801556e-05\n",
      "iteration 443, loss 1.7487865989096463e-05\n",
      "iteration 444, loss 1.6564761608606204e-05\n",
      "iteration 445, loss 1.5689032807131298e-05\n",
      "iteration 446, loss 1.4858845133858267e-05\n",
      "iteration 447, loss 1.4070764336793218e-05\n",
      "iteration 448, loss 1.3322409358806908e-05\n",
      "iteration 449, loss 1.2612445971171837e-05\n",
      "iteration 450, loss 1.1939953765249811e-05\n",
      "iteration 451, loss 1.1302448001515586e-05\n",
      "iteration 452, loss 1.0697559446271043e-05\n",
      "iteration 453, loss 1.0124402251676656e-05\n",
      "iteration 454, loss 9.57939300860744e-06\n",
      "iteration 455, loss 9.064839105121791e-06\n",
      "iteration 456, loss 8.576133950555231e-06\n",
      "iteration 457, loss 8.112603609333746e-06\n",
      "iteration 458, loss 7.673912477912381e-06\n",
      "iteration 459, loss 7.257932338688988e-06\n",
      "iteration 460, loss 6.864668648631778e-06\n",
      "iteration 461, loss 6.4907767409749795e-06\n",
      "iteration 462, loss 6.136524007160915e-06\n",
      "iteration 463, loss 5.8015857575810514e-06\n",
      "iteration 464, loss 5.484525900101289e-06\n",
      "iteration 465, loss 5.184224391996395e-06\n",
      "iteration 466, loss 4.899118721368723e-06\n",
      "iteration 467, loss 4.629959676094586e-06\n",
      "iteration 468, loss 4.374745003588032e-06\n",
      "iteration 469, loss 4.133173661102774e-06\n",
      "iteration 470, loss 3.905379344359972e-06\n",
      "iteration 471, loss 3.6889200600853655e-06\n",
      "iteration 472, loss 3.483842647256097e-06\n",
      "iteration 473, loss 3.2903574265219504e-06\n",
      "iteration 474, loss 3.106879148617736e-06\n",
      "iteration 475, loss 2.9341936169657856e-06\n",
      "iteration 476, loss 2.7702656097972067e-06\n",
      "iteration 477, loss 2.6150235044042347e-06\n",
      "iteration 478, loss 2.468413640599465e-06\n",
      "iteration 479, loss 2.329770950382226e-06\n",
      "iteration 480, loss 2.198565880462411e-06\n",
      "iteration 481, loss 2.0749273517139954e-06\n",
      "iteration 482, loss 1.957163021870656e-06\n",
      "iteration 483, loss 1.8464384083927143e-06\n",
      "iteration 484, loss 1.7420022686565062e-06\n",
      "iteration 485, loss 1.643149971641833e-06\n",
      "iteration 486, loss 1.549617991258856e-06\n",
      "iteration 487, loss 1.4612566019422957e-06\n",
      "iteration 488, loss 1.3779952041659271e-06\n",
      "iteration 489, loss 1.2991679341212148e-06\n",
      "iteration 490, loss 1.2242676348250825e-06\n",
      "iteration 491, loss 1.1540588502612081e-06\n",
      "iteration 492, loss 1.0877314480239875e-06\n",
      "iteration 493, loss 1.0251665116811637e-06\n",
      "iteration 494, loss 9.661372359914822e-07\n",
      "iteration 495, loss 9.100721172217163e-07\n",
      "iteration 496, loss 8.57389011343912e-07\n",
      "iteration 497, loss 8.075838309196115e-07\n",
      "iteration 498, loss 7.608024361616117e-07\n",
      "iteration 499, loss 7.164859425756731e-07\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out))\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model = model.cuda(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss.item()))\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 645.8909912109375\n",
      "iteration 1, loss 629.0782470703125\n",
      "iteration 2, loss 612.7051391601562\n",
      "iteration 3, loss 596.8748779296875\n",
      "iteration 4, loss 581.6117553710938\n",
      "iteration 5, loss 566.8129272460938\n",
      "iteration 6, loss 552.460205078125\n",
      "iteration 7, loss 538.5153198242188\n",
      "iteration 8, loss 525.016845703125\n",
      "iteration 9, loss 511.91925048828125\n",
      "iteration 10, loss 499.3203125\n",
      "iteration 11, loss 487.110107421875\n",
      "iteration 12, loss 475.3283386230469\n",
      "iteration 13, loss 463.8554992675781\n",
      "iteration 14, loss 452.7047119140625\n",
      "iteration 15, loss 441.9375\n",
      "iteration 16, loss 431.5498046875\n",
      "iteration 17, loss 421.43499755859375\n",
      "iteration 18, loss 411.5952453613281\n",
      "iteration 19, loss 402.0108642578125\n",
      "iteration 20, loss 392.67840576171875\n",
      "iteration 21, loss 383.5556945800781\n",
      "iteration 22, loss 374.69439697265625\n",
      "iteration 23, loss 366.0718994140625\n",
      "iteration 24, loss 357.6633605957031\n",
      "iteration 25, loss 349.4852294921875\n",
      "iteration 26, loss 341.5057067871094\n",
      "iteration 27, loss 333.72509765625\n",
      "iteration 28, loss 326.1396789550781\n",
      "iteration 29, loss 318.7693786621094\n",
      "iteration 30, loss 311.56842041015625\n",
      "iteration 31, loss 304.533203125\n",
      "iteration 32, loss 297.6630859375\n",
      "iteration 33, loss 290.98333740234375\n",
      "iteration 34, loss 284.45068359375\n",
      "iteration 35, loss 278.0625915527344\n",
      "iteration 36, loss 271.8343505859375\n",
      "iteration 37, loss 265.75238037109375\n",
      "iteration 38, loss 259.80389404296875\n",
      "iteration 39, loss 253.99072265625\n",
      "iteration 40, loss 248.28939819335938\n",
      "iteration 41, loss 242.7104949951172\n",
      "iteration 42, loss 237.2578125\n",
      "iteration 43, loss 231.9392852783203\n",
      "iteration 44, loss 226.7436981201172\n",
      "iteration 45, loss 221.66265869140625\n",
      "iteration 46, loss 216.6691131591797\n",
      "iteration 47, loss 211.78392028808594\n",
      "iteration 48, loss 206.98086547851562\n",
      "iteration 49, loss 202.27960205078125\n",
      "iteration 50, loss 197.67626953125\n",
      "iteration 51, loss 193.1590576171875\n",
      "iteration 52, loss 188.73483276367188\n",
      "iteration 53, loss 184.39450073242188\n",
      "iteration 54, loss 180.13595581054688\n",
      "iteration 55, loss 175.96408081054688\n",
      "iteration 56, loss 171.87014770507812\n",
      "iteration 57, loss 167.8610382080078\n",
      "iteration 58, loss 163.92422485351562\n",
      "iteration 59, loss 160.05894470214844\n",
      "iteration 60, loss 156.2612762451172\n",
      "iteration 61, loss 152.53225708007812\n",
      "iteration 62, loss 148.86386108398438\n",
      "iteration 63, loss 145.26632690429688\n",
      "iteration 64, loss 141.7515411376953\n",
      "iteration 65, loss 138.3056182861328\n",
      "iteration 66, loss 134.9239501953125\n",
      "iteration 67, loss 131.62252807617188\n",
      "iteration 68, loss 128.39892578125\n",
      "iteration 69, loss 125.23636627197266\n",
      "iteration 70, loss 122.1405029296875\n",
      "iteration 71, loss 119.1047592163086\n",
      "iteration 72, loss 116.12582397460938\n",
      "iteration 73, loss 113.20368957519531\n",
      "iteration 74, loss 110.34440612792969\n",
      "iteration 75, loss 107.54110717773438\n",
      "iteration 76, loss 104.78718566894531\n",
      "iteration 77, loss 102.0890884399414\n",
      "iteration 78, loss 99.43958282470703\n",
      "iteration 79, loss 96.84999084472656\n",
      "iteration 80, loss 94.30904388427734\n",
      "iteration 81, loss 91.81831359863281\n",
      "iteration 82, loss 89.38050842285156\n",
      "iteration 83, loss 86.99014282226562\n",
      "iteration 84, loss 84.6518325805664\n",
      "iteration 85, loss 82.36125946044922\n",
      "iteration 86, loss 80.11607360839844\n",
      "iteration 87, loss 77.92558288574219\n",
      "iteration 88, loss 75.78244018554688\n",
      "iteration 89, loss 73.69100952148438\n",
      "iteration 90, loss 71.64118957519531\n",
      "iteration 91, loss 69.6357421875\n",
      "iteration 92, loss 67.67138671875\n",
      "iteration 93, loss 65.75623321533203\n",
      "iteration 94, loss 63.8851318359375\n",
      "iteration 95, loss 62.056312561035156\n",
      "iteration 96, loss 60.272220611572266\n",
      "iteration 97, loss 58.53184509277344\n",
      "iteration 98, loss 56.831783294677734\n",
      "iteration 99, loss 55.1729736328125\n",
      "iteration 100, loss 53.55547332763672\n",
      "iteration 101, loss 51.97172546386719\n",
      "iteration 102, loss 50.42657470703125\n",
      "iteration 103, loss 48.91941452026367\n",
      "iteration 104, loss 47.455322265625\n",
      "iteration 105, loss 46.030128479003906\n",
      "iteration 106, loss 44.64129638671875\n",
      "iteration 107, loss 43.287940979003906\n",
      "iteration 108, loss 41.971771240234375\n",
      "iteration 109, loss 40.68914031982422\n",
      "iteration 110, loss 39.439178466796875\n",
      "iteration 111, loss 38.221923828125\n",
      "iteration 112, loss 37.03856658935547\n",
      "iteration 113, loss 35.8863639831543\n",
      "iteration 114, loss 34.76523208618164\n",
      "iteration 115, loss 33.675262451171875\n",
      "iteration 116, loss 32.6114387512207\n",
      "iteration 117, loss 31.57750701904297\n",
      "iteration 118, loss 30.57256317138672\n",
      "iteration 119, loss 29.59616470336914\n",
      "iteration 120, loss 28.647716522216797\n",
      "iteration 121, loss 27.722965240478516\n",
      "iteration 122, loss 26.825109481811523\n",
      "iteration 123, loss 25.953712463378906\n",
      "iteration 124, loss 25.10628890991211\n",
      "iteration 125, loss 24.280481338500977\n",
      "iteration 126, loss 23.47792625427246\n",
      "iteration 127, loss 22.699756622314453\n",
      "iteration 128, loss 21.941844940185547\n",
      "iteration 129, loss 21.207338333129883\n",
      "iteration 130, loss 20.49199676513672\n",
      "iteration 131, loss 19.79868507385254\n",
      "iteration 132, loss 19.125598907470703\n",
      "iteration 133, loss 18.472246170043945\n",
      "iteration 134, loss 17.839160919189453\n",
      "iteration 135, loss 17.224163055419922\n",
      "iteration 136, loss 16.628049850463867\n",
      "iteration 137, loss 16.050251007080078\n",
      "iteration 138, loss 15.490827560424805\n",
      "iteration 139, loss 14.94807243347168\n",
      "iteration 140, loss 14.421928405761719\n",
      "iteration 141, loss 13.912879943847656\n",
      "iteration 142, loss 13.419183731079102\n",
      "iteration 143, loss 12.942838668823242\n",
      "iteration 144, loss 12.480100631713867\n",
      "iteration 145, loss 12.032478332519531\n",
      "iteration 146, loss 11.599493026733398\n",
      "iteration 147, loss 11.181014060974121\n",
      "iteration 148, loss 10.775375366210938\n",
      "iteration 149, loss 10.38361930847168\n",
      "iteration 150, loss 10.004425048828125\n",
      "iteration 151, loss 9.637763023376465\n",
      "iteration 152, loss 9.283562660217285\n",
      "iteration 153, loss 8.941554069519043\n",
      "iteration 154, loss 8.61086368560791\n",
      "iteration 155, loss 8.291861534118652\n",
      "iteration 156, loss 7.983716011047363\n",
      "iteration 157, loss 7.685860633850098\n",
      "iteration 158, loss 7.398338317871094\n",
      "iteration 159, loss 7.120393753051758\n",
      "iteration 160, loss 6.851726531982422\n",
      "iteration 161, loss 6.592074394226074\n",
      "iteration 162, loss 6.341769695281982\n",
      "iteration 163, loss 6.099824905395508\n",
      "iteration 164, loss 5.866584300994873\n",
      "iteration 165, loss 5.641595363616943\n",
      "iteration 166, loss 5.4242753982543945\n",
      "iteration 167, loss 5.214846611022949\n",
      "iteration 168, loss 5.0126800537109375\n",
      "iteration 169, loss 4.817770957946777\n",
      "iteration 170, loss 4.62977933883667\n",
      "iteration 171, loss 4.448521614074707\n",
      "iteration 172, loss 4.273907661437988\n",
      "iteration 173, loss 4.105634689331055\n",
      "iteration 174, loss 3.9435911178588867\n",
      "iteration 175, loss 3.7873430252075195\n",
      "iteration 176, loss 3.636843681335449\n",
      "iteration 177, loss 3.491687774658203\n",
      "iteration 178, loss 3.352121353149414\n",
      "iteration 179, loss 3.217461585998535\n",
      "iteration 180, loss 3.0878121852874756\n",
      "iteration 181, loss 2.9629361629486084\n",
      "iteration 182, loss 2.842841863632202\n",
      "iteration 183, loss 2.726987361907959\n",
      "iteration 184, loss 2.61568284034729\n",
      "iteration 185, loss 2.5080671310424805\n",
      "iteration 186, loss 2.4046788215637207\n",
      "iteration 187, loss 2.3051393032073975\n",
      "iteration 188, loss 2.2095069885253906\n",
      "iteration 189, loss 2.1175942420959473\n",
      "iteration 190, loss 2.0290884971618652\n",
      "iteration 191, loss 1.9440605640411377\n",
      "iteration 192, loss 1.8622502088546753\n",
      "iteration 193, loss 1.7836605310440063\n",
      "iteration 194, loss 1.7080860137939453\n",
      "iteration 195, loss 1.6353899240493774\n",
      "iteration 196, loss 1.5655556917190552\n",
      "iteration 197, loss 1.4984824657440186\n",
      "iteration 198, loss 1.4339799880981445\n",
      "iteration 199, loss 1.3719828128814697\n",
      "iteration 200, loss 1.3125343322753906\n",
      "iteration 201, loss 1.2552735805511475\n",
      "iteration 202, loss 1.2003660202026367\n",
      "iteration 203, loss 1.1476924419403076\n",
      "iteration 204, loss 1.0971734523773193\n",
      "iteration 205, loss 1.0486363172531128\n",
      "iteration 206, loss 1.00210702419281\n",
      "iteration 207, loss 0.9574811458587646\n",
      "iteration 208, loss 0.9145962595939636\n",
      "iteration 209, loss 0.8735004663467407\n",
      "iteration 210, loss 0.8341072797775269\n",
      "iteration 211, loss 0.7963517904281616\n",
      "iteration 212, loss 0.7601484656333923\n",
      "iteration 213, loss 0.725460410118103\n",
      "iteration 214, loss 0.6922034621238708\n",
      "iteration 215, loss 0.6603367328643799\n",
      "iteration 216, loss 0.6298094987869263\n",
      "iteration 217, loss 0.6005927920341492\n",
      "iteration 218, loss 0.5726165175437927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 219, loss 0.5458111763000488\n",
      "iteration 220, loss 0.5201559662818909\n",
      "iteration 221, loss 0.49562501907348633\n",
      "iteration 222, loss 0.4721716046333313\n",
      "iteration 223, loss 0.4497354030609131\n",
      "iteration 224, loss 0.4282647371292114\n",
      "iteration 225, loss 0.4077549874782562\n",
      "iteration 226, loss 0.3881549835205078\n",
      "iteration 227, loss 0.3694046139717102\n",
      "iteration 228, loss 0.35150229930877686\n",
      "iteration 229, loss 0.334400475025177\n",
      "iteration 230, loss 0.31806686520576477\n",
      "iteration 231, loss 0.3024771511554718\n",
      "iteration 232, loss 0.28759247064590454\n",
      "iteration 233, loss 0.2733842432498932\n",
      "iteration 234, loss 0.2598424553871155\n",
      "iteration 235, loss 0.24691565334796906\n",
      "iteration 236, loss 0.23459258675575256\n",
      "iteration 237, loss 0.22282052040100098\n",
      "iteration 238, loss 0.2116117775440216\n",
      "iteration 239, loss 0.20092982053756714\n",
      "iteration 240, loss 0.19073930382728577\n",
      "iteration 241, loss 0.1810360550880432\n",
      "iteration 242, loss 0.17179328203201294\n",
      "iteration 243, loss 0.16299059987068176\n",
      "iteration 244, loss 0.15460669994354248\n",
      "iteration 245, loss 0.14662772417068481\n",
      "iteration 246, loss 0.13903099298477173\n",
      "iteration 247, loss 0.1318182647228241\n",
      "iteration 248, loss 0.12492568790912628\n",
      "iteration 249, loss 0.11838418245315552\n",
      "iteration 250, loss 0.11216437816619873\n",
      "iteration 251, loss 0.1062500923871994\n",
      "iteration 252, loss 0.10062766820192337\n",
      "iteration 253, loss 0.09528730809688568\n",
      "iteration 254, loss 0.09020854532718658\n",
      "iteration 255, loss 0.08538371324539185\n",
      "iteration 256, loss 0.08080261200666428\n",
      "iteration 257, loss 0.07645618915557861\n",
      "iteration 258, loss 0.07232837378978729\n",
      "iteration 259, loss 0.0684092566370964\n",
      "iteration 260, loss 0.06468982249498367\n",
      "iteration 261, loss 0.061162643134593964\n",
      "iteration 262, loss 0.05781687796115875\n",
      "iteration 263, loss 0.05464426055550575\n",
      "iteration 264, loss 0.05163729935884476\n",
      "iteration 265, loss 0.04878463223576546\n",
      "iteration 266, loss 0.04608305171132088\n",
      "iteration 267, loss 0.04352283850312233\n",
      "iteration 268, loss 0.041096750646829605\n",
      "iteration 269, loss 0.038798701018095016\n",
      "iteration 270, loss 0.036623068153858185\n",
      "iteration 271, loss 0.03456304594874382\n",
      "iteration 272, loss 0.0326114222407341\n",
      "iteration 273, loss 0.03076614998281002\n",
      "iteration 274, loss 0.029019450768828392\n",
      "iteration 275, loss 0.02736678533256054\n",
      "iteration 276, loss 0.025803150609135628\n",
      "iteration 277, loss 0.024324879050254822\n",
      "iteration 278, loss 0.02292756363749504\n",
      "iteration 279, loss 0.02160615473985672\n",
      "iteration 280, loss 0.02035854011774063\n",
      "iteration 281, loss 0.019177965819835663\n",
      "iteration 282, loss 0.018063880503177643\n",
      "iteration 283, loss 0.017011266201734543\n",
      "iteration 284, loss 0.016017116606235504\n",
      "iteration 285, loss 0.01507890596985817\n",
      "iteration 286, loss 0.014192614704370499\n",
      "iteration 287, loss 0.01335674338042736\n",
      "iteration 288, loss 0.012567561119794846\n",
      "iteration 289, loss 0.011823242530226707\n",
      "iteration 290, loss 0.011121050454676151\n",
      "iteration 291, loss 0.010458902455866337\n",
      "iteration 292, loss 0.0098351389169693\n",
      "iteration 293, loss 0.009245876222848892\n",
      "iteration 294, loss 0.008691169321537018\n",
      "iteration 295, loss 0.008167990483343601\n",
      "iteration 296, loss 0.00767526775598526\n",
      "iteration 297, loss 0.007211082614958286\n",
      "iteration 298, loss 0.006773795001208782\n",
      "iteration 299, loss 0.006361983250826597\n",
      "iteration 300, loss 0.005974136758595705\n",
      "iteration 301, loss 0.005609313957393169\n",
      "iteration 302, loss 0.005265885032713413\n",
      "iteration 303, loss 0.004942240193486214\n",
      "iteration 304, loss 0.004638106096535921\n",
      "iteration 305, loss 0.004351973533630371\n",
      "iteration 306, loss 0.0040829721838235855\n",
      "iteration 307, loss 0.0038297364953905344\n",
      "iteration 308, loss 0.003591685090214014\n",
      "iteration 309, loss 0.0033680815249681473\n",
      "iteration 310, loss 0.0031576016917824745\n",
      "iteration 311, loss 0.002960069105029106\n",
      "iteration 312, loss 0.0027743095997720957\n",
      "iteration 313, loss 0.0025998323690146208\n",
      "iteration 314, loss 0.0024360979441553354\n",
      "iteration 315, loss 0.00228207278996706\n",
      "iteration 316, loss 0.0021375780925154686\n",
      "iteration 317, loss 0.002001882065087557\n",
      "iteration 318, loss 0.0018745344132184982\n",
      "iteration 319, loss 0.0017550402553752065\n",
      "iteration 320, loss 0.0016428267117589712\n",
      "iteration 321, loss 0.0015375985531136394\n",
      "iteration 322, loss 0.0014389045536518097\n",
      "iteration 323, loss 0.00134630489628762\n",
      "iteration 324, loss 0.0012594705913215876\n",
      "iteration 325, loss 0.001178049948066473\n",
      "iteration 326, loss 0.0011017811484634876\n",
      "iteration 327, loss 0.0010302970185875893\n",
      "iteration 328, loss 0.0009633272420614958\n",
      "iteration 329, loss 0.0009005654137581587\n",
      "iteration 330, loss 0.000841780798509717\n",
      "iteration 331, loss 0.0007867077365517616\n",
      "iteration 332, loss 0.0007351457024924457\n",
      "iteration 333, loss 0.0006868511554785073\n",
      "iteration 334, loss 0.0006416328833438456\n",
      "iteration 335, loss 0.0005993067752569914\n",
      "iteration 336, loss 0.0005596947739832103\n",
      "iteration 337, loss 0.0005226953653618693\n",
      "iteration 338, loss 0.00048793829046189785\n",
      "iteration 339, loss 0.00045549788046628237\n",
      "iteration 340, loss 0.0004251522186677903\n",
      "iteration 341, loss 0.00039677531458437443\n",
      "iteration 342, loss 0.0003702555550262332\n",
      "iteration 343, loss 0.0003454104298725724\n",
      "iteration 344, loss 0.00032221339643001556\n",
      "iteration 345, loss 0.00030053092632442713\n",
      "iteration 346, loss 0.00028027064399793744\n",
      "iteration 347, loss 0.0002613327233120799\n",
      "iteration 348, loss 0.0002436319482512772\n",
      "iteration 349, loss 0.00022711306519340724\n",
      "iteration 350, loss 0.00021166827355045825\n",
      "iteration 351, loss 0.0001972694881260395\n",
      "iteration 352, loss 0.00018380023539066315\n",
      "iteration 353, loss 0.0001712349767331034\n",
      "iteration 354, loss 0.00015951317618601024\n",
      "iteration 355, loss 0.0001485646062064916\n",
      "iteration 356, loss 0.0001383477938361466\n",
      "iteration 357, loss 0.0001288211060455069\n",
      "iteration 358, loss 0.00011993199586868286\n",
      "iteration 359, loss 0.00011164697934873402\n",
      "iteration 360, loss 0.00010390629176981747\n",
      "iteration 361, loss 9.670179861132056e-05\n",
      "iteration 362, loss 8.99766746442765e-05\n",
      "iteration 363, loss 8.370762225240469e-05\n",
      "iteration 364, loss 7.78664107201621e-05\n",
      "iteration 365, loss 7.242259744089097e-05\n",
      "iteration 366, loss 6.735458737239242e-05\n",
      "iteration 367, loss 6.263118120841682e-05\n",
      "iteration 368, loss 5.8225599786965176e-05\n",
      "iteration 369, loss 5.412416430772282e-05\n",
      "iteration 370, loss 5.030773172620684e-05\n",
      "iteration 371, loss 4.6750137698836625e-05\n",
      "iteration 372, loss 4.34407738794107e-05\n",
      "iteration 373, loss 4.035928941448219e-05\n",
      "iteration 374, loss 3.7493220588658005e-05\n",
      "iteration 375, loss 3.482533429632895e-05\n",
      "iteration 376, loss 3.234385803807527e-05\n",
      "iteration 377, loss 3.003366509801708e-05\n",
      "iteration 378, loss 2.7884916562470607e-05\n",
      "iteration 379, loss 2.5886480216286145e-05\n",
      "iteration 380, loss 2.402704740234185e-05\n",
      "iteration 381, loss 2.2302483557723463e-05\n",
      "iteration 382, loss 2.0694404156529345e-05\n",
      "iteration 383, loss 1.9202849216526374e-05\n",
      "iteration 384, loss 1.7815087630879134e-05\n",
      "iteration 385, loss 1.652658284001518e-05\n",
      "iteration 386, loss 1.5326753782574087e-05\n",
      "iteration 387, loss 1.421410524926614e-05\n",
      "iteration 388, loss 1.3180406313040294e-05\n",
      "iteration 389, loss 1.2219334166729823e-05\n",
      "iteration 390, loss 1.1329198969178833e-05\n",
      "iteration 391, loss 1.050031823979225e-05\n",
      "iteration 392, loss 9.732143553264905e-06\n",
      "iteration 393, loss 9.01768908079248e-06\n",
      "iteration 394, loss 8.35611717775464e-06\n",
      "iteration 395, loss 7.741386980342213e-06\n",
      "iteration 396, loss 7.170622666308191e-06\n",
      "iteration 397, loss 6.6427983256289735e-06\n",
      "iteration 398, loss 6.151383786345832e-06\n",
      "iteration 399, loss 5.6956487242132425e-06\n",
      "iteration 400, loss 5.273629540170077e-06\n",
      "iteration 401, loss 4.882745997747406e-06\n",
      "iteration 402, loss 4.5199440137366764e-06\n",
      "iteration 403, loss 4.183134478807915e-06\n",
      "iteration 404, loss 3.87080308428267e-06\n",
      "iteration 405, loss 3.5827956708089914e-06\n",
      "iteration 406, loss 3.3152507512568263e-06\n",
      "iteration 407, loss 3.0661581149615813e-06\n",
      "iteration 408, loss 2.837127340171719e-06\n",
      "iteration 409, loss 2.6235620680381544e-06\n",
      "iteration 410, loss 2.426133960398147e-06\n",
      "iteration 411, loss 2.24331347453699e-06\n",
      "iteration 412, loss 2.0746979316754732e-06\n",
      "iteration 413, loss 1.9176534351572627e-06\n",
      "iteration 414, loss 1.7729109913489083e-06\n",
      "iteration 415, loss 1.6388298718084116e-06\n",
      "iteration 416, loss 1.5143630207603564e-06\n",
      "iteration 417, loss 1.3997776022733888e-06\n",
      "iteration 418, loss 1.2930800039612222e-06\n",
      "iteration 419, loss 1.1944975994993001e-06\n",
      "iteration 420, loss 1.103762656384788e-06\n",
      "iteration 421, loss 1.0196355333391693e-06\n",
      "iteration 422, loss 9.416435204911977e-07\n",
      "iteration 423, loss 8.699277032064856e-07\n",
      "iteration 424, loss 8.030916092138796e-07\n",
      "iteration 425, loss 7.413787557197793e-07\n",
      "iteration 426, loss 6.844293238827959e-07\n",
      "iteration 427, loss 6.319937142507115e-07\n",
      "iteration 428, loss 5.832679903505777e-07\n",
      "iteration 429, loss 5.382768222261802e-07\n",
      "iteration 430, loss 4.968910616298672e-07\n",
      "iteration 431, loss 4.5823716732229514e-07\n",
      "iteration 432, loss 4.2294655600016995e-07\n",
      "iteration 433, loss 3.9019485598146275e-07\n",
      "iteration 434, loss 3.5996589531350764e-07\n",
      "iteration 435, loss 3.320379846627475e-07\n",
      "iteration 436, loss 3.0617096058449533e-07\n",
      "iteration 437, loss 2.824488660735369e-07\n",
      "iteration 438, loss 2.604883775347844e-07\n",
      "iteration 439, loss 2.4011717414396117e-07\n",
      "iteration 440, loss 2.2135643007459294e-07\n",
      "iteration 441, loss 2.0417850521425862e-07\n",
      "iteration 442, loss 1.881648756807408e-07\n",
      "iteration 443, loss 1.7345431047033344e-07\n",
      "iteration 444, loss 1.598711349970472e-07\n",
      "iteration 445, loss 1.4723531194249517e-07\n",
      "iteration 446, loss 1.3583630220637133e-07\n",
      "iteration 447, loss 1.2511188174357812e-07\n",
      "iteration 448, loss 1.153343021087494e-07\n",
      "iteration 449, loss 1.0607251255123629e-07\n",
      "iteration 450, loss 9.779208198779088e-08\n",
      "iteration 451, loss 9.000014244975318e-08\n",
      "iteration 452, loss 8.301901743834605e-08\n",
      "iteration 453, loss 7.640825572252652e-08\n",
      "iteration 454, loss 7.039387384111251e-08\n",
      "iteration 455, loss 6.487228176865756e-08\n",
      "iteration 456, loss 5.964377436384893e-08\n",
      "iteration 457, loss 5.489339116593328e-08\n",
      "iteration 458, loss 5.062079821982479e-08\n",
      "iteration 459, loss 4.6521115848463523e-08\n",
      "iteration 460, loss 4.2898697927284957e-08\n",
      "iteration 461, loss 3.948029814182519e-08\n",
      "iteration 462, loss 3.6390730429047835e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 463, loss 3.3503834373505015e-08\n",
      "iteration 464, loss 3.083783894908265e-08\n",
      "iteration 465, loss 2.8345011671149223e-08\n",
      "iteration 466, loss 2.6158872401538247e-08\n",
      "iteration 467, loss 2.4055118785781815e-08\n",
      "iteration 468, loss 2.2112679687325e-08\n",
      "iteration 469, loss 2.0387425081480615e-08\n",
      "iteration 470, loss 1.878484745532205e-08\n",
      "iteration 471, loss 1.728317755578246e-08\n",
      "iteration 472, loss 1.592715470621897e-08\n",
      "iteration 473, loss 1.462845666821977e-08\n",
      "iteration 474, loss 1.3472472915054823e-08\n",
      "iteration 475, loss 1.2395792836628061e-08\n",
      "iteration 476, loss 1.1396334542723707e-08\n",
      "iteration 477, loss 1.0503438119258135e-08\n",
      "iteration 478, loss 9.655490629256747e-09\n",
      "iteration 479, loss 8.874819101833964e-09\n",
      "iteration 480, loss 8.168589360479928e-09\n",
      "iteration 481, loss 7.516428368603556e-09\n",
      "iteration 482, loss 6.932586504859728e-09\n",
      "iteration 483, loss 6.384199835451909e-09\n",
      "iteration 484, loss 5.910928635444179e-09\n",
      "iteration 485, loss 5.464299235313774e-09\n",
      "iteration 486, loss 5.02578334504733e-09\n",
      "iteration 487, loss 4.641195872068238e-09\n",
      "iteration 488, loss 4.293058353255219e-09\n",
      "iteration 489, loss 3.9750305269592445e-09\n",
      "iteration 490, loss 3.672025128409473e-09\n",
      "iteration 491, loss 3.394045711146987e-09\n",
      "iteration 492, loss 3.1463369687401155e-09\n",
      "iteration 493, loss 2.902436957441523e-09\n",
      "iteration 494, loss 2.7000841562596634e-09\n",
      "iteration 495, loss 2.5016200222438556e-09\n",
      "iteration 496, loss 2.3227428869176947e-09\n",
      "iteration 497, loss 2.1718820075733447e-09\n",
      "iteration 498, loss 2.0085841878625388e-09\n",
      "iteration 499, loss 1.860450682400483e-09\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.Linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.Linear2 = torch.nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.Linear1(x)\n",
    "        h = z.clamp(min=0)\n",
    "        y_pred = self.Linear2(h)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model = model.cuda(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss.item()))\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 665.3807373046875\n",
      "iteration 1, loss 667.4822998046875\n",
      "iteration 2, loss 662.0380859375\n",
      "iteration 3, loss 659.9161376953125\n",
      "iteration 4, loss 656.7757568359375\n",
      "iteration 5, loss 652.5364990234375\n",
      "iteration 6, loss 658.068115234375\n",
      "iteration 7, loss 690.323974609375\n",
      "iteration 8, loss 644.5076904296875\n",
      "iteration 9, loss 638.4566650390625\n",
      "iteration 10, loss 628.17724609375\n",
      "iteration 11, loss 653.489501953125\n",
      "iteration 12, loss 633.30126953125\n",
      "iteration 13, loss 630.4693603515625\n",
      "iteration 14, loss 386.815185546875\n",
      "iteration 15, loss 650.8760986328125\n",
      "iteration 16, loss 649.807373046875\n",
      "iteration 17, loss 295.4185791015625\n",
      "iteration 18, loss 646.57666015625\n",
      "iteration 19, loss 644.215087890625\n",
      "iteration 20, loss 208.1729736328125\n",
      "iteration 21, loss 599.1480712890625\n",
      "iteration 22, loss 633.9720458984375\n",
      "iteration 23, loss 628.6323852539062\n",
      "iteration 24, loss 621.5452880859375\n",
      "iteration 25, loss 110.98944091796875\n",
      "iteration 26, loss 603.543701171875\n",
      "iteration 27, loss 542.5189208984375\n",
      "iteration 28, loss 529.5232543945312\n",
      "iteration 29, loss 500.3763427734375\n",
      "iteration 30, loss 89.06681823730469\n",
      "iteration 31, loss 86.8443832397461\n",
      "iteration 32, loss 451.95684814453125\n",
      "iteration 33, loss 430.3346252441406\n",
      "iteration 34, loss 69.27043151855469\n",
      "iteration 35, loss 326.9158630371094\n",
      "iteration 36, loss 58.22926330566406\n",
      "iteration 37, loss 275.87786865234375\n",
      "iteration 38, loss 405.3061218261719\n",
      "iteration 39, loss 45.28407669067383\n",
      "iteration 40, loss 364.0561218261719\n",
      "iteration 41, loss 182.6128387451172\n",
      "iteration 42, loss 252.29234313964844\n",
      "iteration 43, loss 142.99603271484375\n",
      "iteration 44, loss 122.13385009765625\n",
      "iteration 45, loss 102.580810546875\n",
      "iteration 46, loss 85.45613861083984\n",
      "iteration 47, loss 79.84112548828125\n",
      "iteration 48, loss 152.6551055908203\n",
      "iteration 49, loss 51.501155853271484\n",
      "iteration 50, loss 76.74745178222656\n",
      "iteration 51, loss 187.24716186523438\n",
      "iteration 52, loss 165.63485717773438\n",
      "iteration 53, loss 59.57295227050781\n",
      "iteration 54, loss 218.01385498046875\n",
      "iteration 55, loss 55.011966705322266\n",
      "iteration 56, loss 538.9989624023438\n",
      "iteration 57, loss 116.22195434570312\n",
      "iteration 58, loss 211.767578125\n",
      "iteration 59, loss 191.9839324951172\n",
      "iteration 60, loss 318.75921630859375\n",
      "iteration 61, loss 152.3787841796875\n",
      "iteration 62, loss 259.16400146484375\n",
      "iteration 63, loss 239.26116943359375\n",
      "iteration 64, loss 145.6485137939453\n",
      "iteration 65, loss 194.12889099121094\n",
      "iteration 66, loss 172.0635223388672\n",
      "iteration 67, loss 156.5572509765625\n",
      "iteration 68, loss 146.8136749267578\n",
      "iteration 69, loss 271.57489013671875\n",
      "iteration 70, loss 122.49070739746094\n",
      "iteration 71, loss 145.73416137695312\n",
      "iteration 72, loss 63.553348541259766\n",
      "iteration 73, loss 139.51416015625\n",
      "iteration 74, loss 114.76563262939453\n",
      "iteration 75, loss 32.425209045410156\n",
      "iteration 76, loss 73.93424987792969\n",
      "iteration 77, loss 47.81202697753906\n",
      "iteration 78, loss 122.84195709228516\n",
      "iteration 79, loss 114.919921875\n",
      "iteration 80, loss 95.47233581542969\n",
      "iteration 81, loss 77.58474731445312\n",
      "iteration 82, loss 63.00596618652344\n",
      "iteration 83, loss 47.51103210449219\n",
      "iteration 84, loss 42.629859924316406\n",
      "iteration 85, loss 35.741539001464844\n",
      "iteration 86, loss 37.61311340332031\n",
      "iteration 87, loss 37.666507720947266\n",
      "iteration 88, loss 66.75938415527344\n",
      "iteration 89, loss 25.354875564575195\n",
      "iteration 90, loss 40.96742248535156\n",
      "iteration 91, loss 55.691917419433594\n",
      "iteration 92, loss 58.57828903198242\n",
      "iteration 93, loss 28.88828468322754\n",
      "iteration 94, loss 27.580551147460938\n",
      "iteration 95, loss 36.66615676879883\n",
      "iteration 96, loss 65.46316528320312\n",
      "iteration 97, loss 45.54692077636719\n",
      "iteration 98, loss 16.05134391784668\n",
      "iteration 99, loss 19.245502471923828\n",
      "iteration 100, loss 50.61809539794922\n",
      "iteration 101, loss 27.431026458740234\n",
      "iteration 102, loss 13.150978088378906\n",
      "iteration 103, loss 37.027488708496094\n",
      "iteration 104, loss 24.014278411865234\n",
      "iteration 105, loss 27.011837005615234\n",
      "iteration 106, loss 14.647930145263672\n",
      "iteration 107, loss 12.074944496154785\n",
      "iteration 108, loss 26.61042594909668\n",
      "iteration 109, loss 22.148900985717773\n",
      "iteration 110, loss 40.66188430786133\n",
      "iteration 111, loss 19.012910842895508\n",
      "iteration 112, loss 24.79309844970703\n",
      "iteration 113, loss 41.615455627441406\n",
      "iteration 114, loss 27.467309951782227\n",
      "iteration 115, loss 30.484466552734375\n",
      "iteration 116, loss 16.376911163330078\n",
      "iteration 117, loss 16.419689178466797\n",
      "iteration 118, loss 8.226079940795898\n",
      "iteration 119, loss 11.864397048950195\n",
      "iteration 120, loss 13.901311874389648\n",
      "iteration 121, loss 45.09604263305664\n",
      "iteration 122, loss 25.840436935424805\n",
      "iteration 123, loss 15.518186569213867\n",
      "iteration 124, loss 18.483970642089844\n",
      "iteration 125, loss 32.537200927734375\n",
      "iteration 126, loss 43.2156982421875\n",
      "iteration 127, loss 10.533226013183594\n",
      "iteration 128, loss 21.271526336669922\n",
      "iteration 129, loss 18.310653686523438\n",
      "iteration 130, loss 25.24571990966797\n",
      "iteration 131, loss 13.827743530273438\n",
      "iteration 132, loss 22.28472137451172\n",
      "iteration 133, loss 8.709211349487305\n",
      "iteration 134, loss 19.750093460083008\n",
      "iteration 135, loss 6.940408706665039\n",
      "iteration 136, loss 12.540700912475586\n",
      "iteration 137, loss 8.875053405761719\n",
      "iteration 138, loss 13.237763404846191\n",
      "iteration 139, loss 6.252812385559082\n",
      "iteration 140, loss 6.626097679138184\n",
      "iteration 141, loss 6.640542030334473\n",
      "iteration 142, loss 7.486516952514648\n",
      "iteration 143, loss 24.804561614990234\n",
      "iteration 144, loss 4.3323869705200195\n",
      "iteration 145, loss 6.879574775695801\n",
      "iteration 146, loss 9.032272338867188\n",
      "iteration 147, loss 10.628049850463867\n",
      "iteration 148, loss 10.146843910217285\n",
      "iteration 149, loss 7.365720748901367\n",
      "iteration 150, loss 5.740720748901367\n",
      "iteration 151, loss 4.861584663391113\n",
      "iteration 152, loss 4.546713352203369\n",
      "iteration 153, loss 8.262934684753418\n",
      "iteration 154, loss 4.254531383514404\n",
      "iteration 155, loss 4.653010368347168\n",
      "iteration 156, loss 3.4354066848754883\n",
      "iteration 157, loss 5.70953893661499\n",
      "iteration 158, loss 3.9201595783233643\n",
      "iteration 159, loss 3.7376387119293213\n",
      "iteration 160, loss 2.9039056301116943\n",
      "iteration 161, loss 2.692067861557007\n",
      "iteration 162, loss 2.1636404991149902\n",
      "iteration 163, loss 5.687909126281738\n",
      "iteration 164, loss 3.950765609741211\n",
      "iteration 165, loss 2.342984199523926\n",
      "iteration 166, loss 2.6301560401916504\n",
      "iteration 167, loss 3.074873447418213\n",
      "iteration 168, loss 3.132272243499756\n",
      "iteration 169, loss 4.3060302734375\n",
      "iteration 170, loss 3.239027976989746\n",
      "iteration 171, loss 2.2894327640533447\n",
      "iteration 172, loss 6.974739074707031\n",
      "iteration 173, loss 3.218843698501587\n",
      "iteration 174, loss 3.5501980781555176\n",
      "iteration 175, loss 2.950160503387451\n",
      "iteration 176, loss 2.406982421875\n",
      "iteration 177, loss 2.160024642944336\n",
      "iteration 178, loss 1.282284140586853\n",
      "iteration 179, loss 4.599667549133301\n",
      "iteration 180, loss 1.4038376808166504\n",
      "iteration 181, loss 1.8034807443618774\n",
      "iteration 182, loss 1.871056318283081\n",
      "iteration 183, loss 1.5474518537521362\n",
      "iteration 184, loss 5.914847373962402\n",
      "iteration 185, loss 1.2993652820587158\n",
      "iteration 186, loss 1.2287923097610474\n",
      "iteration 187, loss 1.762182354927063\n",
      "iteration 188, loss 1.0662424564361572\n",
      "iteration 189, loss 4.373104572296143\n",
      "iteration 190, loss 0.9839729070663452\n",
      "iteration 191, loss 3.5158791542053223\n",
      "iteration 192, loss 1.5007658004760742\n",
      "iteration 193, loss 1.384156584739685\n",
      "iteration 194, loss 1.024818778038025\n",
      "iteration 195, loss 3.1899988651275635\n",
      "iteration 196, loss 2.687681198120117\n",
      "iteration 197, loss 1.4465136528015137\n",
      "iteration 198, loss 0.9500715136528015\n",
      "iteration 199, loss 0.8533750772476196\n",
      "iteration 200, loss 1.783707857131958\n",
      "iteration 201, loss 0.574459969997406\n",
      "iteration 202, loss 0.4748038649559021\n",
      "iteration 203, loss 0.4027773439884186\n",
      "iteration 204, loss 1.618337631225586\n",
      "iteration 205, loss 0.310449481010437\n",
      "iteration 206, loss 2.6433277130126953\n",
      "iteration 207, loss 1.6805598735809326\n",
      "iteration 208, loss 1.7740240097045898\n",
      "iteration 209, loss 2.1778664588928223\n",
      "iteration 210, loss 2.0044870376586914\n",
      "iteration 211, loss 0.9539342522621155\n",
      "iteration 212, loss 0.8495833873748779\n",
      "iteration 213, loss 0.7509825229644775\n",
      "iteration 214, loss 1.1854417324066162\n",
      "iteration 215, loss 0.7226215600967407\n",
      "iteration 216, loss 1.1056159734725952\n",
      "iteration 217, loss 1.0236650705337524\n",
      "iteration 218, loss 0.6057484745979309\n",
      "iteration 219, loss 0.6910321712493896\n",
      "iteration 220, loss 4.248265266418457\n",
      "iteration 221, loss 0.8182552456855774\n",
      "iteration 222, loss 0.3764142692089081\n",
      "iteration 223, loss 0.9213220477104187\n",
      "iteration 224, loss 0.3380237817764282\n",
      "iteration 225, loss 0.8774347305297852\n",
      "iteration 226, loss 2.9886763095855713\n",
      "iteration 227, loss 1.1557449102401733\n",
      "iteration 228, loss 2.362123727798462\n",
      "iteration 229, loss 1.0953805446624756\n",
      "iteration 230, loss 1.1160435676574707\n",
      "iteration 231, loss 2.0067591667175293\n",
      "iteration 232, loss 1.8720319271087646\n",
      "iteration 233, loss 1.7161163091659546\n",
      "iteration 234, loss 1.5141515731811523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 235, loss 1.269221305847168\n",
      "iteration 236, loss 1.016634225845337\n",
      "iteration 237, loss 1.99981689453125\n",
      "iteration 238, loss 0.96180260181427\n",
      "iteration 239, loss 1.0563240051269531\n",
      "iteration 240, loss 1.7325334548950195\n",
      "iteration 241, loss 1.9191982746124268\n",
      "iteration 242, loss 1.783538579940796\n",
      "iteration 243, loss 1.5374025106430054\n",
      "iteration 244, loss 1.0737732648849487\n",
      "iteration 245, loss 1.053750991821289\n",
      "iteration 246, loss 0.8736110925674438\n",
      "iteration 247, loss 0.8216944932937622\n",
      "iteration 248, loss 0.8604941368103027\n",
      "iteration 249, loss 0.8533264398574829\n",
      "iteration 250, loss 0.7644381523132324\n",
      "iteration 251, loss 0.7355989217758179\n",
      "iteration 252, loss 0.828278660774231\n",
      "iteration 253, loss 0.7325918078422546\n",
      "iteration 254, loss 0.5133733153343201\n",
      "iteration 255, loss 0.5911728143692017\n",
      "iteration 256, loss 3.1362500190734863\n",
      "iteration 257, loss 0.6509977579116821\n",
      "iteration 258, loss 0.3626253604888916\n",
      "iteration 259, loss 0.7949477434158325\n",
      "iteration 260, loss 0.6475604176521301\n",
      "iteration 261, loss 0.8000372648239136\n",
      "iteration 262, loss 0.3185620903968811\n",
      "iteration 263, loss 0.486640065908432\n",
      "iteration 264, loss 2.7368428707122803\n",
      "iteration 265, loss 0.7836745381355286\n",
      "iteration 266, loss 0.6079509258270264\n",
      "iteration 267, loss 0.7572238445281982\n",
      "iteration 268, loss 2.0706653594970703\n",
      "iteration 269, loss 0.24596571922302246\n",
      "iteration 270, loss 1.9555023908615112\n",
      "iteration 271, loss 0.47832202911376953\n",
      "iteration 272, loss 0.832077145576477\n",
      "iteration 273, loss 1.5210106372833252\n",
      "iteration 274, loss 0.6585147976875305\n",
      "iteration 275, loss 1.212676763534546\n",
      "iteration 276, loss 0.8477374911308289\n",
      "iteration 277, loss 0.8397480249404907\n",
      "iteration 278, loss 1.2385008335113525\n",
      "iteration 279, loss 1.0337555408477783\n",
      "iteration 280, loss 0.7679867148399353\n",
      "iteration 281, loss 1.1435527801513672\n",
      "iteration 282, loss 1.0256364345550537\n",
      "iteration 283, loss 1.1927130222320557\n",
      "iteration 284, loss 0.7226686477661133\n",
      "iteration 285, loss 1.2969061136245728\n",
      "iteration 286, loss 0.859756350517273\n",
      "iteration 287, loss 0.6161795854568481\n",
      "iteration 288, loss 0.5866426229476929\n",
      "iteration 289, loss 1.6116708517074585\n",
      "iteration 290, loss 0.3643161654472351\n",
      "iteration 291, loss 0.3490811586380005\n",
      "iteration 292, loss 0.3096204400062561\n",
      "iteration 293, loss 0.9135463833808899\n",
      "iteration 294, loss 1.5862102508544922\n",
      "iteration 295, loss 0.5201592445373535\n",
      "iteration 296, loss 0.6427494287490845\n",
      "iteration 297, loss 1.4340293407440186\n",
      "iteration 298, loss 0.20748768746852875\n",
      "iteration 299, loss 0.1397426277399063\n",
      "iteration 300, loss 0.11069262027740479\n",
      "iteration 301, loss 0.48029613494873047\n",
      "iteration 302, loss 0.5107457041740417\n",
      "iteration 303, loss 1.4586853981018066\n",
      "iteration 304, loss 1.3260235786437988\n",
      "iteration 305, loss 0.677824854850769\n",
      "iteration 306, loss 0.4585680067539215\n",
      "iteration 307, loss 1.226468563079834\n",
      "iteration 308, loss 1.0215165615081787\n",
      "iteration 309, loss 0.180013507604599\n",
      "iteration 310, loss 1.0706369876861572\n",
      "iteration 311, loss 0.16700738668441772\n",
      "iteration 312, loss 0.950812816619873\n",
      "iteration 313, loss 0.1313433051109314\n",
      "iteration 314, loss 0.6121149063110352\n",
      "iteration 315, loss 0.6023098826408386\n",
      "iteration 316, loss 0.6057864427566528\n",
      "iteration 317, loss 0.5375229120254517\n",
      "iteration 318, loss 0.16550786793231964\n",
      "iteration 319, loss 0.1683294177055359\n",
      "iteration 320, loss 0.561999499797821\n",
      "iteration 321, loss 0.400064617395401\n",
      "iteration 322, loss 0.4194757342338562\n",
      "iteration 323, loss 0.15878432989120483\n",
      "iteration 324, loss 0.3560125231742859\n",
      "iteration 325, loss 1.9601296186447144\n",
      "iteration 326, loss 0.2869350016117096\n",
      "iteration 327, loss 0.4745999574661255\n",
      "iteration 328, loss 0.5398184061050415\n",
      "iteration 329, loss 0.14171120524406433\n",
      "iteration 330, loss 0.453824520111084\n",
      "iteration 331, loss 1.4352236986160278\n",
      "iteration 332, loss 1.2904577255249023\n",
      "iteration 333, loss 1.2162272930145264\n",
      "iteration 334, loss 1.0482802391052246\n",
      "iteration 335, loss 0.15758410096168518\n",
      "iteration 336, loss 0.17928123474121094\n",
      "iteration 337, loss 1.0901381969451904\n",
      "iteration 338, loss 0.7264775037765503\n",
      "iteration 339, loss 0.7312761545181274\n",
      "iteration 340, loss 0.18740969896316528\n",
      "iteration 341, loss 1.2495958805084229\n",
      "iteration 342, loss 0.1334223747253418\n",
      "iteration 343, loss 0.5337343811988831\n",
      "iteration 344, loss 0.46615904569625854\n",
      "iteration 345, loss 0.7266260385513306\n",
      "iteration 346, loss 0.38732701539993286\n",
      "iteration 347, loss 0.6062754392623901\n",
      "iteration 348, loss 0.30294331908226013\n",
      "iteration 349, loss 0.3328741192817688\n",
      "iteration 350, loss 0.4491960406303406\n",
      "iteration 351, loss 0.39987894892692566\n",
      "iteration 352, loss 0.32917311787605286\n",
      "iteration 353, loss 0.37968045473098755\n",
      "iteration 354, loss 0.37222152948379517\n",
      "iteration 355, loss 1.9239413738250732\n",
      "iteration 356, loss 0.24298962950706482\n",
      "iteration 357, loss 0.2753855288028717\n",
      "iteration 358, loss 1.3668709993362427\n",
      "iteration 359, loss 1.1943550109863281\n",
      "iteration 360, loss 0.9994133710861206\n",
      "iteration 361, loss 0.14849436283111572\n",
      "iteration 362, loss 0.761029064655304\n",
      "iteration 363, loss 0.7975755929946899\n",
      "iteration 364, loss 0.17734172940254211\n",
      "iteration 365, loss 0.9987738728523254\n",
      "iteration 366, loss 0.6222423315048218\n",
      "iteration 367, loss 0.13221371173858643\n",
      "iteration 368, loss 0.12780262529850006\n",
      "iteration 369, loss 0.6269239187240601\n",
      "iteration 370, loss 0.1097218245267868\n",
      "iteration 371, loss 0.5108925104141235\n",
      "iteration 372, loss 0.10957567393779755\n",
      "iteration 373, loss 0.3446981906890869\n",
      "iteration 374, loss 0.2893790006637573\n",
      "iteration 375, loss 0.7675089836120605\n",
      "iteration 376, loss 0.582619309425354\n",
      "iteration 377, loss 0.538938045501709\n",
      "iteration 378, loss 0.6590772867202759\n",
      "iteration 379, loss 0.4619067907333374\n",
      "iteration 380, loss 0.3806167542934418\n",
      "iteration 381, loss 0.34615713357925415\n",
      "iteration 382, loss 0.24058692157268524\n",
      "iteration 383, loss 1.4912534952163696\n",
      "iteration 384, loss 0.47354280948638916\n",
      "iteration 385, loss 0.300179660320282\n",
      "iteration 386, loss 0.292937695980072\n",
      "iteration 387, loss 0.33418646454811096\n",
      "iteration 388, loss 0.3129342496395111\n",
      "iteration 389, loss 1.6614539623260498\n",
      "iteration 390, loss 0.28261667490005493\n",
      "iteration 391, loss 0.46006348729133606\n",
      "iteration 392, loss 1.1118788719177246\n",
      "iteration 393, loss 0.4001266360282898\n",
      "iteration 394, loss 0.35898086428642273\n",
      "iteration 395, loss 0.31231361627578735\n",
      "iteration 396, loss 0.142721027135849\n",
      "iteration 397, loss 0.4194716811180115\n",
      "iteration 398, loss 0.11429975926876068\n",
      "iteration 399, loss 1.904872179031372\n",
      "iteration 400, loss 0.2026304453611374\n",
      "iteration 401, loss 0.3747131824493408\n",
      "iteration 402, loss 0.16885432600975037\n",
      "iteration 403, loss 0.14794427156448364\n",
      "iteration 404, loss 0.8027083873748779\n",
      "iteration 405, loss 0.4212982654571533\n",
      "iteration 406, loss 1.1820393800735474\n",
      "iteration 407, loss 0.591489851474762\n",
      "iteration 408, loss 1.1248395442962646\n",
      "iteration 409, loss 0.38858699798583984\n",
      "iteration 410, loss 0.5565661191940308\n",
      "iteration 411, loss 0.35895511507987976\n",
      "iteration 412, loss 0.636724591255188\n",
      "iteration 413, loss 0.12873581051826477\n",
      "iteration 414, loss 0.2683354616165161\n",
      "iteration 415, loss 1.6557788848876953\n",
      "iteration 416, loss 0.8733652234077454\n",
      "iteration 417, loss 0.494884192943573\n",
      "iteration 418, loss 0.5996094346046448\n",
      "iteration 419, loss 0.3235720992088318\n",
      "iteration 420, loss 2.796076774597168\n",
      "iteration 421, loss 0.08762971311807632\n",
      "iteration 422, loss 0.7020681500434875\n",
      "iteration 423, loss 0.7835336923599243\n",
      "iteration 424, loss 0.4545999765396118\n",
      "iteration 425, loss 1.8568809032440186\n",
      "iteration 426, loss 0.7109895944595337\n",
      "iteration 427, loss 0.6426392793655396\n",
      "iteration 428, loss 2.0221750736236572\n",
      "iteration 429, loss 1.6076292991638184\n",
      "iteration 430, loss 0.6545966267585754\n",
      "iteration 431, loss 0.7613992094993591\n",
      "iteration 432, loss 0.6463830471038818\n",
      "iteration 433, loss 0.6170886754989624\n",
      "iteration 434, loss 1.2150617837905884\n",
      "iteration 435, loss 0.3495306968688965\n",
      "iteration 436, loss 0.5622333288192749\n",
      "iteration 437, loss 0.5286774635314941\n",
      "iteration 438, loss 0.7284784913063049\n",
      "iteration 439, loss 0.5205987691879272\n",
      "iteration 440, loss 0.49299120903015137\n",
      "iteration 441, loss 0.16455838084220886\n",
      "iteration 442, loss 0.36972177028656006\n",
      "iteration 443, loss 0.12300529330968857\n",
      "iteration 444, loss 0.3631674349308014\n",
      "iteration 445, loss 0.3729540705680847\n",
      "iteration 446, loss 0.33061009645462036\n",
      "iteration 447, loss 0.21051089465618134\n",
      "iteration 448, loss 0.9282997846603394\n",
      "iteration 449, loss 0.11866353452205658\n",
      "iteration 450, loss 0.878959059715271\n",
      "iteration 451, loss 0.7963004112243652\n",
      "iteration 452, loss 0.578748881816864\n",
      "iteration 453, loss 0.11818842589855194\n",
      "iteration 454, loss 0.7128328084945679\n",
      "iteration 455, loss 0.8487743139266968\n",
      "iteration 456, loss 0.18229040503501892\n",
      "iteration 457, loss 0.17921878397464752\n",
      "iteration 458, loss 0.7617529630661011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 459, loss 0.11623502522706985\n",
      "iteration 460, loss 0.5585395693778992\n",
      "iteration 461, loss 0.48920246958732605\n",
      "iteration 462, loss 0.07855023443698883\n",
      "iteration 463, loss 0.1149032860994339\n",
      "iteration 464, loss 0.40988779067993164\n",
      "iteration 465, loss 0.33608633279800415\n",
      "iteration 466, loss 0.32324522733688354\n",
      "iteration 467, loss 0.2836441397666931\n",
      "iteration 468, loss 0.1806631088256836\n",
      "iteration 469, loss 0.2867644727230072\n",
      "iteration 470, loss 0.2633781433105469\n",
      "iteration 471, loss 0.21497085690498352\n",
      "iteration 472, loss 0.164661705493927\n",
      "iteration 473, loss 0.31206971406936646\n",
      "iteration 474, loss 0.32596883177757263\n",
      "iteration 475, loss 0.9420532584190369\n",
      "iteration 476, loss 0.28692981600761414\n",
      "iteration 477, loss 0.2793562412261963\n",
      "iteration 478, loss 0.89931321144104\n",
      "iteration 479, loss 0.16659320890903473\n",
      "iteration 480, loss 0.2636495530605316\n",
      "iteration 481, loss 0.1808192878961563\n",
      "iteration 482, loss 0.20606039464473724\n",
      "iteration 483, loss 0.24417173862457275\n",
      "iteration 484, loss 0.20268774032592773\n",
      "iteration 485, loss 0.1675044596195221\n",
      "iteration 486, loss 0.09084329754114151\n",
      "iteration 487, loss 0.09399823844432831\n",
      "iteration 488, loss 0.1824554204940796\n",
      "iteration 489, loss 0.3257790803909302\n",
      "iteration 490, loss 0.2671772837638855\n",
      "iteration 491, loss 0.21383100748062134\n",
      "iteration 492, loss 0.12334441393613815\n",
      "iteration 493, loss 1.1475443840026855\n",
      "iteration 494, loss 0.16759082674980164\n",
      "iteration 495, loss 0.8225357532501221\n",
      "iteration 496, loss 0.09456176310777664\n",
      "iteration 497, loss 0.08917149156332016\n",
      "iteration 498, loss 0.8032844066619873\n",
      "iteration 499, loss 0.5965542793273926\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.Linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.middle = torch.nn.Linear(H, H)\n",
    "        self.Linear2 = torch.nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.Linear1(x)\n",
    "        h = z.clamp(min=0)\n",
    "        \n",
    "        for i in range(random.randint(0,3)):\n",
    "            h = self.middle(h).clamp(min=0)\n",
    "\n",
    "        y_pred = self.Linear2(h)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=datatype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=datatype, device=device)\n",
    "\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model = model.cuda(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n",
    "\n",
    "for iteration in np.arange(500):\n",
    "    # Forward pass (forward pass ends at loss)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(\"iteration {0}, loss {1}\".format(iteration, loss.item()))\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
